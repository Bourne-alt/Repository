[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 334 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 332 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 335 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 333 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 336 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 334 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 337 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 335 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 338 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 336 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 339 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 337 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 340 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 338 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 341 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 339 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 342 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 340 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 343 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 341 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 344 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 342 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 345 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 343 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 346 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 344 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 347 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 345 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 348 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 346 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 349 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 347 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 350 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 348 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 351 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 349 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 352 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 350 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 353 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 351 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 354 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 352 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 355 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 353 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 356 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 354 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 357 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 355 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 358 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 356 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 359 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 357 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 360 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 358 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 361 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 359 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 362 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 360 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 363 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 364 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 361 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 362 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 365 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 363 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 366 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from cba1dced94776128d76ca029e7a35351.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0e7932e1-68ef-4ee6-a3e7-783f00abf175.
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 367 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 364 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 20f9d4f6f5381a4322c52a52227778ed: SlotReport{SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_0, allocationID=c1fe1a4b4e19bc1e160c1d59723a6c16, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_1, allocationID=ab79687eef7bbc363f0b3663ed667539, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_2, allocationID=4974421c5ae7ae524c00c9c5aaa6c7f3, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_3, allocationID=07e9af6d5b5148015941b6307aedf0c7, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_4, allocationID=f2135d08c7d2ce5dcb772dabb572c26c, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_5, allocationID=dcfacd4844dd5c6d8c9a863a0cbd1cd4, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_6, allocationID=8867acf297bcabcb3084ff9a8309a069, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_7, allocationID=7b123f39d1fc43864955926c101ed199, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_8, allocationID=51fc14e9cab63149ad7c1c7533527094, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_9, allocationID=9a4082def1b1b0bcee5a7e41d3260954, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_10, allocationID=ebd18bb5f5ba652ae4617df2cccdffdd, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_11, allocationID=a1ad30c752ff09633d98835051669cc6, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0e7932e1-68ef-4ee6-a3e7-783f00abf175: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from cba1dced94776128d76ca029e7a35351.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 36ad6c4cd1624448a2fec7f418a97339.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 368 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 365 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 366 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 369 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 370 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 367 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 368 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 371 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 369 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 372 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 36ad6c4cd1624448a2fec7f418a97339.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0e7932e1-68ef-4ee6-a3e7-783f00abf175.
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 370 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 373 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 374 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 371 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 375 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 372 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 376 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 373 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 377 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 374 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 378 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 375 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 379 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 376 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 380 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 377 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 378 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 381 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 379 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 382 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 383 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 380 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 381 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 384 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 382 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 385 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 383 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 386 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 384 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 387 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 388 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 385 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 389 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 386 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 390 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 387 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 391 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 388 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 392 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 389 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 393 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 390 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 391 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 394 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 395 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 392 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 396 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 393 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 394 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 397 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 398 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 395 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 396 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 399 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 400 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 397 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 401 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 398 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 399 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 402 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 403 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 400 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 404 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 401 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 402 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 405 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 406 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 403 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 404 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 407 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 408 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 405 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 409 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 406 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 410 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 407 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 411 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 408 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 412 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 409 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 413 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 410 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 414 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 411 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 415 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 412 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 416 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 413 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 417 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 414 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 418 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 415 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 419 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 416 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 420 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 417 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 421 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 418 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 422 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 419 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 423 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 420 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 424 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 421 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 425 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 422 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 426 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 423 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 427 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 424 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 428 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 425 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 429 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 426 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 430 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 427 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 431 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 428 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 432 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 429 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 433 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 430 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 434 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 431 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 435 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 432 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 436 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 433 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 437 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 434 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 438 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 435 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 439 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 436 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 440 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 437 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 441 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 438 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 442 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 439 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 443 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 440 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 444 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 441 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 445 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 442 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 446 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 443 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from cba1dced94776128d76ca029e7a35351.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0e7932e1-68ef-4ee6-a3e7-783f00abf175.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 20f9d4f6f5381a4322c52a52227778ed: SlotReport{SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_0, allocationID=c1fe1a4b4e19bc1e160c1d59723a6c16, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_1, allocationID=ab79687eef7bbc363f0b3663ed667539, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_2, allocationID=4974421c5ae7ae524c00c9c5aaa6c7f3, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_3, allocationID=07e9af6d5b5148015941b6307aedf0c7, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_4, allocationID=f2135d08c7d2ce5dcb772dabb572c26c, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_5, allocationID=dcfacd4844dd5c6d8c9a863a0cbd1cd4, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_6, allocationID=8867acf297bcabcb3084ff9a8309a069, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_7, allocationID=7b123f39d1fc43864955926c101ed199, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_8, allocationID=51fc14e9cab63149ad7c1c7533527094, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_9, allocationID=9a4082def1b1b0bcee5a7e41d3260954, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_10, allocationID=ebd18bb5f5ba652ae4617df2cccdffdd, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}SlotStatus{slotID=0e7932e1-68ef-4ee6-a3e7-783f00abf175_11, allocationID=a1ad30c752ff09633d98835051669cc6, jobID=d79b95531fbe9f65ceab757e8a11ef55, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0e7932e1-68ef-4ee6-a3e7-783f00abf175: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from cba1dced94776128d76ca029e7a35351.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 36ad6c4cd1624448a2fec7f418a97339.
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 447 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 444 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 448 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 445 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 449 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 446 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 450 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 447 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 451 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 448 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 452 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 449 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 36ad6c4cd1624448a2fec7f418a97339.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0e7932e1-68ef-4ee6-a3e7-783f00abf175.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 453 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 450 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 454 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 451 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 455 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 452 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 456 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 453 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 457 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 454 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 458 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 455 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 459 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 456 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 460 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 457 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 461 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 458 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 462 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 459 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 463 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 460 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 464 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 461 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 465 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 462 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 466 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 463 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 467 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 464 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 468 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-9, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-10, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-8, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-12, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-7, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-7, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-10, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-8, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-12, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-9, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-7, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-10, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-8, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-12, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-9, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 465 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-9, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-12, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-8, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-7, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-10, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (4/12)#0 (d8149ae01eed56e8b9927f11ed83d7e3) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (8/12)#0 (5887c8a090110fb517bd28f83ae5adae) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (3/12)#0 (1ea5faa3f057f4dd552762cd257b4ee6) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (2/12)#0 (5bf5b095c1ab291f9b9d1f246072c471) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (9/12)#0 (3056d026b02ea37b41a89e991d3f28af) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (3/12)#0 (1ea5faa3f057f4dd552762cd257b4ee6).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (3/12)#0 network resources (state: FAILED).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (9/12)#0 (3056d026b02ea37b41a89e991d3f28af).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (8/12)#0 (5887c8a090110fb517bd28f83ae5adae).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (8/12)#0 network resources (state: FAILED).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (2/12)#0 (5bf5b095c1ab291f9b9d1f246072c471).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (2/12)#0 network resources (state: FAILED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (9/12)#0 network resources (state: FAILED).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (4/12)#0 (d8149ae01eed56e8b9927f11ed83d7e3).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (4/12)#0 network resources (state: FAILED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (9/12)#0 (3056d026b02ea37b41a89e991d3f28af) [FAILED]
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (8/12)#0 (5887c8a090110fb517bd28f83ae5adae) [FAILED]
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (4/12)#0 (d8149ae01eed56e8b9927f11ed83d7e3) [FAILED]
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (2/12)#0 (5bf5b095c1ab291f9b9d1f246072c471) [FAILED]
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (3/12)#0 (1ea5faa3f057f4dd552762cd257b4ee6) [FAILED]
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (2/12)#0 5bf5b095c1ab291f9b9d1f246072c471.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (4/12)#0 d8149ae01eed56e8b9927f11ed83d7e3.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (8/12)#0 5887c8a090110fb517bd28f83ae5adae.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (9/12)#0 3056d026b02ea37b41a89e991d3f28af.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (2/12) (5bf5b095c1ab291f9b9d1f246072c471) switched from INITIALIZING to FAILED on 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (3/12)#0 1ea5faa3f057f4dd552762cd257b4ee6.
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 469 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{00405c3fdd625cdc791723cb0a106749}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_1) from the physical slot (SlotRequestId{c8922bdf2d638b944542dc3de55bb98d})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{c8922bdf2d638b944542dc3de55bb98d})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{c8922bdf2d638b944542dc3de55bb98d}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot f2135d08c7d2ce5dcb772dabb572c26c.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{c8922bdf2d638b944542dc3de55bb98d})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot f2135d08c7d2ce5dcb772dabb572c26c @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 4 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=11}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=11}]
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 466 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_1.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]1 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_1. 
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{28165e85ba5ef0976e832ba73933977b}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{26900903ab24301cc1e050a5e18702c7}) from cbc357ccb763df2852fee8c4fc7d55f2_0
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_1
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{c63ad8579e8bafa86e137e50cefd6c20}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_2) from the physical slot (SlotRequestId{aac48b3f9d2439621632502c447ff7d1}) from cbc357ccb763df2852fee8c4fc7d55f2_2
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{1c219d8818c59958b6859732a62c950b}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_3) from the physical slot (SlotRequestId{84cefd94b43f6a6248e1f20e653d4c4c}) from cbc357ccb763df2852fee8c4fc7d55f2_3
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{228198fb3d0aadab7eba73fdaeb5fd15}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_4) from the physical slot (SlotRequestId{f4dffc7fd6728490849dffd8840a7977}) from cbc357ccb763df2852fee8c4fc7d55f2_4
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{91a734d16c10ec4616eae254f347e7d0}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_5) from the physical slot (SlotRequestId{d2625758151b238ff94ae653c3acaa13}) from cbc357ccb763df2852fee8c4fc7d55f2_5
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{3b6d82a109add93038dce66cf729cdd2}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_6) from the physical slot (SlotRequestId{fdc6b5a90142a9256824c799f79ab5a2}) from cbc357ccb763df2852fee8c4fc7d55f2_6
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{16d835f1b2752556bdd4799016e47934}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_7) from the physical slot (SlotRequestId{ae0ae9b0505b5dbd2979519bc8d474eb}) from cbc357ccb763df2852fee8c4fc7d55f2_7
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{16026ebf8a53e708a16d989a06213212}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_8) from the physical slot (SlotRequestId{e7e912dec25120e3d655e49ce9a0267a}) from cbc357ccb763df2852fee8c4fc7d55f2_8
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{ce2c53fe712a627a6739a27e9491fb0d}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_9) from the physical slot (SlotRequestId{9dd8905f29cfa9cc626409b468dfe31d}) from cbc357ccb763df2852fee8c4fc7d55f2_9
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{68f17e0dc8a662441a3a06c28bf4ddcc}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_10) from the physical slot (SlotRequestId{f34ac388f1e057cbff6e394cd9f12268}) from cbc357ccb763df2852fee8c4fc7d55f2_10
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{6cf23149dce01fe3269243ffe9844776}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_11) from the physical slot (SlotRequestId{40cda26bfe4192dd1946f1a7f2484f32}) from cbc357ccb763df2852fee8c4fc7d55f2_11
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (d79b95531fbe9f65ceab757e8a11ef55) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/12) (47bb4435bf28a6681f59d27bb7d24bf3) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Attempting to cancel task Source: Custom Source -> Sink: Print to Std. Out (1/12)#0 (47bb4435bf28a6681f59d27bb7d24bf3).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/12)#0 (47bb4435bf28a6681f59d27bb7d24bf3) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Triggering cancellation of task code Source: Custom Source -> Sink: Print to Std. Out (1/12)#0 (47bb4435bf28a6681f59d27bb7d24bf3).
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (3/12) (1ea5faa3f057f4dd552762cd257b4ee6) switched from INITIALIZING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (4/12) (d8149ae01eed56e8b9927f11ed83d7e3) switched from INITIALIZING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (5/12) (ef069e440cba9080ec960c9a59f82421) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (6/12) (9b8faab8eafbf4438fd7b6add185c67d) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (7/12) (669b56cc0110f5d2e32d3c0b2464f243) switched from RUNNING to CANCELING.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (8/12) (5887c8a090110fb517bd28f83ae5adae) switched from INITIALIZING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (9/12) (3056d026b02ea37b41a89e991d3f28af) switched from INITIALIZING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (10/12) (f399f08cf7a53825b3b9bb856f373bed) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (11/12) (a0910cf42ee2bac4943779977f14540d) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (12/12) (bf3aac64ee71a5610ab9150115ac4d32) switched from RUNNING to CANCELING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 1ea5faa3f057f4dd552762cd257b4ee6.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution d8149ae01eed56e8b9927f11ed83d7e3.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Attempting to cancel task Source: Custom Source -> Sink: Print to Std. Out (5/12)#0 (ef069e440cba9080ec960c9a59f82421).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (5/12)#0 (ef069e440cba9080ec960c9a59f82421) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Triggering cancellation of task code Source: Custom Source -> Sink: Print to Std. Out (5/12)#0 (ef069e440cba9080ec960c9a59f82421).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Attempting to cancel task Source: Custom Source -> Sink: Print to Std. Out (6/12)#0 (9b8faab8eafbf4438fd7b6add185c67d).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (6/12)#0 (9b8faab8eafbf4438fd7b6add185c67d) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Triggering cancellation of task code Source: Custom Source -> Sink: Print to Std. Out (6/12)#0 (9b8faab8eafbf4438fd7b6add185c67d).
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Attempting to cancel task Source: Custom Source -> Sink: Print to Std. Out (7/12)#0 (669b56cc0110f5d2e32d3c0b2464f243).
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-11, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-6, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (4/12) (d8149ae01eed56e8b9927f11ed83d7e3) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (7/12)#0 (669b56cc0110f5d2e32d3c0b2464f243) switched from RUNNING to CANCELING.
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Triggering cancellation of task code Source: Custom Source -> Sink: Print to Std. Out (7/12)#0 (669b56cc0110f5d2e32d3c0b2464f243).
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-6, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-11, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-6, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (4/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{1c219d8818c59958b6859732a62c950b}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_3) from the physical slot (SlotRequestId{84cefd94b43f6a6248e1f20e653d4c4c})
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-11, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{84cefd94b43f6a6248e1f20e653d4c4c})
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{84cefd94b43f6a6248e1f20e653d4c4c}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 9a4082def1b1b0bcee5a7e41d3260954.
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 5887c8a090110fb517bd28f83ae5adae.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 3056d026b02ea37b41a89e991d3f28af.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Attempting to cancel task Source: Custom Source -> Sink: Print to Std. Out (10/12)#0 (f399f08cf7a53825b3b9bb856f373bed).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (10/12)#0 (f399f08cf7a53825b3b9bb856f373bed) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Triggering cancellation of task code Source: Custom Source -> Sink: Print to Std. Out (10/12)#0 (f399f08cf7a53825b3b9bb856f373bed).
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{84cefd94b43f6a6248e1f20e653d4c4c})
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-3, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 470 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-3, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-3, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Attempting to cancel task Source: Custom Source -> Sink: Print to Std. Out (11/12)#0 (a0910cf42ee2bac4943779977f14540d).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (11/12)#0 (a0910cf42ee2bac4943779977f14540d) switched from RUNNING to CANCELING.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Triggering cancellation of task code Source: Custom Source -> Sink: Print to Std. Out (11/12)#0 (a0910cf42ee2bac4943779977f14540d).
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 9a4082def1b1b0bcee5a7e41d3260954 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 9 to any pending request.
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Attempting to cancel task Source: Custom Source -> Sink: Print to Std. Out (12/12)#0 (bf3aac64ee71a5610ab9150115ac4d32).
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (12/12)#0 (bf3aac64ee71a5610ab9150115ac4d32) switched from RUNNING to CANCELING.
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-4, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=10}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Triggering cancellation of task code Source: Custom Source -> Sink: Print to Std. Out (12/12)#0 (bf3aac64ee71a5610ab9150115ac4d32).
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-4, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-4, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-11, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 467 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-5, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=10}]
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-5, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-5, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor]Closing the mailbox dropped mails [poison mail].
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Discarding the results produced by task execution d8149ae01eed56e8b9927f11ed83d7e3.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (6/12)#0 (9b8faab8eafbf4438fd7b6add185c67d) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (6/12)#0 (9b8faab8eafbf4438fd7b6add185c67d).
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (6/12)#0 network resources (state: CANCELED).
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (6/12)#0 (9b8faab8eafbf4438fd7b6add185c67d) [CANCELED]
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--1.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--1.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--1.latency
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state CANCELED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (6/12)#0 9b8faab8eafbf4438fd7b6add185c67d.
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor]Closing the mailbox dropped mails [poison mail].
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (7/12)#0 (669b56cc0110f5d2e32d3c0b2464f243) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.latency
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (7/12)#0 (669b56cc0110f5d2e32d3c0b2464f243).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (7/12)#0 network resources (state: CANCELED).
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-6, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (7/12)#0 (669b56cc0110f5d2e32d3c0b2464f243) [CANCELED]
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor]Closing the mailbox dropped mails [Report throwable org.apache.flink.streaming.connectors.kafka.internals.Handover$ClosedException].
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state CANCELED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (7/12)#0 669b56cc0110f5d2e32d3c0b2464f243.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor]Closing the mailbox dropped mails [poison mail].
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/12)#0 (47bb4435bf28a6681f59d27bb7d24bf3) switched from CANCELING to CANCELED.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (1/12)#0 (47bb4435bf28a6681f59d27bb7d24bf3).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (1/12)#0 network resources (state: CANCELED).
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (5/12)#0 (ef069e440cba9080ec960c9a59f82421) switched from CANCELING to CANCELED.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (5/12)#0 (ef069e440cba9080ec960c9a59f82421).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (5/12)#0 network resources (state: CANCELED).
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (1/12)#0 (47bb4435bf28a6681f59d27bb7d24bf3) [CANCELED]
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (5/12)#0 (ef069e440cba9080ec960c9a59f82421) [CANCELED]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (8/12) (5887c8a090110fb517bd28f83ae5adae) switched from CANCELING to CANCELED.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state CANCELED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (1/12)#0 47bb4435bf28a6681f59d27bb7d24bf3.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state CANCELED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (5/12)#0 ef069e440cba9080ec960c9a59f82421.
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (8/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-3, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor]Closing the mailbox dropped mails [poison mail].
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (10/12)#0 (f399f08cf7a53825b3b9bb856f373bed) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (10/12)#0 (f399f08cf7a53825b3b9bb856f373bed).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (10/12)#0 network resources (state: CANCELED).
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{16d835f1b2752556bdd4799016e47934}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_7) from the physical slot (SlotRequestId{ae0ae9b0505b5dbd2979519bc8d474eb})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{ae0ae9b0505b5dbd2979519bc8d474eb})
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (10/12)#0 (f399f08cf7a53825b3b9bb856f373bed) [CANCELED]
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{ae0ae9b0505b5dbd2979519bc8d474eb}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot ab79687eef7bbc363f0b3663ed667539.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{ae0ae9b0505b5dbd2979519bc8d474eb})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot ab79687eef7bbc363f0b3663ed667539 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 1 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=9}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state CANCELED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (10/12)#0 f399f08cf7a53825b3b9bb856f373bed.
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Discarding the results produced by task execution 5887c8a090110fb517bd28f83ae5adae.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=9}]
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 471 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 468 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-4, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor]Closing the mailbox dropped mails [poison mail].
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (11/12)#0 (a0910cf42ee2bac4943779977f14540d) switched from CANCELING to CANCELED.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (11/12)#0 (a0910cf42ee2bac4943779977f14540d).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (11/12)#0 network resources (state: CANCELED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (11/12)#0 (a0910cf42ee2bac4943779977f14540d) [CANCELED]
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-5, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor]Closing the mailbox dropped mails [Report throwable org.apache.flink.streaming.connectors.kafka.internals.Handover$ClosedException].
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (12/12)#0 (bf3aac64ee71a5610ab9150115ac4d32) switched from CANCELING to CANCELED.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (12/12)#0 (bf3aac64ee71a5610ab9150115ac4d32).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (12/12)#0 network resources (state: CANCELED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (12/12)#0 (bf3aac64ee71a5610ab9150115ac4d32) [CANCELED]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state CANCELED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (11/12)#0 a0910cf42ee2bac4943779977f14540d.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state CANCELED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (12/12)#0 bf3aac64ee71a5610ab9150115ac4d32.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (9/12) (3056d026b02ea37b41a89e991d3f28af) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (9/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{16026ebf8a53e708a16d989a06213212}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_8) from the physical slot (SlotRequestId{e7e912dec25120e3d655e49ce9a0267a})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{e7e912dec25120e3d655e49ce9a0267a})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{e7e912dec25120e3d655e49ce9a0267a}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 4974421c5ae7ae524c00c9c5aaa6c7f3.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{e7e912dec25120e3d655e49ce9a0267a})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 4974421c5ae7ae524c00c9c5aaa6c7f3 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 2 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=8}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Discarding the results produced by task execution 3056d026b02ea37b41a89e991d3f28af.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=8}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (3/12) (1ea5faa3f057f4dd552762cd257b4ee6) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (3/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{c63ad8579e8bafa86e137e50cefd6c20}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_2) from the physical slot (SlotRequestId{aac48b3f9d2439621632502c447ff7d1})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{aac48b3f9d2439621632502c447ff7d1})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{aac48b3f9d2439621632502c447ff7d1}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot ebd18bb5f5ba652ae4617df2cccdffdd.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{aac48b3f9d2439621632502c447ff7d1})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot ebd18bb5f5ba652ae4617df2cccdffdd @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 10 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=7}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Discarding the results produced by task execution 1ea5faa3f057f4dd552762cd257b4ee6.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=7}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 1ea5faa3f057f4dd552762cd257b4ee6.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution d8149ae01eed56e8b9927f11ed83d7e3.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 5887c8a090110fb517bd28f83ae5adae.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 3056d026b02ea37b41a89e991d3f28af.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (6/12) (9b8faab8eafbf4438fd7b6add185c67d) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (6/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{91a734d16c10ec4616eae254f347e7d0}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_5) from the physical slot (SlotRequestId{d2625758151b238ff94ae653c3acaa13})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{d2625758151b238ff94ae653c3acaa13})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{d2625758151b238ff94ae653c3acaa13}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot dcfacd4844dd5c6d8c9a863a0cbd1cd4.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{d2625758151b238ff94ae653c3acaa13})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot dcfacd4844dd5c6d8c9a863a0cbd1cd4 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 5 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (7/12) (669b56cc0110f5d2e32d3c0b2464f243) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (7/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{3b6d82a109add93038dce66cf729cdd2}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_6) from the physical slot (SlotRequestId{fdc6b5a90142a9256824c799f79ab5a2})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{fdc6b5a90142a9256824c799f79ab5a2})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{fdc6b5a90142a9256824c799f79ab5a2}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 8867acf297bcabcb3084ff9a8309a069.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{fdc6b5a90142a9256824c799f79ab5a2})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 8867acf297bcabcb3084ff9a8309a069 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 6 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/12) (47bb4435bf28a6681f59d27bb7d24bf3) switched from CANCELING to CANCELED.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (1/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{28165e85ba5ef0976e832ba73933977b}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{26900903ab24301cc1e050a5e18702c7})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{26900903ab24301cc1e050a5e18702c7})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{26900903ab24301cc1e050a5e18702c7}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 51fc14e9cab63149ad7c1c7533527094.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{26900903ab24301cc1e050a5e18702c7})
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 51fc14e9cab63149ad7c1c7533527094 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 8 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (5/12) (ef069e440cba9080ec960c9a59f82421) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (5/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{228198fb3d0aadab7eba73fdaeb5fd15}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_4) from the physical slot (SlotRequestId{f4dffc7fd6728490849dffd8840a7977})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{f4dffc7fd6728490849dffd8840a7977})
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{f4dffc7fd6728490849dffd8840a7977}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot c1fe1a4b4e19bc1e160c1d59723a6c16.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{f4dffc7fd6728490849dffd8840a7977})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot c1fe1a4b4e19bc1e160c1d59723a6c16 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (10/12) (f399f08cf7a53825b3b9bb856f373bed) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (10/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{ce2c53fe712a627a6739a27e9491fb0d}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_9) from the physical slot (SlotRequestId{9dd8905f29cfa9cc626409b468dfe31d})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{9dd8905f29cfa9cc626409b468dfe31d})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{9dd8905f29cfa9cc626409b468dfe31d}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot a1ad30c752ff09633d98835051669cc6.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{9dd8905f29cfa9cc626409b468dfe31d})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot a1ad30c752ff09633d98835051669cc6 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 11 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (11/12) (a0910cf42ee2bac4943779977f14540d) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (11/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{68f17e0dc8a662441a3a06c28bf4ddcc}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_10) from the physical slot (SlotRequestId{f34ac388f1e057cbff6e394cd9f12268})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{f34ac388f1e057cbff6e394cd9f12268})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{f34ac388f1e057cbff6e394cd9f12268}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 07e9af6d5b5148015941b6307aedf0c7.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{f34ac388f1e057cbff6e394cd9f12268})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 07e9af6d5b5148015941b6307aedf0c7 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 3 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d79b95531fbe9f65ceab757e8a11ef55: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (12/12) (bf3aac64ee71a5610ab9150115ac4d32) switched from CANCELING to CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (12/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{6cf23149dce01fe3269243ffe9844776}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_11) from the physical slot (SlotRequestId{40cda26bfe4192dd1946f1a7f2484f32})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{40cda26bfe4192dd1946f1a7f2484f32})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{40cda26bfe4192dd1946f1a7f2484f32}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 7b123f39d1fc43864955926c101ed199.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{40cda26bfe4192dd1946f1a7f2484f32})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 7b123f39d1fc43864955926c101ed199 @ 0e7932e1-68ef-4ee6-a3e7-783f00abf175 @ localhost (dataPort=-1) - 7 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d79b95531fbe9f65ceab757e8a11ef55.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=12}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job d79b95531fbe9f65ceab757e8a11ef55
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (d79b95531fbe9f65ceab757e8a11ef55) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d79b95531fbe9f65ceab757e8a11ef55: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}}]
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph d79b95531fbe9f65ceab757e8a11ef55 reached terminal state FAILED.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job d79b95531fbe9f65ceab757e8a11ef55.
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 472 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 469 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 1ea5faa3f057f4dd552762cd257b4ee6.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution d8149ae01eed56e8b9927f11ed83d7e3.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 5887c8a090110fb517bd28f83ae5adae.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 3056d026b02ea37b41a89e991d3f28af.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job d79b95531fbe9f65ceab757e8a11ef55 under leader id ae2a290c-3dbf-414a-b964-cb551ff58228 reached a globally terminal state FAILED.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 1ea5faa3f057f4dd552762cd257b4ee6.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job d79b95531fbe9f65ceab757e8a11ef55.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution d8149ae01eed56e8b9927f11ed83d7e3.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 5887c8a090110fb517bd28f83ae5adae.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 3056d026b02ea37b41a89e991d3f28af.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job d79b95531fbe9f65ceab757e8a11ef55 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job d79b95531fbe9f65ceab757e8a11ef55.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job d79b95531fbe9f65ceab757e8a11ef55 under leader id ae2a290c-3dbf-414a-b964-cb551ff58228.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (3/12) - execution #0 to FAILED while being CANCELED.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection cba1dced94776128d76ca029e7a35351.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (4/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (8/12) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source -> Sink: Print to Std. Out (9/12) - execution #0 to FAILED while being CANCELED.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection 0e7932e1-68ef-4ee6-a3e7-783f00abf175 because: The TaskExecutor is shutting down.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor 20f9d4f6f5381a4322c52a52227778ed from the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job d79b95531fbe9f65ceab757e8a11ef55.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_1
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_2
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_3
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_4
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_5
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_6
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_7
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_8
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_9
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_10
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_11
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor 0e7932e1-68ef-4ee6-a3e7-783f00abf175 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [51fc14e9cab63149ad7c1c7533527094].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:8, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 51fc14e9cab63149ad7c1c7533527094, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job d79b95531fbe9f65ceab757e8a11ef55.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor 0e7932e1-68ef-4ee6-a3e7-783f00abf175.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [f2135d08c7d2ce5dcb772dabb572c26c].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Not replacing existing epoch 49 with new epoch 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 4 to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Not replacing existing epoch 56 with new epoch 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:4, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: f2135d08c7d2ce5dcb772dabb572c26c, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updating last seen epoch from 2 to 2 for partition mem.used-3
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-13, groupId=metric_consumer_g] Updated cluster metadata updateVersion 470 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Updated cluster metadata updateVersion 473 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:10, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: ebd18bb5f5ba652ae4617df2cccdffdd, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [ebd18bb5f5ba652ae4617df2cccdffdd].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:9, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 9a4082def1b1b0bcee5a7e41d3260954, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-18, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [9a4082def1b1b0bcee5a7e41d3260954].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: c1fe1a4b4e19bc1e160c1d59723a6c16, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [c1fe1a4b4e19bc1e160c1d59723a6c16].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [dcfacd4844dd5c6d8c9a863a0cbd1cd4].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:5, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: dcfacd4844dd5c6d8c9a863a0cbd1cd4, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [8867acf297bcabcb3084ff9a8309a069].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:6, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 8867acf297bcabcb3084ff9a8309a069, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [ab79687eef7bbc363f0b3663ed667539].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [4974421c5ae7ae524c00c9c5aaa6c7f3].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [a1ad30c752ff09633d98835051669cc6].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [07e9af6d5b5148015941b6307aedf0c7].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:1, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: ab79687eef7bbc363f0b3663ed667539, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [7b123f39d1fc43864955926c101ed199].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:2, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 4974421c5ae7ae524c00c9c5aaa6c7f3, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection cba1dced94776128d76ca029e7a35351.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:11, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: a1ad30c752ff09633d98835051669cc6, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:3, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 07e9af6d5b5148015941b6307aedf0c7, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:7, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 7b123f39d1fc43864955926c101ed199, jobId: d79b95531fbe9f65ceab757e8a11ef55).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job d79b95531fbe9f65ceab757e8a11ef55 from job leader id monitoring.
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager b964cb551ff58228ae2a290c3dbf414a@akka://flink/user/rpc/jobmanager_3 for job d79b95531fbe9f65ceab757e8a11ef55 from the resource manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job d79b95531fbe9f65ceab757e8a11ef55.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job d79b95531fbe9f65ceab757e8a11ef55.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 51fc14e9cab63149ad7c1c7533527094 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 51fc14e9cab63149ad7c1c7533527094.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 51fc14e9cab63149ad7c1c7533527094.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 51fc14e9cab63149ad7c1c7533527094.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-12][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id f2135d08c7d2ce5dcb772dabb572c26c because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id f2135d08c7d2ce5dcb772dabb572c26c.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for f2135d08c7d2ce5dcb772dabb572c26c.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id f2135d08c7d2ce5dcb772dabb572c26c.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id ebd18bb5f5ba652ae4617df2cccdffdd because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id ebd18bb5f5ba652ae4617df2cccdffdd.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for ebd18bb5f5ba652ae4617df2cccdffdd.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id ebd18bb5f5ba652ae4617df2cccdffdd.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 9a4082def1b1b0bcee5a7e41d3260954 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 9a4082def1b1b0bcee5a7e41d3260954.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 9a4082def1b1b0bcee5a7e41d3260954.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 9a4082def1b1b0bcee5a7e41d3260954.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id c1fe1a4b4e19bc1e160c1d59723a6c16 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id c1fe1a4b4e19bc1e160c1d59723a6c16.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for c1fe1a4b4e19bc1e160c1d59723a6c16.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id c1fe1a4b4e19bc1e160c1d59723a6c16.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id dcfacd4844dd5c6d8c9a863a0cbd1cd4 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job d79b95531fbe9f65ceab757e8a11ef55 has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id dcfacd4844dd5c6d8c9a863a0cbd1cd4.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for dcfacd4844dd5c6d8c9a863a0cbd1cd4.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id dcfacd4844dd5c6d8c9a863a0cbd1cd4.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job d79b95531fbe9f65ceab757e8a11ef55 under leader id ae2a290c-3dbf-414a-b964-cb551ff58228 has been terminated.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job d79b95531fbe9f65ceab757e8a11ef55. Address: null, leader id: null.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]JobManager for job d79b95531fbe9f65ceab757e8a11ef55 with leader id b964cb551ff58228ae2a290c3dbf414a lost leadership.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 8867acf297bcabcb3084ff9a8309a069 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 8867acf297bcabcb3084ff9a8309a069.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 8867acf297bcabcb3084ff9a8309a069.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 8867acf297bcabcb3084ff9a8309a069.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id ab79687eef7bbc363f0b3663ed667539 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id ab79687eef7bbc363f0b3663ed667539.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for ab79687eef7bbc363f0b3663ed667539.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id ab79687eef7bbc363f0b3663ed667539.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 4974421c5ae7ae524c00c9c5aaa6c7f3 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 4974421c5ae7ae524c00c9c5aaa6c7f3.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 4974421c5ae7ae524c00c9c5aaa6c7f3.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 4974421c5ae7ae524c00c9c5aaa6c7f3.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id a1ad30c752ff09633d98835051669cc6 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id a1ad30c752ff09633d98835051669cc6.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for a1ad30c752ff09633d98835051669cc6.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id a1ad30c752ff09633d98835051669cc6.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 07e9af6d5b5148015941b6307aedf0c7 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 07e9af6d5b5148015941b6307aedf0c7.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 07e9af6d5b5148015941b6307aedf0c7.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 07e9af6d5b5148015941b6307aedf0c7.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 7b123f39d1fc43864955926c101ed199 because: Stopping JobMaster for job metricStream(d79b95531fbe9f65ceab757e8a11ef55).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 7b123f39d1fc43864955926c101ed199.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 7b123f39d1fc43864955926c101ed199.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 7b123f39d1fc43864955926c101ed199.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-4ab3b0c9-9b34-43d3-bd63-ae23fc3783b4
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-e0a5ceb1-6823-401d-804e-778f98f6a96d
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-114c22ef-c5f8-48ff-a259-3a12b8194217
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-12][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:50753
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=2, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '7df19f87deec5680128845fd9a6ca18d' for node 'Sink: Print to Std. Out-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-30587c0b-3bfc-4f6a-bc58-55af3efebd4e
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:50796 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-ca4b15c4-adc7-4fd1-a588-393796060715
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-a5acb245-4a8c-4881-bec7-720a047d437d
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: 0177e34b-86b1-4435-9974-e376a8920f71
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 369 GB (80.22% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-5a21ec97-35bc-41ea-88b6-6de561612528 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-369fde5c-d609-4bb8-9fb2-3dbb6d9cf227 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-7ea3acfb-4e3e-417f-998a-861d5c28bf59
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2188 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:50797
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:50797
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:50797.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:50797 was granted leadership with leaderSessionID=4d501502-1ab5-48ef-a023-abf6b4dc0d0a
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:50797 , session=4d501502-1ab5-48ef-a023-abf6b4dc0d0a
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id 38fda7ed-11b6-455c-9277-dbf8c453f48c. Creating new DispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 94999095155a1e4dcccdfa629a5f4b8f
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=cccdfa62-9a5f-4b8f-9499-9095155a1e4d
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(94999095155a1e4dcccdfa629a5f4b8f).
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=38fda7ed-11b6-455c-9277-dbf8c453f48c
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission b7049debb1da4f7849a26005fc2653e5 (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job b7049debb1da4f7849a26005fc2653e5 (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID 0177e34b-86b1-4435-9974-e376a8920f71 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id da82ab06115ac04b44b6a4a6119b9698.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor 0177e34b-86b1-4435-9974-e376a8920f71 under da82ab06115ac04b44b6a4a6119b9698 at the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job b7049debb1da4f7849a26005fc2653e5.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under 9a7cb8d1-879e-4ccc-9e7e-c873c5e02031.
[DEBUG][22-06-12][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (b7049debb1da4f7849a26005fc2653e5).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (b7049debb1da4f7849a26005fc2653e5).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (b7049debb1da4f7849a26005fc2653e5).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 0 ms.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (b7049debb1da4f7849a26005fc2653e5).
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Sink: Print to Std. Out) to 0 predecessors.
[INFO][22-06-12][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 2 ms
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (b7049debb1da4f7849a26005fc2653e5).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6f2bdf30
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job b7049debb1da4f7849a26005fc2653e5 after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@2f98cbc8 for metricStream (b7049debb1da4f7849a26005fc2653e5).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job b7049debb1da4f7849a26005fc2653e5 under leader id 9a7cb8d1-879e-4ccc-9e7e-c873c5e02031.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership 9a7cb8d1-879e-4ccc-9e7e-c873c5e02031.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=9a7cb8d1-879e-4ccc-9e7e-c873c5e02031
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (b7049debb1da4f7849a26005fc2653e5) under job master id 9e7ec873c5e020319a7cb8d1879e4ccc.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (b7049debb1da4f7849a26005fc2653e5) switched from state CREATED to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (b87c69528efa0416efab513edfdc97c0) switched from CREATED to SCHEDULED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job b7049debb1da4f7849a26005fc2653e5.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{c43398b957d0fac8615a2c555b916ea7}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(94999095155a1e4dcccdfa629a5f4b8f)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job b7049debb1da4f7849a26005fc2653e5 to job leader id monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job b7049debb1da4f7849a26005fc2653e5 has a new job leader 9a7cb8d1-879e-4ccc-9e7e-c873c5e02031@akka://flink/user/rpc/jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager 9e7ec873c5e020319a7cb8d1879e4ccc@akka://flink/user/rpc/jobmanager_3 for job b7049debb1da4f7849a26005fc2653e5.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager 9e7ec873c5e020319a7cb8d1879e4ccc@akka://flink/user/rpc/jobmanager_3 for job b7049debb1da4f7849a26005fc2653e5.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: 94999095155a1e4dcccdfa629a5f4b8f.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job b7049debb1da4f7849a26005fc2653e5: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job b7049debb1da4f7849a26005fc2653e5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot 0177e34b-86b1-4435-9974-e376a8920f71_0 for job b7049debb1da4f7849a26005fc2653e5 with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request 6cf06915e7e4c9ecec68b823a2aa4651 for job b7049debb1da4f7849a26005fc2653e5 from resource manager with leader id 94999095155a1e4dcccdfa629a5f4b8f.
[DEBUG][22-06-12][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for 6cf06915e7e4c9ecec68b823a2aa4651.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job b7049debb1da4f7849a26005fc2653e5 for job leader monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job b7049debb1da4f7849a26005fc2653e5. Address: akka://flink/user/rpc/jobmanager_3, leader id: 9e7ec873c5e020319a7cb8d1879e4ccc.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 9a7cb8d1-879e-4ccc-9e7e-c873c5e02031.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job b7049debb1da4f7849a26005fc2653e5.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job b7049debb1da4f7849a26005fc2653e5.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job b7049debb1da4f7849a26005fc2653e5.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor 0177e34b-86b1-4435-9974-e376a8920f71 @ localhost (dataPort=-1).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer 6cf06915e7e4c9ecec68b823a2aa4651 to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot 6cf06915e7e4c9ecec68b823a2aa4651 @ 0177e34b-86b1-4435-9974-e376a8920f71 @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot 6cf06915e7e4c9ecec68b823a2aa4651 for slot request id SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id 6cf06915e7e4c9ecec68b823a2aa4651.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{c43398b957d0fac8615a2c555b916ea7}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a})
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (b87c69528efa0416efab513edfdc97c0) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id b87c69528efa0416efab513edfdc97c0 to 0177e34b-86b1-4435-9974-e376a8920f71 @ localhost (dataPort=-1) with allocation id 6cf06915e7e4c9ecec68b823a2aa4651
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 6cf06915e7e4c9ecec68b823a2aa4651.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id 6cf06915e7e4c9ecec68b823a2aa4651 for local state stores for job b7049debb1da4f7849a26005fc2653e5.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_6cf06915e7e4c9ecec68b823a2aa4651], jobID=b7049debb1da4f7849a26005fc2653e5, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for b7049debb1da4f7849a26005fc2653e5 - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id 6cf06915e7e4c9ecec68b823a2aa4651.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0), deploy into slot with allocation id 6cf06915e7e4c9ecec68b823a2aa4651.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0) switched from CREATED to DEPLOYING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0) [DEPLOYING]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0) [DEPLOYING].
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task b87c69528efa0416efab513edfdc97c0 at library cache manager took 1 milliseconds
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 6cf06915e7e4c9ecec68b823a2aa4651.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2ea3441c
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Sink: Print to Std. Out (1/1)#0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (b87c69528efa0416efab513edfdc97c0) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Sink: Print to Std. Out (1/1)#0
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-throttle-time
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name heartbeat-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name join-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name sync-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name rebalance-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-rebalance
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name commit-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-revoked-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-assigned-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-lost-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lag
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lead
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name time-between-poll
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name poll-idle-ratio-avg
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.0
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: 77a89fcf8d7fa018
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655005629360
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker1:9092 (id: -1 rack: null) using address worker1/47.104.89.120
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--1.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--1.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--1.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -1
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Completed connection to node -1. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating API versions fetch from node -1.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Recorded API versions for node -1: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1:9092 (id: -1 rack: null)
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[], controller=worker1.cluster:9092 (id: 1 rack: null)}
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 will start reading the following 12 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic='mem.used', partition=10}, KafkaTopicPartition{topic='mem.used', partition=11}, KafkaTopicPartition{topic='mem.used', partition=8}, KafkaTopicPartition{topic='mem.used', partition=9}, KafkaTopicPartition{topic='mem.used', partition=6}, KafkaTopicPartition{topic='mem.used', partition=7}, KafkaTopicPartition{topic='mem.used', partition=4}, KafkaTopicPartition{topic='mem.used', partition=5}, KafkaTopicPartition{topic='mem.used', partition=2}, KafkaTopicPartition{topic='mem.used', partition=3}, KafkaTopicPartition{topic='mem.used', partition=0}, KafkaTopicPartition{topic='mem.used', partition=1}]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (b87c69528efa0416efab513edfdc97c0) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='mem.used', partition=10}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=11}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=8}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=9}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=6}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=7}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=4}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=5}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=2}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=3}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=0}=-915623761773, KafkaTopicPartition{topic='mem.used', partition=1}=-915623761773}.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-throttle-time
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name heartbeat-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name join-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name sync-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name rebalance-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-rebalance
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name commit-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-revoked-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-assigned-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-lost-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lag
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lead
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name time-between-poll
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name poll-idle-ratio-avg
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.0
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: 77a89fcf8d7fa018
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655005629830
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer initialized
[INFO][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Subscribed to partition(s): mem.used-10, mem.used-11, mem.used-8, mem.used-9, mem.used-6, mem.used-7, mem.used-4, mem.used-5, mem.used-2, mem.used-3, mem.used-0, mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending FindCoordinator request to broker worker3:9092 (id: -3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker3:9092 (id: -3 rack: null) using address worker3/47.104.133.128
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -3
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Completed connection to node -3. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating API versions fetch from node -3.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Recorded API versions for node -3: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='mem.used')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3:9092 (id: -3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 2 for partition mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 4 for partition mem.used-5
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 2 for partition mem.used-10
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 4 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 4 for partition mem.used-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 2 for partition mem.used-9
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 4 for partition mem.used-11
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 2 for partition mem.used-4
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 2 for partition mem.used-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 2 for partition mem.used-6
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 2 for partition mem.used-7
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 2 for partition mem.used-3
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 3, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 2, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 1, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 7, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 6, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 5, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 4, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 11, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 10, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 9, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), epoch=2}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = mem.used, partition = 8, leader = 3, replicas = [3], isr = [3], offlineReplicas = []), epoch=4}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Received FindCoordinator response ClientResponse(receivedTimeMs=1655005629997, latencyMs=128, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=3, clientId=consumer-metric_consumer_g-2, correlationId=0), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='NONE', nodeId=1, host='worker1.cluster', port=9092))
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Discovered group coordinator worker1.cluster:9092 (id: 2147483646 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker1.cluster:9092 (id: 2147483646 rack: null) using address worker1.cluster/47.104.89.120
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Fetching committed offsets for partitions: [mem.used-3, mem.used-2, mem.used-1, mem.used-0, mem.used-7, mem.used-6, mem.used-5, mem.used-4, mem.used-11, mem.used-10, mem.used-9, mem.used-8]
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-2147483646.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-2147483646.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-2147483646.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node 2147483646
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Completed connection to node 2147483646. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating API versions fetch from node 2147483646.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Recorded API versions for node 2147483646: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-3 to the committed offset FetchPosition{offset=2034, offsetEpoch=Optional[49], currentLeader=LeaderAndEpoch{leader=worker1.cluster:9092 (id: 1 rack: null), epoch=2}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 2 to 49 for partition mem.used-3
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-2 to the committed offset FetchPosition{offset=2627, offsetEpoch=Optional[54], currentLeader=LeaderAndEpoch{leader=worker3.cluster:9092 (id: 3 rack: null), epoch=4}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 4 to 54 for partition mem.used-2
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-1 to the committed offset FetchPosition{offset=1605, offsetEpoch=Optional[47], currentLeader=LeaderAndEpoch{leader=worker2cluster:9092 (id: 2 rack: null), epoch=2}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 2 to 47 for partition mem.used-1
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-0 to the committed offset FetchPosition{offset=831, offsetEpoch=Optional[50], currentLeader=LeaderAndEpoch{leader=worker1.cluster:9092 (id: 1 rack: null), epoch=2}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 2 to 50 for partition mem.used-0
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-7 to the committed offset FetchPosition{offset=1216, offsetEpoch=Optional[46], currentLeader=LeaderAndEpoch{leader=worker2cluster:9092 (id: 2 rack: null), epoch=2}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 2 to 46 for partition mem.used-7
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-6 to the committed offset FetchPosition{offset=1130, offsetEpoch=Optional[49], currentLeader=LeaderAndEpoch{leader=worker1.cluster:9092 (id: 1 rack: null), epoch=2}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 2 to 49 for partition mem.used-6
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-5 to the committed offset FetchPosition{offset=1877, offsetEpoch=Optional[54], currentLeader=LeaderAndEpoch{leader=worker3.cluster:9092 (id: 3 rack: null), epoch=4}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 4 to 54 for partition mem.used-5
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-4 to the committed offset FetchPosition{offset=2206, offsetEpoch=Optional[46], currentLeader=LeaderAndEpoch{leader=worker2cluster:9092 (id: 2 rack: null), epoch=2}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 2 to 46 for partition mem.used-4
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-11 to the committed offset FetchPosition{offset=837, offsetEpoch=Optional[56], currentLeader=LeaderAndEpoch{leader=worker3.cluster:9092 (id: 3 rack: null), epoch=4}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 4 to 56 for partition mem.used-11
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-10 to the committed offset FetchPosition{offset=2435, offsetEpoch=Optional[46], currentLeader=LeaderAndEpoch{leader=worker2cluster:9092 (id: 2 rack: null), epoch=2}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 2 to 46 for partition mem.used-10
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-9 to the committed offset FetchPosition{offset=1308, offsetEpoch=Optional[49], currentLeader=LeaderAndEpoch{leader=worker1.cluster:9092 (id: 1 rack: null), epoch=2}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 2 to 49 for partition mem.used-9
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Setting offset for partition mem.used-8 to the committed offset FetchPosition{offset=2451, offsetEpoch=Optional[55], currentLeader=LeaderAndEpoch{leader=worker3.cluster:9092 (id: 3 rack: null), epoch=4}}
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 4 to 55 for partition mem.used-8
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initialize connection to node worker2cluster:9092 (id: 2 rack: null) for sending metadata request
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker2cluster:9092 (id: 2 rack: null) using address worker2cluster/47.104.108.98
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance da82ab06115ac04b44b6a4a6119b9698: SlotReport{SlotStatus{slotID=0177e34b-86b1-4435-9974-e376a8920f71_0, allocationID=6cf06915e7e4c9ecec68b823a2aa4651, jobID=b7049debb1da4f7849a26005fc2653e5, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0177e34b-86b1-4435-9974-e376a8920f71: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance da82ab06115ac04b44b6a4a6119b9698: SlotReport{SlotStatus{slotID=0177e34b-86b1-4435-9974-e376a8920f71_0, allocationID=6cf06915e7e4c9ecec68b823a2aa4651, jobID=b7049debb1da4f7849a26005fc2653e5, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0177e34b-86b1-4435-9974-e376a8920f71: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance da82ab06115ac04b44b6a4a6119b9698: SlotReport{SlotStatus{slotID=0177e34b-86b1-4435-9974-e376a8920f71_0, allocationID=6cf06915e7e4c9ecec68b823a2aa4651, jobID=b7049debb1da4f7849a26005fc2653e5, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0177e34b-86b1-4435-9974-e376a8920f71: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance da82ab06115ac04b44b6a4a6119b9698: SlotReport{SlotStatus{slotID=0177e34b-86b1-4435-9974-e376a8920f71_0, allocationID=6cf06915e7e4c9ecec68b823a2aa4651, jobID=b7049debb1da4f7849a26005fc2653e5, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0177e34b-86b1-4435-9974-e376a8920f71: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance da82ab06115ac04b44b6a4a6119b9698: SlotReport{SlotStatus{slotID=0177e34b-86b1-4435-9974-e376a8920f71_0, allocationID=6cf06915e7e4c9ecec68b823a2aa4651, jobID=b7049debb1da4f7849a26005fc2653e5, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0177e34b-86b1-4435-9974-e376a8920f71: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fbddab754f1cb90b6dd98e3ae2609622.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance da82ab06115ac04b44b6a4a6119b9698: SlotReport{SlotStatus{slotID=0177e34b-86b1-4435-9974-e376a8920f71_0, allocationID=6cf06915e7e4c9ecec68b823a2aa4651, jobID=b7049debb1da4f7849a26005fc2653e5, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0177e34b-86b1-4435-9974-e376a8920f71: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5c8e286601bfb26a630964cdfd202daf.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0177e34b-86b1-4435-9974-e376a8920f71.
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--1.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--1.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--1.latency
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0) switched from RUNNING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-10 could be determined

[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (b87c69528efa0416efab513edfdc97c0) [FAILED]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 b87c69528efa0416efab513edfdc97c0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (b87c69528efa0416efab513edfdc97c0) switched from RUNNING to FAILED on 0177e34b-86b1-4435-9974-e376a8920f71 @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-10 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{c43398b957d0fac8615a2c555b916ea7}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 6cf06915e7e4c9ecec68b823a2aa4651.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{a2d0d9505c09767ce4d61ee7a2ac4a9a})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 6cf06915e7e4c9ecec68b823a2aa4651 @ 0177e34b-86b1-4435-9974-e376a8920f71 @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job b7049debb1da4f7849a26005fc2653e5.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job b7049debb1da4f7849a26005fc2653e5
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job b7049debb1da4f7849a26005fc2653e5: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]1 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0. 
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (b7049debb1da4f7849a26005fc2653e5) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-10 could be determined
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (b7049debb1da4f7849a26005fc2653e5) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-10 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph b7049debb1da4f7849a26005fc2653e5 reached terminal state FAILED.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job b7049debb1da4f7849a26005fc2653e5.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-10 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job b7049debb1da4f7849a26005fc2653e5 under leader id 9a7cb8d1-879e-4ccc-9e7e-c873c5e02031 reached a globally terminal state FAILED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job b7049debb1da4f7849a26005fc2653e5.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job b7049debb1da4f7849a26005fc2653e5 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-10 could be determined
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job b7049debb1da4f7849a26005fc2653e5.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job b7049debb1da4f7849a26005fc2653e5 under leader id 9a7cb8d1-879e-4ccc-9e7e-c873c5e02031.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection fbddab754f1cb90b6dd98e3ae2609622.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection 0177e34b-86b1-4435-9974-e376a8920f71 because: The TaskExecutor is shutting down.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor da82ab06115ac04b44b6a4a6119b9698 from the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(b7049debb1da4f7849a26005fc2653e5).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job b7049debb1da4f7849a26005fc2653e5.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job b7049debb1da4f7849a26005fc2653e5.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor 0177e34b-86b1-4435-9974-e376a8920f71.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor 0177e34b-86b1-4435-9974-e376a8920f71 because: Stopping JobMaster for job metricStream(b7049debb1da4f7849a26005fc2653e5).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [6cf06915e7e4c9ecec68b823a2aa4651].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(b7049debb1da4f7849a26005fc2653e5).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection fbddab754f1cb90b6dd98e3ae2609622.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(b7049debb1da4f7849a26005fc2653e5).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: 6cf06915e7e4c9ecec68b823a2aa4651, jobId: b7049debb1da4f7849a26005fc2653e5).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job b7049debb1da4f7849a26005fc2653e5 from job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager 9e7ec873c5e020319a7cb8d1879e4ccc@akka://flink/user/rpc/jobmanager_3 for job b7049debb1da4f7849a26005fc2653e5 from the resource manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job b7049debb1da4f7849a26005fc2653e5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job b7049debb1da4f7849a26005fc2653e5.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 6cf06915e7e4c9ecec68b823a2aa4651 because: Stopping JobMaster for job metricStream(b7049debb1da4f7849a26005fc2653e5).
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 6cf06915e7e4c9ecec68b823a2aa4651.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 6cf06915e7e4c9ecec68b823a2aa4651.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 6cf06915e7e4c9ecec68b823a2aa4651.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job b7049debb1da4f7849a26005fc2653e5 has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job b7049debb1da4f7849a26005fc2653e5 under leader id 9a7cb8d1-879e-4ccc-9e7e-c873c5e02031 has been terminated.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-5a21ec97-35bc-41ea-88b6-6de561612528
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-369fde5c-d609-4bb8-9fb2-3dbb6d9cf227
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-7ea3acfb-4e3e-417f-998a-861d5c28bf59
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-12][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-12][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:50796
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=2, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '7df19f87deec5680128845fd9a6ca18d' for node 'Sink: Print to Std. Out-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-08b4c296-8f9d-462b-ae1a-f21d4597de30
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:51428 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-a43cdcf1-3bb2-45e8-b39f-6197e95a12be
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-ac57ee2e-13f8-4c0d-8759-4bf99bb3880d
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: 0cf19b6e-fb78-490d-b756-1245df0085a1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 369 GB (80.22% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-eae04a09-828d-4f58-8074-98feda92dfcf for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-0a371fc1-4450-4cbe-9174-7a783f04e025 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-8cd14e66-475e-43de-925a-295a08520bec
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2403 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:51429
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:51429
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:51429.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:51429 was granted leadership with leaderSessionID=0f9d6f46-79b5-4e51-a0e5-fd2583a22ae8
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:51429 , session=0f9d6f46-79b5-4e51-a0e5-fd2583a22ae8
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id 1f9f878b-ef39-490c-9001-58b04650c399. Creating new DispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 91f3631cee97c7db209107ef8ff8484c
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=209107ef-8ff8-484c-91f3-631cee97c7db
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(91f3631cee97c7db209107ef8ff8484c).
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=1f9f878b-ef39-490c-9001-58b04650c399
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID 0cf19b6e-fb78-490d-b756-1245df0085a1 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission 0bbd621326176d4e2d78378c145b719d (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job 0bbd621326176d4e2d78378c145b719d (metricStream).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 4ff6ce7c0c2914a9ee394793932a8385.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor 0cf19b6e-fb78-490d-b756-1245df0085a1 under 4ff6ce7c0c2914a9ee394793932a8385 at the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job 0bbd621326176d4e2d78378c145b719d.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under 5d5876eb-d4b1-48ff-8251-3309d9df74fc.
[DEBUG][22-06-12][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (0bbd621326176d4e2d78378c145b719d).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (0bbd621326176d4e2d78378c145b719d).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (0bbd621326176d4e2d78378c145b719d).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 0 ms.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (0bbd621326176d4e2d78378c145b719d).
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Sink: Print to Std. Out) to 0 predecessors.
[INFO][22-06-12][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 5 ms
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (0bbd621326176d4e2d78378c145b719d).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@121f7266
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job 0bbd621326176d4e2d78378c145b719d after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@2cbc102d for metricStream (0bbd621326176d4e2d78378c145b719d).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job 0bbd621326176d4e2d78378c145b719d under leader id 5d5876eb-d4b1-48ff-8251-3309d9df74fc.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership 5d5876eb-d4b1-48ff-8251-3309d9df74fc.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=5d5876eb-d4b1-48ff-8251-3309d9df74fc
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (0bbd621326176d4e2d78378c145b719d) under job master id 82513309d9df74fc5d5876ebd4b148ff.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (0bbd621326176d4e2d78378c145b719d) switched from state CREATED to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (c14f0a607fb18b2652c91e29d17b0469) switched from CREATED to SCHEDULED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{d4178b0c1e617f58ff2b74998222527b}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{d4178b0c1e617f58ff2b74998222527b} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 0bbd621326176d4e2d78378c145b719d.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{cc320565ceff3dfb7ecc818a05c31c99}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{d4178b0c1e617f58ff2b74998222527b})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(91f3631cee97c7db209107ef8ff8484c)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job 0bbd621326176d4e2d78378c145b719d to job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager 82513309d9df74fc5d5876ebd4b148ff@akka://flink/user/rpc/jobmanager_3 for job 0bbd621326176d4e2d78378c145b719d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job 0bbd621326176d4e2d78378c145b719d has a new job leader 5d5876eb-d4b1-48ff-8251-3309d9df74fc@akka://flink/user/rpc/jobmanager_3.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager 82513309d9df74fc5d5876ebd4b148ff@akka://flink/user/rpc/jobmanager_3 for job 0bbd621326176d4e2d78378c145b719d.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: 91f3631cee97c7db209107ef8ff8484c.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job 0bbd621326176d4e2d78378c145b719d: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 0bbd621326176d4e2d78378c145b719d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot 0cf19b6e-fb78-490d-b756-1245df0085a1_0 for job 0bbd621326176d4e2d78378c145b719d with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request f81599dd37f4257fe43c6183a3bf1112 for job 0bbd621326176d4e2d78378c145b719d from resource manager with leader id 91f3631cee97c7db209107ef8ff8484c.
[DEBUG][22-06-12][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for f81599dd37f4257fe43c6183a3bf1112.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job 0bbd621326176d4e2d78378c145b719d for job leader monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job 0bbd621326176d4e2d78378c145b719d. Address: akka://flink/user/rpc/jobmanager_3, leader id: 82513309d9df74fc5d5876ebd4b148ff.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 5d5876eb-d4b1-48ff-8251-3309d9df74fc.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 0bbd621326176d4e2d78378c145b719d.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job 0bbd621326176d4e2d78378c145b719d.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job 0bbd621326176d4e2d78378c145b719d.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor 0cf19b6e-fb78-490d-b756-1245df0085a1 @ localhost (dataPort=-1).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer f81599dd37f4257fe43c6183a3bf1112 to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot f81599dd37f4257fe43c6183a3bf1112 @ 0cf19b6e-fb78-490d-b756-1245df0085a1 @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{d4178b0c1e617f58ff2b74998222527b}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot f81599dd37f4257fe43c6183a3bf1112 for slot request id SlotRequestId{d4178b0c1e617f58ff2b74998222527b}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id f81599dd37f4257fe43c6183a3bf1112.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{cc320565ceff3dfb7ecc818a05c31c99}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{d4178b0c1e617f58ff2b74998222527b})
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (c14f0a607fb18b2652c91e29d17b0469) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id c14f0a607fb18b2652c91e29d17b0469 to 0cf19b6e-fb78-490d-b756-1245df0085a1 @ localhost (dataPort=-1) with allocation id f81599dd37f4257fe43c6183a3bf1112
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot f81599dd37f4257fe43c6183a3bf1112.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id f81599dd37f4257fe43c6183a3bf1112 for local state stores for job 0bbd621326176d4e2d78378c145b719d.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_f81599dd37f4257fe43c6183a3bf1112], jobID=0bbd621326176d4e2d78378c145b719d, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for 0bbd621326176d4e2d78378c145b719d - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id f81599dd37f4257fe43c6183a3bf1112.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c14f0a607fb18b2652c91e29d17b0469), deploy into slot with allocation id f81599dd37f4257fe43c6183a3bf1112.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c14f0a607fb18b2652c91e29d17b0469) switched from CREATED to DEPLOYING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c14f0a607fb18b2652c91e29d17b0469) [DEPLOYING]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c14f0a607fb18b2652c91e29d17b0469) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot f81599dd37f4257fe43c6183a3bf1112.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task c14f0a607fb18b2652c91e29d17b0469 at library cache manager took 0 milliseconds
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c14f0a607fb18b2652c91e29d17b0469) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@26e4db77
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c14f0a607fb18b2652c91e29d17b0469) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Sink: Print to Std. Out (1/1)#0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (c14f0a607fb18b2652c91e29d17b0469) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Sink: Print to Std. Out (1/1)#0
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-throttle-time
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name heartbeat-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name join-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name sync-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name rebalance-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-rebalance
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name commit-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-revoked-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-assigned-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-lost-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lag
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lead
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name time-between-poll
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name poll-idle-ratio-avg
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.0
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: 77a89fcf8d7fa018
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655006937073
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4ff6ce7c0c2914a9ee394793932a8385: SlotReport{SlotStatus{slotID=0cf19b6e-fb78-490d-b756-1245df0085a1_0, allocationID=f81599dd37f4257fe43c6183a3bf1112, jobID=0bbd621326176d4e2d78378c145b719d, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0cf19b6e-fb78-490d-b756-1245df0085a1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4ff6ce7c0c2914a9ee394793932a8385: SlotReport{SlotStatus{slotID=0cf19b6e-fb78-490d-b756-1245df0085a1_0, allocationID=f81599dd37f4257fe43c6183a3bf1112, jobID=0bbd621326176d4e2d78378c145b719d, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0cf19b6e-fb78-490d-b756-1245df0085a1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4ff6ce7c0c2914a9ee394793932a8385: SlotReport{SlotStatus{slotID=0cf19b6e-fb78-490d-b756-1245df0085a1_0, allocationID=f81599dd37f4257fe43c6183a3bf1112, jobID=0bbd621326176d4e2d78378c145b719d, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0cf19b6e-fb78-490d-b756-1245df0085a1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4ff6ce7c0c2914a9ee394793932a8385: SlotReport{SlotStatus{slotID=0cf19b6e-fb78-490d-b756-1245df0085a1_0, allocationID=f81599dd37f4257fe43c6183a3bf1112, jobID=0bbd621326176d4e2d78378c145b719d, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0cf19b6e-fb78-490d-b756-1245df0085a1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4ff6ce7c0c2914a9ee394793932a8385: SlotReport{SlotStatus{slotID=0cf19b6e-fb78-490d-b756-1245df0085a1_0, allocationID=f81599dd37f4257fe43c6183a3bf1112, jobID=0bbd621326176d4e2d78378c145b719d, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0cf19b6e-fb78-490d-b756-1245df0085a1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7b6225c86ed39bd17db8982968a3a187.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4ff6ce7c0c2914a9ee394793932a8385: SlotReport{SlotStatus{slotID=0cf19b6e-fb78-490d-b756-1245df0085a1_0, allocationID=f81599dd37f4257fe43c6183a3bf1112, jobID=0bbd621326176d4e2d78378c145b719d, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 0cf19b6e-fb78-490d-b756-1245df0085a1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 02e4f872e2d230c565dc9691b80c9a9f.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c14f0a607fb18b2652c91e29d17b0469) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c14f0a607fb18b2652c91e29d17b0469).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c14f0a607fb18b2652c91e29d17b0469) [FAILED]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 c14f0a607fb18b2652c91e29d17b0469.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (c14f0a607fb18b2652c91e29d17b0469) switched from INITIALIZING to FAILED on 0cf19b6e-fb78-490d-b756-1245df0085a1 @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{cc320565ceff3dfb7ecc818a05c31c99}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{d4178b0c1e617f58ff2b74998222527b})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{d4178b0c1e617f58ff2b74998222527b})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{d4178b0c1e617f58ff2b74998222527b}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot f81599dd37f4257fe43c6183a3bf1112.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{d4178b0c1e617f58ff2b74998222527b})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot f81599dd37f4257fe43c6183a3bf1112 @ 0cf19b6e-fb78-490d-b756-1245df0085a1 @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 0bbd621326176d4e2d78378c145b719d.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job 0bbd621326176d4e2d78378c145b719d
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job 0bbd621326176d4e2d78378c145b719d: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]1 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0. 
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (0bbd621326176d4e2d78378c145b719d) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (0bbd621326176d4e2d78378c145b719d) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph 0bbd621326176d4e2d78378c145b719d reached terminal state FAILED.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job 0bbd621326176d4e2d78378c145b719d.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job 0bbd621326176d4e2d78378c145b719d under leader id 5d5876eb-d4b1-48ff-8251-3309d9df74fc reached a globally terminal state FAILED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job 0bbd621326176d4e2d78378c145b719d.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job 0bbd621326176d4e2d78378c145b719d reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection 7b6225c86ed39bd17db8982968a3a187.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job 0bbd621326176d4e2d78378c145b719d.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job 0bbd621326176d4e2d78378c145b719d under leader id 5d5876eb-d4b1-48ff-8251-3309d9df74fc.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection 0cf19b6e-fb78-490d-b756-1245df0085a1 because: The TaskExecutor is shutting down.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor 4ff6ce7c0c2914a9ee394793932a8385 from the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 0bbd621326176d4e2d78378c145b719d.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor 0cf19b6e-fb78-490d-b756-1245df0085a1.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job 0bbd621326176d4e2d78378c145b719d.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(0bbd621326176d4e2d78378c145b719d).
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: f81599dd37f4257fe43c6183a3bf1112, jobId: 0bbd621326176d4e2d78378c145b719d).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor 0cf19b6e-fb78-490d-b756-1245df0085a1 because: Stopping JobMaster for job metricStream(0bbd621326176d4e2d78378c145b719d).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [f81599dd37f4257fe43c6183a3bf1112].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(0bbd621326176d4e2d78378c145b719d).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection 7b6225c86ed39bd17db8982968a3a187.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(0bbd621326176d4e2d78378c145b719d).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job 0bbd621326176d4e2d78378c145b719d from job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager 82513309d9df74fc5d5876ebd4b148ff@akka://flink/user/rpc/jobmanager_3 for job 0bbd621326176d4e2d78378c145b719d from the resource manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 0bbd621326176d4e2d78378c145b719d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 0bbd621326176d4e2d78378c145b719d.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-eae04a09-828d-4f58-8074-98feda92dfcf
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[INFO][22-06-12][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-0a371fc1-4450-4cbe-9174-7a783f04e025
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-8cd14e66-475e-43de-925a-295a08520bec
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job 0bbd621326176d4e2d78378c145b719d has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job 0bbd621326176d4e2d78378c145b719d under leader id 5d5876eb-d4b1-48ff-8251-3309d9df74fc has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-12][akka.event.EventStream]shutting down: StandardOutLogger
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [f81599dd37f4257fe43c6183a3bf1112] of registered TaskExecutor 0cf19b6e-fb78-490d-b756-1245df0085a1 failed. Discarding slot.
java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.freeSlot(org.apache.flink.runtime.clusterframework.types.AllocationID,java.lang.Throwable,org.apache.flink.api.common.time.Time) timed out.
	at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.freeSlot(RpcTaskManagerGateway.java:108)
	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlots(DefaultDeclarativeSlotPool.java:486)
	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeAndReleaseSlots(DefaultDeclarativeSlotPool.java:412)
	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlots(DefaultDeclarativeSlotPool.java:382)
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.internalReleaseTaskManager(DeclarativeSlotPoolService.java:249)
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.releaseTaskManager(DeclarativeSlotPoolService.java:230)
	at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:497)
	at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManagerResourceManagerConnections(JobMaster.java:942)
	at org.apache.flink.runtime.jobmaster.JobMaster.lambda$stopJobExecution$4(JobMaster.java:931)
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$runAfterwardsAsync$18(FutureUtils.java:687)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
	at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:217)
	at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:765)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:795)
	at java.util.concurrent.CompletableFuture.whenCompleteAsync(CompletableFuture.java:2163)
	at org.apache.flink.runtime.concurrent.FutureUtils.runAfterwardsAsync(FutureUtils.java:684)
	at org.apache.flink.runtime.concurrent.FutureUtils.runAfterwards(FutureUtils.java:651)
	at org.apache.flink.runtime.jobmaster.JobMaster.stopJobExecution(JobMaster.java:928)
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:398)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/taskmanager_0#-1862289003]] after [300000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.run(LightArrayRevolverScheduler.scala:337)
	at akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(LightArrayRevolverScheduler.scala:141)
	at akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(LightArrayRevolverScheduler.scala:140)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at akka.actor.LightArrayRevolverScheduler.close(LightArrayRevolverScheduler.scala:139)
	at akka.actor.ActorSystemImpl.stopScheduler(ActorSystem.scala:937)
	at akka.actor.ActorSystemImpl$$anonfun$liftedTree2$1$1.apply$mcV$sp(ActorSystem.scala:872)
	at akka.actor.ActorSystemImpl$$anonfun$liftedTree2$1$1.apply(ActorSystem.scala:872)
	at akka.actor.ActorSystemImpl$$anonfun$liftedTree2$1$1.apply(ActorSystem.scala:872)
	at akka.actor.ActorSystemImpl$$anon$3.run(ActorSystem.scala:892)
	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1068)
	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1068)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	... 4 more
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:51428
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=2, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '7df19f87deec5680128845fd9a6ca18d' for node 'Sink: Print to Std. Out-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-1afa8c66-0b12-42ee-a6b4-1b289aa63676
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:51867 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-958c5900-e441-4e06-a236-8a2d1e34f261
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-c5e1acb5-1626-4d02-bf3a-516668af7b49
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: 65c0ed27-10c3-43eb-ab89-601636468c8d
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 369 GB (80.22% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-f9ab9d92-c28c-4a18-9d3c-d44678bc8231 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-d70d5fcf-a6f2-4791-8453-d1b69d64f82f for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-396d9c01-5e3c-4d11-8994-50b7a91dc171
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2578 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:51868
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:51868
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:51868.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:51868 was granted leadership with leaderSessionID=8001e515-52b9-4b22-acf7-de0e73ed2af2
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:51868 , session=8001e515-52b9-4b22-acf7-de0e73ed2af2
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id b27581e6-7a4f-4e94-8630-2d6a8d0b8661. Creating new DispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token a7cbf2eeb105fc2b1117a51d252649f8
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=1117a51d-2526-49f8-a7cb-f2eeb105fc2b
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(a7cbf2eeb105fc2b1117a51d252649f8).
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=b27581e6-7a4f-4e94-8630-2d6a8d0b8661
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission d76aee723d47735ecd17fe9376eebe98 (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID 65c0ed27-10c3-43eb-ab89-601636468c8d (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job d76aee723d47735ecd17fe9376eebe98 (metricStream).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 087a11a3c94afcaa0bb5f7078ed13be7.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor 65c0ed27-10c3-43eb-ab89-601636468c8d under 087a11a3c94afcaa0bb5f7078ed13be7 at the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job d76aee723d47735ecd17fe9376eebe98.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under 8dff4167-37cc-4d4a-bc92-a95d750eac17.
[DEBUG][22-06-12][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (d76aee723d47735ecd17fe9376eebe98).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (d76aee723d47735ecd17fe9376eebe98).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (d76aee723d47735ecd17fe9376eebe98).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 0 ms.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (d76aee723d47735ecd17fe9376eebe98).
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Sink: Print to Std. Out) to 0 predecessors.
[INFO][22-06-12][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 2 ms
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (d76aee723d47735ecd17fe9376eebe98).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@984dd2b
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job d76aee723d47735ecd17fe9376eebe98 after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@71a699a8 for metricStream (d76aee723d47735ecd17fe9376eebe98).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job d76aee723d47735ecd17fe9376eebe98 under leader id 8dff4167-37cc-4d4a-bc92-a95d750eac17.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership 8dff4167-37cc-4d4a-bc92-a95d750eac17.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=8dff4167-37cc-4d4a-bc92-a95d750eac17
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (d76aee723d47735ecd17fe9376eebe98) under job master id bc92a95d750eac178dff416737cc4d4a.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (d76aee723d47735ecd17fe9376eebe98) switched from state CREATED to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (c1411805fa2ffd56df34cb079c86f88b) switched from CREATED to SCHEDULED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{45a581d25e3edf13533530fca3b66abb}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{45a581d25e3edf13533530fca3b66abb} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d76aee723d47735ecd17fe9376eebe98.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{5648160be157f6e6decceb04b246986e}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{45a581d25e3edf13533530fca3b66abb})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(a7cbf2eeb105fc2b1117a51d252649f8)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job d76aee723d47735ecd17fe9376eebe98 to job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager bc92a95d750eac178dff416737cc4d4a@akka://flink/user/rpc/jobmanager_3 for job d76aee723d47735ecd17fe9376eebe98.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job d76aee723d47735ecd17fe9376eebe98 has a new job leader 8dff4167-37cc-4d4a-bc92-a95d750eac17@akka://flink/user/rpc/jobmanager_3.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager bc92a95d750eac178dff416737cc4d4a@akka://flink/user/rpc/jobmanager_3 for job d76aee723d47735ecd17fe9376eebe98.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: a7cbf2eeb105fc2b1117a51d252649f8.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job d76aee723d47735ecd17fe9376eebe98: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job d76aee723d47735ecd17fe9376eebe98.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot 65c0ed27-10c3-43eb-ab89-601636468c8d_0 for job d76aee723d47735ecd17fe9376eebe98 with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request 4afa3a5e55b78a948538caceb02d538e for job d76aee723d47735ecd17fe9376eebe98 from resource manager with leader id a7cbf2eeb105fc2b1117a51d252649f8.
[DEBUG][22-06-12][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for 4afa3a5e55b78a948538caceb02d538e.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job d76aee723d47735ecd17fe9376eebe98 for job leader monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job d76aee723d47735ecd17fe9376eebe98. Address: akka://flink/user/rpc/jobmanager_3, leader id: bc92a95d750eac178dff416737cc4d4a.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 8dff4167-37cc-4d4a-bc92-a95d750eac17.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job d76aee723d47735ecd17fe9376eebe98.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job d76aee723d47735ecd17fe9376eebe98.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job d76aee723d47735ecd17fe9376eebe98.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor 65c0ed27-10c3-43eb-ab89-601636468c8d @ localhost (dataPort=-1).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer 4afa3a5e55b78a948538caceb02d538e to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot 4afa3a5e55b78a948538caceb02d538e @ 65c0ed27-10c3-43eb-ab89-601636468c8d @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{45a581d25e3edf13533530fca3b66abb}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot 4afa3a5e55b78a948538caceb02d538e for slot request id SlotRequestId{45a581d25e3edf13533530fca3b66abb}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id 4afa3a5e55b78a948538caceb02d538e.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{5648160be157f6e6decceb04b246986e}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{45a581d25e3edf13533530fca3b66abb})
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (c1411805fa2ffd56df34cb079c86f88b) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id c1411805fa2ffd56df34cb079c86f88b to 65c0ed27-10c3-43eb-ab89-601636468c8d @ localhost (dataPort=-1) with allocation id 4afa3a5e55b78a948538caceb02d538e
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 4afa3a5e55b78a948538caceb02d538e.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 4afa3a5e55b78a948538caceb02d538e.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id 4afa3a5e55b78a948538caceb02d538e for local state stores for job d76aee723d47735ecd17fe9376eebe98.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_4afa3a5e55b78a948538caceb02d538e], jobID=d76aee723d47735ecd17fe9376eebe98, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for d76aee723d47735ecd17fe9376eebe98 - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id 4afa3a5e55b78a948538caceb02d538e.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b), deploy into slot with allocation id 4afa3a5e55b78a948538caceb02d538e.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b) switched from CREATED to DEPLOYING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b) [DEPLOYING]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b) [DEPLOYING].
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task c1411805fa2ffd56df34cb079c86f88b at library cache manager took 0 milliseconds
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@483828de
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Sink: Print to Std. Out (1/1)#0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (c1411805fa2ffd56df34cb079c86f88b) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Sink: Print to Std. Out (1/1)#0
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-throttle-time
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name heartbeat-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name join-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name sync-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name rebalance-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-rebalance
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name commit-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-revoked-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-assigned-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-lost-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lag
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lead
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name time-between-poll
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name poll-idle-ratio-avg
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.0
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: 77a89fcf8d7fa018
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655007860520
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker3:9092 (id: -3 rack: null) using address worker3/47.104.133.128
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -3
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Completed connection to node -3. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating API versions fetch from node -3.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Recorded API versions for node -3: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3:9092 (id: -3 rack: null)
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[], controller=worker1.cluster:9092 (id: 1 rack: null)}
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 will start reading the following 3 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic='metric_topic', partition=2}, KafkaTopicPartition{topic='metric_topic', partition=0}, KafkaTopicPartition{topic='metric_topic', partition=1}]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (c1411805fa2ffd56df34cb079c86f88b) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='metric_topic', partition=2}=-915623761773, KafkaTopicPartition{topic='metric_topic', partition=0}=-915623761773, KafkaTopicPartition{topic='metric_topic', partition=1}=-915623761773}.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-throttle-time
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name heartbeat-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name join-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name sync-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name rebalance-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-rebalance
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name commit-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-revoked-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-assigned-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-lost-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lag
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lead
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name time-between-poll
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name poll-idle-ratio-avg
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.0
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: 77a89fcf8d7fa018
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655007861016
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer initialized
[INFO][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Subscribed to partition(s): metric_topic-2, metric_topic-0, metric_topic-1
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending FindCoordinator request to broker worker1:9092 (id: -1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker1:9092 (id: -1 rack: null) using address worker1/47.104.89.120
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--1.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--1.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--1.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -1
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Completed connection to node -1. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating API versions fetch from node -1.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Recorded API versions for node -1: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='metric_topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1:9092 (id: -1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 0 for partition metric_topic-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 0 for partition metric_topic-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 0 for partition metric_topic-1
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = metric_topic, partition = 2, leader = 1, replicas = [1,2,3], isr = [1,2,3], offlineReplicas = []), epoch=0}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = metric_topic, partition = 1, leader = 3, replicas = [3,1,2], isr = [3,1,2], offlineReplicas = []), epoch=0}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = metric_topic, partition = 0, leader = 2, replicas = [2,3,1], isr = [2,3,1], offlineReplicas = []), epoch=0}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Received FindCoordinator response ClientResponse(receivedTimeMs=1655007861172, latencyMs=107, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=3, clientId=consumer-metric_consumer_g-2, correlationId=0), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='NONE', nodeId=1, host='worker1.cluster', port=9092))
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Discovered group coordinator worker1.cluster:9092 (id: 2147483646 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker1.cluster:9092 (id: 2147483646 rack: null) using address worker1.cluster/47.104.89.120
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Fetching committed offsets for partitions: [metric_topic-1, metric_topic-0, metric_topic-2]
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-2147483646.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-2147483646.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-2147483646.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node 2147483646
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Completed connection to node 2147483646. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating API versions fetch from node 2147483646.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Recorded API versions for node 2147483646: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Found no committed offset for partition metric_topic-1
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Found no committed offset for partition metric_topic-0
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Found no committed offset for partition metric_topic-2
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending ListOffsetRequest (type=ListOffsetRequest, replicaId=-1, partitionTimestamps={metric_topic-0={timestamp: -2, maxNumOffsets: 1, currentLeaderEpoch: Optional[0]}}, isolationLevel=READ_UNCOMMITTED) to broker worker2cluster:9092 (id: 2 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending ListOffsetRequest (type=ListOffsetRequest, replicaId=-1, partitionTimestamps={metric_topic-1={timestamp: -2, maxNumOffsets: 1, currentLeaderEpoch: Optional[0]}}, isolationLevel=READ_UNCOMMITTED) to broker worker3.cluster:9092 (id: 3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending ListOffsetRequest (type=ListOffsetRequest, replicaId=-1, partitionTimestamps={metric_topic-2={timestamp: -2, maxNumOffsets: 1, currentLeaderEpoch: Optional[0]}}, isolationLevel=READ_UNCOMMITTED) to broker worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker2cluster:9092 (id: 2 rack: null) using address worker2cluster/47.104.108.98
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker3.cluster:9092 (id: 3 rack: null) using address worker3.cluster/47.104.133.128
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker1.cluster:9092 (id: 1 rack: null) using address worker1.cluster/47.104.89.120
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-3.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node 3
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Completed connection to node 3. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating API versions fetch from node 3.
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-1.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-1.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-1.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node 1
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Completed connection to node 1. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating API versions fetch from node 1.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Recorded API versions for node 3: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Recorded API versions for node 1: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Handling ListOffsetResponse response for metric_topic-1. Fetched offset 0, timestamp -1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Not replacing existing epoch 0 with new epoch 0 for partition metric_topic-1
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.SubscriptionState][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Resetting offset for partition metric_topic-1 to offset 0.
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Handling ListOffsetResponse response for metric_topic-2. Fetched offset 0, timestamp -1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Not replacing existing epoch 0 with new epoch 0 for partition metric_topic-2
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.SubscriptionState][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Resetting offset for partition metric_topic-2 to offset 0.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 087a11a3c94afcaa0bb5f7078ed13be7: SlotReport{SlotStatus{slotID=65c0ed27-10c3-43eb-ab89-601636468c8d_0, allocationID=4afa3a5e55b78a948538caceb02d538e, jobID=d76aee723d47735ecd17fe9376eebe98, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 65c0ed27-10c3-43eb-ab89-601636468c8d: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 087a11a3c94afcaa0bb5f7078ed13be7: SlotReport{SlotStatus{slotID=65c0ed27-10c3-43eb-ab89-601636468c8d_0, allocationID=4afa3a5e55b78a948538caceb02d538e, jobID=d76aee723d47735ecd17fe9376eebe98, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 65c0ed27-10c3-43eb-ab89-601636468c8d: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 087a11a3c94afcaa0bb5f7078ed13be7: SlotReport{SlotStatus{slotID=65c0ed27-10c3-43eb-ab89-601636468c8d_0, allocationID=4afa3a5e55b78a948538caceb02d538e, jobID=d76aee723d47735ecd17fe9376eebe98, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 65c0ed27-10c3-43eb-ab89-601636468c8d: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='metric_topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1.cluster:9092 (id: 1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 0 to 0 for partition metric_topic-0
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 0 to 0 for partition metric_topic-2
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from 0 to 0 for partition metric_topic-1
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updated cluster metadata updateVersion 3 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = metric_topic, partition = 2, leader = 1, replicas = [1,2,3], isr = [1,2,3], offlineReplicas = []), epoch=0}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = metric_topic, partition = 1, leader = 3, replicas = [3,1,2], isr = [3,1,2], offlineReplicas = []), epoch=0}, PartitionInfoAndEpoch{partitionInfo=Partition(topic = metric_topic, partition = 0, leader = 2, replicas = [2,3,1], isr = [2,3,1], offlineReplicas = []), epoch=0}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending ListOffsetRequest (type=ListOffsetRequest, replicaId=-1, partitionTimestamps={metric_topic-0={timestamp: -2, maxNumOffsets: 1, currentLeaderEpoch: Optional[0]}}, isolationLevel=READ_UNCOMMITTED) to broker worker2cluster:9092 (id: 2 rack: null)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 087a11a3c94afcaa0bb5f7078ed13be7: SlotReport{SlotStatus{slotID=65c0ed27-10c3-43eb-ab89-601636468c8d_0, allocationID=4afa3a5e55b78a948538caceb02d538e, jobID=d76aee723d47735ecd17fe9376eebe98, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 65c0ed27-10c3-43eb-ab89-601636468c8d: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 087a11a3c94afcaa0bb5f7078ed13be7: SlotReport{SlotStatus{slotID=65c0ed27-10c3-43eb-ab89-601636468c8d_0, allocationID=4afa3a5e55b78a948538caceb02d538e, jobID=d76aee723d47735ecd17fe9376eebe98, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 65c0ed27-10c3-43eb-ab89-601636468c8d: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 44f96cfb4d971479454a87e32887cfc2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 087a11a3c94afcaa0bb5f7078ed13be7: SlotReport{SlotStatus{slotID=65c0ed27-10c3-43eb-ab89-601636468c8d_0, allocationID=4afa3a5e55b78a948538caceb02d538e, jobID=d76aee723d47735ecd17fe9376eebe98, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 65c0ed27-10c3-43eb-ab89-601636468c8d: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 914040d0cf68c713daeb74600d37e530.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 65c0ed27-10c3-43eb-ab89-601636468c8d.
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b) switched from RUNNING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined

[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (c1411805fa2ffd56df34cb079c86f88b) [FAILED]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 c1411805fa2ffd56df34cb079c86f88b.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (c1411805fa2ffd56df34cb079c86f88b) switched from RUNNING to FAILED on 65c0ed27-10c3-43eb-ab89-601636468c8d @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{5648160be157f6e6decceb04b246986e}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{45a581d25e3edf13533530fca3b66abb})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{45a581d25e3edf13533530fca3b66abb})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{45a581d25e3edf13533530fca3b66abb}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 4afa3a5e55b78a948538caceb02d538e.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{45a581d25e3edf13533530fca3b66abb})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 4afa3a5e55b78a948538caceb02d538e @ 65c0ed27-10c3-43eb-ab89-601636468c8d @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job d76aee723d47735ecd17fe9376eebe98.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job d76aee723d47735ecd17fe9376eebe98
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job d76aee723d47735ecd17fe9376eebe98: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]1 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0. 
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (d76aee723d47735ecd17fe9376eebe98) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (d76aee723d47735ecd17fe9376eebe98) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph d76aee723d47735ecd17fe9376eebe98 reached terminal state FAILED.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job d76aee723d47735ecd17fe9376eebe98.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job d76aee723d47735ecd17fe9376eebe98 under leader id 8dff4167-37cc-4d4a-bc92-a95d750eac17 reached a globally terminal state FAILED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job d76aee723d47735ecd17fe9376eebe98.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job d76aee723d47735ecd17fe9376eebe98 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job d76aee723d47735ecd17fe9376eebe98.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job d76aee723d47735ecd17fe9376eebe98 under leader id 8dff4167-37cc-4d4a-bc92-a95d750eac17.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection 44f96cfb4d971479454a87e32887cfc2.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(d76aee723d47735ecd17fe9376eebe98).
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job d76aee723d47735ecd17fe9376eebe98.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection 65c0ed27-10c3-43eb-ab89-601636468c8d because: The TaskExecutor is shutting down.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor 65c0ed27-10c3-43eb-ab89-601636468c8d because: Stopping JobMaster for job metricStream(d76aee723d47735ecd17fe9376eebe98).
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor 087a11a3c94afcaa0bb5f7078ed13be7 from the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job d76aee723d47735ecd17fe9376eebe98.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [4afa3a5e55b78a948538caceb02d538e].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d76aee723d47735ecd17fe9376eebe98).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor 65c0ed27-10c3-43eb-ab89-601636468c8d.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection 44f96cfb4d971479454a87e32887cfc2.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(d76aee723d47735ecd17fe9376eebe98).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job d76aee723d47735ecd17fe9376eebe98 from job leader id monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: 4afa3a5e55b78a948538caceb02d538e, jobId: d76aee723d47735ecd17fe9376eebe98).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager bc92a95d750eac178dff416737cc4d4a@akka://flink/user/rpc/jobmanager_3 for job d76aee723d47735ecd17fe9376eebe98 from the resource manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job d76aee723d47735ecd17fe9376eebe98.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job d76aee723d47735ecd17fe9376eebe98.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 4afa3a5e55b78a948538caceb02d538e because: Stopping JobMaster for job metricStream(d76aee723d47735ecd17fe9376eebe98).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 4afa3a5e55b78a948538caceb02d538e.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 4afa3a5e55b78a948538caceb02d538e.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 4afa3a5e55b78a948538caceb02d538e.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]DefaultJobLeaderService's leader retrieval listener reported a new leader for job d76aee723d47735ecd17fe9376eebe98. However, the service is no longer running.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job d76aee723d47735ecd17fe9376eebe98 has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job d76aee723d47735ecd17fe9376eebe98 under leader id 8dff4167-37cc-4d4a-bc92-a95d750eac17 has been terminated.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-f9ab9d92-c28c-4a18-9d3c-d44678bc8231
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-d70d5fcf-a6f2-4791-8453-d1b69d64f82f
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-396d9c01-5e3c-4d11-8994-50b7a91dc171
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-12][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:51867
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=2, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '7df19f87deec5680128845fd9a6ca18d' for node 'Sink: Print to Std. Out-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-d97799ef-fed2-4830-a56d-305074dde376
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:52062 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-62db76f7-cd41-496b-b441-5f37ca066b8a
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-75ba54d1-4b54-404e-a3cd-672f1a64c0d1
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: 43d56b29-1f85-447a-a640-fa4b9916a810
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 369 GB (80.22% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-2a7c96c1-35aa-4329-802f-de8c4334603c for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-418a81a2-4035-41a5-8073-6503ee6f264d for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-d9839df2-f71d-46be-86f8-71431f3d2ea9
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2777 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:52063
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:52063
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:52063.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:52063 was granted leadership with leaderSessionID=c4693471-2f5b-4601-893c-dd8e969fe1fe
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:52063 , session=c4693471-2f5b-4601-893c-dd8e969fe1fe
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id 996f6518-75f9-4aba-ac31-43a0f1f0c33b. Creating new DispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token ac6355f3578aed0356ab619e21e44557
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=56ab619e-21e4-4557-ac63-55f3578aed03
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(ac6355f3578aed0356ab619e21e44557).
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=996f6518-75f9-4aba-ac31-43a0f1f0c33b
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission 351e59b9bd7b5f2b9350fa749f0e9c0c (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID 43d56b29-1f85-447a-a640-fa4b9916a810 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job 351e59b9bd7b5f2b9350fa749f0e9c0c (metricStream).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id f2f55bb5c545a7843065a6bae5e3b395.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor 43d56b29-1f85-447a-a640-fa4b9916a810 under f2f55bb5c545a7843065a6bae5e3b395 at the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under 742d645a-c0b8-4c8d-804a-47c6d954c7fd.
[DEBUG][22-06-12][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 0 ms.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c).
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Sink: Print to Std. Out) to 0 predecessors.
[INFO][22-06-12][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 2 ms
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5a3c473
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job 351e59b9bd7b5f2b9350fa749f0e9c0c after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@537af3c1 for metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job 351e59b9bd7b5f2b9350fa749f0e9c0c under leader id 742d645a-c0b8-4c8d-804a-47c6d954c7fd.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership 742d645a-c0b8-4c8d-804a-47c6d954c7fd.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=742d645a-c0b8-4c8d-804a-47c6d954c7fd
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c) under job master id 804a47c6d954c7fd742d645ac0b84c8d.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c) switched from state CREATED to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (0d7426255ee6106ba6a303bdfc0b9b9d) switched from CREATED to SCHEDULED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{63fce7d12793954bd8a108226d91dd69}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{63fce7d12793954bd8a108226d91dd69} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{9b611c694e94fab3bd2b07f43a313da8}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{63fce7d12793954bd8a108226d91dd69})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(ac6355f3578aed0356ab619e21e44557)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job 351e59b9bd7b5f2b9350fa749f0e9c0c to job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager 804a47c6d954c7fd742d645ac0b84c8d@akka://flink/user/rpc/jobmanager_3 for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job 351e59b9bd7b5f2b9350fa749f0e9c0c has a new job leader 742d645a-c0b8-4c8d-804a-47c6d954c7fd@akka://flink/user/rpc/jobmanager_3.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager 804a47c6d954c7fd742d645ac0b84c8d@akka://flink/user/rpc/jobmanager_3 for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: ac6355f3578aed0356ab619e21e44557.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job 351e59b9bd7b5f2b9350fa749f0e9c0c: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot 43d56b29-1f85-447a-a640-fa4b9916a810_0 for job 351e59b9bd7b5f2b9350fa749f0e9c0c with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request 5f1c107f095bd3f9a80294d674540df2 for job 351e59b9bd7b5f2b9350fa749f0e9c0c from resource manager with leader id ac6355f3578aed0356ab619e21e44557.
[DEBUG][22-06-12][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for 5f1c107f095bd3f9a80294d674540df2.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job 351e59b9bd7b5f2b9350fa749f0e9c0c for job leader monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job 351e59b9bd7b5f2b9350fa749f0e9c0c. Address: akka://flink/user/rpc/jobmanager_3, leader id: 804a47c6d954c7fd742d645ac0b84c8d.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 742d645a-c0b8-4c8d-804a-47c6d954c7fd.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor 43d56b29-1f85-447a-a640-fa4b9916a810 @ localhost (dataPort=-1).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer 5f1c107f095bd3f9a80294d674540df2 to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot 5f1c107f095bd3f9a80294d674540df2 @ 43d56b29-1f85-447a-a640-fa4b9916a810 @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{63fce7d12793954bd8a108226d91dd69}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot 5f1c107f095bd3f9a80294d674540df2 for slot request id SlotRequestId{63fce7d12793954bd8a108226d91dd69}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id 5f1c107f095bd3f9a80294d674540df2.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{9b611c694e94fab3bd2b07f43a313da8}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{63fce7d12793954bd8a108226d91dd69})
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (0d7426255ee6106ba6a303bdfc0b9b9d) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id 0d7426255ee6106ba6a303bdfc0b9b9d to 43d56b29-1f85-447a-a640-fa4b9916a810 @ localhost (dataPort=-1) with allocation id 5f1c107f095bd3f9a80294d674540df2
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 5f1c107f095bd3f9a80294d674540df2.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id 5f1c107f095bd3f9a80294d674540df2 for local state stores for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_5f1c107f095bd3f9a80294d674540df2], jobID=351e59b9bd7b5f2b9350fa749f0e9c0c, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for 351e59b9bd7b5f2b9350fa749f0e9c0c - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id 5f1c107f095bd3f9a80294d674540df2.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d), deploy into slot with allocation id 5f1c107f095bd3f9a80294d674540df2.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d) switched from CREATED to DEPLOYING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d) [DEPLOYING]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 5f1c107f095bd3f9a80294d674540df2.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d) [DEPLOYING].
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 0d7426255ee6106ba6a303bdfc0b9b9d at library cache manager took 0 milliseconds
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@575c7857
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Sink: Print to Std. Out (1/1)#0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (0d7426255ee6106ba6a303bdfc0b9b9d) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Sink: Print to Std. Out (1/1)#0
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-throttle-time
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name heartbeat-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name join-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name sync-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name rebalance-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-rebalance
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name commit-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-revoked-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-assigned-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-lost-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lag
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lead
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name time-between-poll
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name poll-idle-ratio-avg
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.0
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: 77a89fcf8d7fa018
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655009318360
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker3:9092 (id: -3 rack: null) using address worker3/47.104.133.128
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -3
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Completed connection to node -3. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating API versions fetch from node -3.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Recorded API versions for node -3: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3:9092 (id: -3 rack: null)
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Topic metadata fetch included errors: {metric_topic=LEADER_NOT_AVAILABLE}
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker3.cluster:9092 (id: 3 rack: null) using address worker3.cluster/47.104.133.128
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node-3.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node 3
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Completed connection to node 3. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating API versions fetch from node 3.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Recorded API versions for node 3: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 will start reading the following 1 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic='metric_topic', partition=0}]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (0d7426255ee6106ba6a303bdfc0b9b9d) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='metric_topic', partition=0}=-915623761773}.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-throttle-time
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name heartbeat-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name join-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name sync-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name rebalance-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name failed-rebalance
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name commit-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-revoked-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-assigned-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name partition-lost-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name bytes-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-fetched
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name fetch-latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lag
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name records-lead
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name time-between-poll
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name poll-idle-ratio-avg
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.0
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: 77a89fcf8d7fa018
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655009319042
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer initialized
[INFO][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Subscribed to partition(s): metric_topic-0
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending FindCoordinator request to broker worker3:9092 (id: -3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker3:9092 (id: -3 rack: null) using address worker3/47.104.133.128
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Added sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -3
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Completed connection to node -3. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating API versions fetch from node -3.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Recorded API versions for node -3: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='metric_topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker3:9092 (id: -3 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 0 for partition metric_topic-0
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = metric_topic, partition = 0, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=0}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Received FindCoordinator response ClientResponse(receivedTimeMs=1655009319191, latencyMs=106, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=3, clientId=consumer-metric_consumer_g-2, correlationId=0), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='NONE', nodeId=2, host='worker2cluster', port=9092))
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Discovered group coordinator worker2cluster:9092 (id: 2147483645 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker2cluster:9092 (id: 2147483645 rack: null) using address worker2cluster/47.104.108.98
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Fetching committed offsets for partitions: [metric_topic-0]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance f2f55bb5c545a7843065a6bae5e3b395: SlotReport{SlotStatus{slotID=43d56b29-1f85-447a-a640-fa4b9916a810_0, allocationID=5f1c107f095bd3f9a80294d674540df2, jobID=351e59b9bd7b5f2b9350fa749f0e9c0c, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 43d56b29-1f85-447a-a640-fa4b9916a810: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance f2f55bb5c545a7843065a6bae5e3b395: SlotReport{SlotStatus{slotID=43d56b29-1f85-447a-a640-fa4b9916a810_0, allocationID=5f1c107f095bd3f9a80294d674540df2, jobID=351e59b9bd7b5f2b9350fa749f0e9c0c, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 43d56b29-1f85-447a-a640-fa4b9916a810: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance f2f55bb5c545a7843065a6bae5e3b395: SlotReport{SlotStatus{slotID=43d56b29-1f85-447a-a640-fa4b9916a810_0, allocationID=5f1c107f095bd3f9a80294d674540df2, jobID=351e59b9bd7b5f2b9350fa749f0e9c0c, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 43d56b29-1f85-447a-a640-fa4b9916a810: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Fetching committed offsets for partitions: [metric_topic-0]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance f2f55bb5c545a7843065a6bae5e3b395: SlotReport{SlotStatus{slotID=43d56b29-1f85-447a-a640-fa4b9916a810_0, allocationID=5f1c107f095bd3f9a80294d674540df2, jobID=351e59b9bd7b5f2b9350fa749f0e9c0c, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 43d56b29-1f85-447a-a640-fa4b9916a810: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance f2f55bb5c545a7843065a6bae5e3b395: SlotReport{SlotStatus{slotID=43d56b29-1f85-447a-a640-fa4b9916a810_0, allocationID=5f1c107f095bd3f9a80294d674540df2, jobID=351e59b9bd7b5f2b9350fa749f0e9c0c, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 43d56b29-1f85-447a-a640-fa4b9916a810: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 5ab18c69234df7912e9388321d442583.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance f2f55bb5c545a7843065a6bae5e3b395: SlotReport{SlotStatus{slotID=43d56b29-1f85-447a-a640-fa4b9916a810_0, allocationID=5f1c107f095bd3f9a80294d674540df2, jobID=351e59b9bd7b5f2b9350fa749f0e9c0c, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 43d56b29-1f85-447a-a640-fa4b9916a810: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from b0efb01e3898a200ac4f864753e053c1.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 43d56b29-1f85-447a-a640-fa4b9916a810.
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-closed:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name connections-created:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name successful-authentication-no-reauth:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-authentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name failed-reauthentication:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name reauthentication-latency:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-sent:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name bytes-received:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name select-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name io-time:
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node--3.latency
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node-3.bytes-sent
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node-3.bytes-received
[DEBUG][22-06-12][org.apache.kafka.common.metrics.Metrics]Removed sensor with name node-3.latency
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d) switched from RUNNING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined

[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (0d7426255ee6106ba6a303bdfc0b9b9d) [FAILED]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 0d7426255ee6106ba6a303bdfc0b9b9d.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (0d7426255ee6106ba6a303bdfc0b9b9d) switched from RUNNING to FAILED on 43d56b29-1f85-447a-a640-fa4b9916a810 @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{9b611c694e94fab3bd2b07f43a313da8}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{63fce7d12793954bd8a108226d91dd69})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{63fce7d12793954bd8a108226d91dd69})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{63fce7d12793954bd8a108226d91dd69}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 5f1c107f095bd3f9a80294d674540df2.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{63fce7d12793954bd8a108226d91dd69})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 5f1c107f095bd3f9a80294d674540df2 @ 43d56b29-1f85-447a-a640-fa4b9916a810 @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job 351e59b9bd7b5f2b9350fa749f0e9c0c
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job 351e59b9bd7b5f2b9350fa749f0e9c0c: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]1 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0. 
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (351e59b9bd7b5f2b9350fa749f0e9c0c) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph 351e59b9bd7b5f2b9350fa749f0e9c0c reached terminal state FAILED.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job 351e59b9bd7b5f2b9350fa749f0e9c0c under leader id 742d645a-c0b8-4c8d-804a-47c6d954c7fd reached a globally terminal state FAILED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job 351e59b9bd7b5f2b9350fa749f0e9c0c reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection 5ab18c69234df7912e9388321d442583.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job 351e59b9bd7b5f2b9350fa749f0e9c0c under leader id 742d645a-c0b8-4c8d-804a-47c6d954c7fd.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(351e59b9bd7b5f2b9350fa749f0e9c0c).
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection 43d56b29-1f85-447a-a640-fa4b9916a810 because: The TaskExecutor is shutting down.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor f2f55bb5c545a7843065a6bae5e3b395 from the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor 43d56b29-1f85-447a-a640-fa4b9916a810.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: 5f1c107f095bd3f9a80294d674540df2, jobId: 351e59b9bd7b5f2b9350fa749f0e9c0c).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor 43d56b29-1f85-447a-a640-fa4b9916a810 because: Stopping JobMaster for job metricStream(351e59b9bd7b5f2b9350fa749f0e9c0c).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [5f1c107f095bd3f9a80294d674540df2].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(351e59b9bd7b5f2b9350fa749f0e9c0c).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection 5ab18c69234df7912e9388321d442583.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(351e59b9bd7b5f2b9350fa749f0e9c0c).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job 351e59b9bd7b5f2b9350fa749f0e9c0c from job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager 804a47c6d954c7fd742d645ac0b84c8d@akka://flink/user/rpc/jobmanager_3 for job 351e59b9bd7b5f2b9350fa749f0e9c0c from the resource manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 351e59b9bd7b5f2b9350fa749f0e9c0c.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 5f1c107f095bd3f9a80294d674540df2 because: Stopping JobMaster for job metricStream(351e59b9bd7b5f2b9350fa749f0e9c0c).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 5f1c107f095bd3f9a80294d674540df2.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 5f1c107f095bd3f9a80294d674540df2.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 5f1c107f095bd3f9a80294d674540df2.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-2a7c96c1-35aa-4329-802f-de8c4334603c
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job 351e59b9bd7b5f2b9350fa749f0e9c0c has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job 351e59b9bd7b5f2b9350fa749f0e9c0c under leader id 742d645a-c0b8-4c8d-804a-47c6d954c7fd has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-418a81a2-4035-41a5-8073-6503ee6f264d
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-d9839df2-f71d-46be-86f8-71431f3d2ea9
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-12][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:52062
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=2, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '7df19f87deec5680128845fd9a6ca18d' for node 'Sink: Print to Std. Out-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-1f9e05ba-ed07-42f5-8212-e662bf421b0d
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:52132 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-4d988227-8e38-4576-a87a-a71f23d5992b
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-8993a36b-db5e-4454-91d2-408608dddec7
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: 07ab5162-f4f0-4d3b-9372-da21d5cfcf11
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 369 GB (80.22% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-c9d5ca54-3ecc-41b1-abb3-327cf26340d3 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-0adba2f6-1434-49ac-8a88-f0dbd9feb03c for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-312e48f8-45a7-43e2-b663-0a3500dd3acf
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2841 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:52133
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:52133
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:52133.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:52133 was granted leadership with leaderSessionID=4cc54e07-7e2d-4e3c-9d70-b25e9ac1f846
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:52133 , session=4cc54e07-7e2d-4e3c-9d70-b25e9ac1f846
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id 815b2c0e-6742-4ac3-be4c-3c4a621eed57. Creating new DispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token aa6b714e643c29ac71adfc551d094f70
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=71adfc55-1d09-4f70-aa6b-714e643c29ac
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(aa6b714e643c29ac71adfc551d094f70).
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=815b2c0e-6742-4ac3-be4c-3c4a621eed57
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission b6f38d1285fda0292a595509fd54e18b (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID 07ab5162-f4f0-4d3b-9372-da21d5cfcf11 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job b6f38d1285fda0292a595509fd54e18b (metricStream).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 37098bb3752ea02a1564681dde946c8c.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11 under 37098bb3752ea02a1564681dde946c8c at the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job b6f38d1285fda0292a595509fd54e18b.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under 2dd6c012-efdd-42b6-bb9f-1adbe119035f.
[DEBUG][22-06-12][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (b6f38d1285fda0292a595509fd54e18b).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (b6f38d1285fda0292a595509fd54e18b).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (b6f38d1285fda0292a595509fd54e18b).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 2 ms.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (b6f38d1285fda0292a595509fd54e18b).
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Sink: Print to Std. Out) to 0 predecessors.
[INFO][22-06-12][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 5 ms
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (b6f38d1285fda0292a595509fd54e18b).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@77d60828
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job b6f38d1285fda0292a595509fd54e18b after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@3c5d25fe for metricStream (b6f38d1285fda0292a595509fd54e18b).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job b6f38d1285fda0292a595509fd54e18b under leader id 2dd6c012-efdd-42b6-bb9f-1adbe119035f.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership 2dd6c012-efdd-42b6-bb9f-1adbe119035f.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=2dd6c012-efdd-42b6-bb9f-1adbe119035f
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (b6f38d1285fda0292a595509fd54e18b) under job master id bb9f1adbe119035f2dd6c012efdd42b6.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (b6f38d1285fda0292a595509fd54e18b) switched from state CREATED to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (2ef97cc8185db8c6987a741f7e3def8b) switched from CREATED to SCHEDULED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job b6f38d1285fda0292a595509fd54e18b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{cf508d87088976700ed076cd834d99f1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(aa6b714e643c29ac71adfc551d094f70)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job b6f38d1285fda0292a595509fd54e18b to job leader id monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job b6f38d1285fda0292a595509fd54e18b has a new job leader 2dd6c012-efdd-42b6-bb9f-1adbe119035f@akka://flink/user/rpc/jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager bb9f1adbe119035f2dd6c012efdd42b6@akka://flink/user/rpc/jobmanager_3 for job b6f38d1285fda0292a595509fd54e18b.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager bb9f1adbe119035f2dd6c012efdd42b6@akka://flink/user/rpc/jobmanager_3 for job b6f38d1285fda0292a595509fd54e18b.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: aa6b714e643c29ac71adfc551d094f70.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job b6f38d1285fda0292a595509fd54e18b: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job b6f38d1285fda0292a595509fd54e18b.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot 07ab5162-f4f0-4d3b-9372-da21d5cfcf11_0 for job b6f38d1285fda0292a595509fd54e18b with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request 8712bd969a9e6e30a9acb32b6924ff85 for job b6f38d1285fda0292a595509fd54e18b from resource manager with leader id aa6b714e643c29ac71adfc551d094f70.
[DEBUG][22-06-12][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for 8712bd969a9e6e30a9acb32b6924ff85.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job b6f38d1285fda0292a595509fd54e18b for job leader monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job b6f38d1285fda0292a595509fd54e18b. Address: akka://flink/user/rpc/jobmanager_3, leader id: bb9f1adbe119035f2dd6c012efdd42b6.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 2dd6c012-efdd-42b6-bb9f-1adbe119035f.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job b6f38d1285fda0292a595509fd54e18b.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job b6f38d1285fda0292a595509fd54e18b.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job b6f38d1285fda0292a595509fd54e18b.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11 @ localhost (dataPort=-1).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer 8712bd969a9e6e30a9acb32b6924ff85 to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot 8712bd969a9e6e30a9acb32b6924ff85 @ 07ab5162-f4f0-4d3b-9372-da21d5cfcf11 @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot 8712bd969a9e6e30a9acb32b6924ff85 for slot request id SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id 8712bd969a9e6e30a9acb32b6924ff85.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{cf508d87088976700ed076cd834d99f1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e})
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (2ef97cc8185db8c6987a741f7e3def8b) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id 2ef97cc8185db8c6987a741f7e3def8b to 07ab5162-f4f0-4d3b-9372-da21d5cfcf11 @ localhost (dataPort=-1) with allocation id 8712bd969a9e6e30a9acb32b6924ff85
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 8712bd969a9e6e30a9acb32b6924ff85.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id 8712bd969a9e6e30a9acb32b6924ff85 for local state stores for job b6f38d1285fda0292a595509fd54e18b.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_8712bd969a9e6e30a9acb32b6924ff85], jobID=b6f38d1285fda0292a595509fd54e18b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for b6f38d1285fda0292a595509fd54e18b - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id 8712bd969a9e6e30a9acb32b6924ff85.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b), deploy into slot with allocation id 8712bd969a9e6e30a9acb32b6924ff85.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b) switched from CREATED to DEPLOYING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b) [DEPLOYING]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b) [DEPLOYING].
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 2ef97cc8185db8c6987a741f7e3def8b at library cache manager took 1 milliseconds
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 8712bd969a9e6e30a9acb32b6924ff85.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@584b6d30
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Sink: Print to Std. Out (1/1)#0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (2ef97cc8185db8c6987a741f7e3def8b) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Sink: Print to Std. Out (1/1)#0
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655009665335
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker1:9092 (id: -1 rack: null) using address worker1/47.104.89.120
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -1
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Completed connection to node -1. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating API versions fetch from node -1.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Recorded API versions for node -1: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1:9092 (id: -1 rack: null)
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[], controller=worker1.cluster:9092 (id: 1 rack: null)}
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 will start reading the following 1 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic='metric_topic', partition=0}]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (2ef97cc8185db8c6987a741f7e3def8b) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='metric_topic', partition=0}=-915623761773}.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initializing the Kafka consumer
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655009665838
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer initialized
[INFO][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Subscribed to partition(s): metric_topic-0
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending FindCoordinator request to broker worker2:9092 (id: -2 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 37098bb3752ea02a1564681dde946c8c: SlotReport{SlotStatus{slotID=07ab5162-f4f0-4d3b-9372-da21d5cfcf11_0, allocationID=8712bd969a9e6e30a9acb32b6924ff85, jobID=b6f38d1285fda0292a595509fd54e18b, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 37098bb3752ea02a1564681dde946c8c: SlotReport{SlotStatus{slotID=07ab5162-f4f0-4d3b-9372-da21d5cfcf11_0, allocationID=8712bd969a9e6e30a9acb32b6924ff85, jobID=b6f38d1285fda0292a595509fd54e18b, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 37098bb3752ea02a1564681dde946c8c: SlotReport{SlotStatus{slotID=07ab5162-f4f0-4d3b-9372-da21d5cfcf11_0, allocationID=8712bd969a9e6e30a9acb32b6924ff85, jobID=b6f38d1285fda0292a595509fd54e18b, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Coordinator discovery failed, refreshing metadata
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 37098bb3752ea02a1564681dde946c8c: SlotReport{SlotStatus{slotID=07ab5162-f4f0-4d3b-9372-da21d5cfcf11_0, allocationID=8712bd969a9e6e30a9acb32b6924ff85, jobID=b6f38d1285fda0292a595509fd54e18b, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 37098bb3752ea02a1564681dde946c8c: SlotReport{SlotStatus{slotID=07ab5162-f4f0-4d3b-9372-da21d5cfcf11_0, allocationID=8712bd969a9e6e30a9acb32b6924ff85, jobID=b6f38d1285fda0292a595509fd54e18b, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 2ca0f22cee0584b83bd78df79c003eeb.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 37098bb3752ea02a1564681dde946c8c: SlotReport{SlotStatus{slotID=07ab5162-f4f0-4d3b-9372-da21d5cfcf11_0, allocationID=8712bd969a9e6e30a9acb32b6924ff85, jobID=b6f38d1285fda0292a595509fd54e18b, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 7725610da7469f8ba99ffd8d7dbb6517.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b) switched from RUNNING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined

[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (2ef97cc8185db8c6987a741f7e3def8b) [FAILED]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 2ef97cc8185db8c6987a741f7e3def8b.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (2ef97cc8185db8c6987a741f7e3def8b) switched from RUNNING to FAILED on 07ab5162-f4f0-4d3b-9372-da21d5cfcf11 @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{cf508d87088976700ed076cd834d99f1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 8712bd969a9e6e30a9acb32b6924ff85.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{a1b15f0acca6fd5aad1a87b788bcb79e})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 8712bd969a9e6e30a9acb32b6924ff85 @ 07ab5162-f4f0-4d3b-9372-da21d5cfcf11 @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job b6f38d1285fda0292a595509fd54e18b.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job b6f38d1285fda0292a595509fd54e18b
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job b6f38d1285fda0292a595509fd54e18b: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]1 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0. 
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (b6f38d1285fda0292a595509fd54e18b) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (b6f38d1285fda0292a595509fd54e18b) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph b6f38d1285fda0292a595509fd54e18b reached terminal state FAILED.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job b6f38d1285fda0292a595509fd54e18b.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job b6f38d1285fda0292a595509fd54e18b under leader id 2dd6c012-efdd-42b6-bb9f-1adbe119035f reached a globally terminal state FAILED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job b6f38d1285fda0292a595509fd54e18b.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job b6f38d1285fda0292a595509fd54e18b reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition metric_topic-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job b6f38d1285fda0292a595509fd54e18b.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job b6f38d1285fda0292a595509fd54e18b under leader id 2dd6c012-efdd-42b6-bb9f-1adbe119035f.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection 2ca0f22cee0584b83bd78df79c003eeb.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(b6f38d1285fda0292a595509fd54e18b).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job b6f38d1285fda0292a595509fd54e18b.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection 07ab5162-f4f0-4d3b-9372-da21d5cfcf11 because: The TaskExecutor is shutting down.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor 37098bb3752ea02a1564681dde946c8c from the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job b6f38d1285fda0292a595509fd54e18b.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor 07ab5162-f4f0-4d3b-9372-da21d5cfcf11 because: Stopping JobMaster for job metricStream(b6f38d1285fda0292a595509fd54e18b).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [8712bd969a9e6e30a9acb32b6924ff85].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(b6f38d1285fda0292a595509fd54e18b).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: 8712bd969a9e6e30a9acb32b6924ff85, jobId: b6f38d1285fda0292a595509fd54e18b).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection 2ca0f22cee0584b83bd78df79c003eeb.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(b6f38d1285fda0292a595509fd54e18b).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job b6f38d1285fda0292a595509fd54e18b from job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager bb9f1adbe119035f2dd6c012efdd42b6@akka://flink/user/rpc/jobmanager_3 for job b6f38d1285fda0292a595509fd54e18b from the resource manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job b6f38d1285fda0292a595509fd54e18b.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job b6f38d1285fda0292a595509fd54e18b.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 8712bd969a9e6e30a9acb32b6924ff85 because: Stopping JobMaster for job metricStream(b6f38d1285fda0292a595509fd54e18b).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 8712bd969a9e6e30a9acb32b6924ff85.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 8712bd969a9e6e30a9acb32b6924ff85.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 8712bd969a9e6e30a9acb32b6924ff85.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-12][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-c9d5ca54-3ecc-41b1-abb3-327cf26340d3
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-0adba2f6-1434-49ac-8a88-f0dbd9feb03c
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-312e48f8-45a7-43e2-b663-0a3500dd3acf
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job b6f38d1285fda0292a595509fd54e18b has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job b6f38d1285fda0292a595509fd54e18b under leader id 2dd6c012-efdd-42b6-bb9f-1adbe119035f has been terminated.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-12][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:52132
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=2, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '7df19f87deec5680128845fd9a6ca18d' for node 'Sink: Print to Std. Out-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-18702b55-8c49-4ba9-8652-17ec22a5fbc6
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:52239 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-728cbabd-fffb-432f-8c13-7ee2b2f91d86
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-6f6a677a-89b6-4fe2-a1cf-6a64c61169f6
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: e2f82294-c83e-43b2-8a52-8f8bc70229f3
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 369 GB (80.22% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-dcc3cf1d-cd82-4115-b9a9-68d9fd77c848 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-4b544960-d5a5-479f-b338-ddbbcc1bb8a3 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-4953d168-fb41-4760-aae4-4272ac0d83d1
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2882 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:52240
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:52240
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:52240.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:52240 was granted leadership with leaderSessionID=4efe5a27-3880-4880-a98d-19df3c155589
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:52240 , session=4efe5a27-3880-4880-a98d-19df3c155589
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id 5843e81a-5acc-45c9-9c51-a616282ceaf9. Creating new DispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 885c2d70c076034519b07d4d8cf8446c
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=19b07d4d-8cf8-446c-885c-2d70c0760345
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(885c2d70c076034519b07d4d8cf8446c).
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=5843e81a-5acc-45c9-9c51-a616282ceaf9
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission 8903afdeabb4299f1f07ffcfdd3f2827 (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID e2f82294-c83e-43b2-8a52-8f8bc70229f3 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job 8903afdeabb4299f1f07ffcfdd3f2827 (metricStream).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 2d6d9699f4e43f8b629763b6229bfa49.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor e2f82294-c83e-43b2-8a52-8f8bc70229f3 under 2d6d9699f4e43f8b629763b6229bfa49 at the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job 8903afdeabb4299f1f07ffcfdd3f2827.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under 41dca103-4ce3-4a8e-b1da-c787c906d03a.
[DEBUG][22-06-12][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (8903afdeabb4299f1f07ffcfdd3f2827).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (8903afdeabb4299f1f07ffcfdd3f2827).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (8903afdeabb4299f1f07ffcfdd3f2827).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 0 ms.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (8903afdeabb4299f1f07ffcfdd3f2827).
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Sink: Print to Std. Out) to 0 predecessors.
[INFO][22-06-12][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 2 ms
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (8903afdeabb4299f1f07ffcfdd3f2827).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3b97aee5
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job 8903afdeabb4299f1f07ffcfdd3f2827 after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@5b7a9d37 for metricStream (8903afdeabb4299f1f07ffcfdd3f2827).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job 8903afdeabb4299f1f07ffcfdd3f2827 under leader id 41dca103-4ce3-4a8e-b1da-c787c906d03a.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership 41dca103-4ce3-4a8e-b1da-c787c906d03a.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=41dca103-4ce3-4a8e-b1da-c787c906d03a
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (8903afdeabb4299f1f07ffcfdd3f2827) under job master id b1dac787c906d03a41dca1034ce34a8e.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (8903afdeabb4299f1f07ffcfdd3f2827) switched from state CREATED to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (7b5c91023318775d3ec0c77cd1e51073) switched from CREATED to SCHEDULED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{be5e3907b7805487bcbca1ee2c5ff9bc}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{be5e3907b7805487bcbca1ee2c5ff9bc} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 8903afdeabb4299f1f07ffcfdd3f2827.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{60f00ac7370b54aa913463f16f37b733}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{be5e3907b7805487bcbca1ee2c5ff9bc})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(885c2d70c076034519b07d4d8cf8446c)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job 8903afdeabb4299f1f07ffcfdd3f2827 to job leader id monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job 8903afdeabb4299f1f07ffcfdd3f2827 has a new job leader 41dca103-4ce3-4a8e-b1da-c787c906d03a@akka://flink/user/rpc/jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager b1dac787c906d03a41dca1034ce34a8e@akka://flink/user/rpc/jobmanager_3 for job 8903afdeabb4299f1f07ffcfdd3f2827.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager b1dac787c906d03a41dca1034ce34a8e@akka://flink/user/rpc/jobmanager_3 for job 8903afdeabb4299f1f07ffcfdd3f2827.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: 885c2d70c076034519b07d4d8cf8446c.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job 8903afdeabb4299f1f07ffcfdd3f2827: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 8903afdeabb4299f1f07ffcfdd3f2827.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot e2f82294-c83e-43b2-8a52-8f8bc70229f3_0 for job 8903afdeabb4299f1f07ffcfdd3f2827 with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request 316ee10e1a8ba9282bcb72347733cb56 for job 8903afdeabb4299f1f07ffcfdd3f2827 from resource manager with leader id 885c2d70c076034519b07d4d8cf8446c.
[DEBUG][22-06-12][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for 316ee10e1a8ba9282bcb72347733cb56.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job 8903afdeabb4299f1f07ffcfdd3f2827 for job leader monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job 8903afdeabb4299f1f07ffcfdd3f2827. Address: akka://flink/user/rpc/jobmanager_3, leader id: b1dac787c906d03a41dca1034ce34a8e.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 41dca103-4ce3-4a8e-b1da-c787c906d03a.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor e2f82294-c83e-43b2-8a52-8f8bc70229f3.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 8903afdeabb4299f1f07ffcfdd3f2827.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job 8903afdeabb4299f1f07ffcfdd3f2827.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job 8903afdeabb4299f1f07ffcfdd3f2827.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor e2f82294-c83e-43b2-8a52-8f8bc70229f3 @ localhost (dataPort=-1).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer 316ee10e1a8ba9282bcb72347733cb56 to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot 316ee10e1a8ba9282bcb72347733cb56 @ e2f82294-c83e-43b2-8a52-8f8bc70229f3 @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{be5e3907b7805487bcbca1ee2c5ff9bc}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot 316ee10e1a8ba9282bcb72347733cb56 for slot request id SlotRequestId{be5e3907b7805487bcbca1ee2c5ff9bc}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id 316ee10e1a8ba9282bcb72347733cb56.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{60f00ac7370b54aa913463f16f37b733}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{be5e3907b7805487bcbca1ee2c5ff9bc})
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (7b5c91023318775d3ec0c77cd1e51073) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id 7b5c91023318775d3ec0c77cd1e51073 to e2f82294-c83e-43b2-8a52-8f8bc70229f3 @ localhost (dataPort=-1) with allocation id 316ee10e1a8ba9282bcb72347733cb56
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 316ee10e1a8ba9282bcb72347733cb56.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id 316ee10e1a8ba9282bcb72347733cb56 for local state stores for job 8903afdeabb4299f1f07ffcfdd3f2827.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_316ee10e1a8ba9282bcb72347733cb56], jobID=8903afdeabb4299f1f07ffcfdd3f2827, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for 8903afdeabb4299f1f07ffcfdd3f2827 - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id 316ee10e1a8ba9282bcb72347733cb56.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (7b5c91023318775d3ec0c77cd1e51073), deploy into slot with allocation id 316ee10e1a8ba9282bcb72347733cb56.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (7b5c91023318775d3ec0c77cd1e51073) switched from CREATED to DEPLOYING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (7b5c91023318775d3ec0c77cd1e51073) [DEPLOYING]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 316ee10e1a8ba9282bcb72347733cb56.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (7b5c91023318775d3ec0c77cd1e51073) [DEPLOYING].
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 7b5c91023318775d3ec0c77cd1e51073 at library cache manager took 1 milliseconds
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (7b5c91023318775d3ec0c77cd1e51073) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@30838475
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (7b5c91023318775d3ec0c77cd1e51073) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Sink: Print to Std. Out (1/1)#0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (7b5c91023318775d3ec0c77cd1e51073) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Sink: Print to Std. Out (1/1)#0
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655009891133
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker1:9092 (id: -1 rack: null) using address worker1/47.104.89.120
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -1
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Completed connection to node -1. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating API versions fetch from node -1.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Recorded API versions for node -1: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1:9092 (id: -1 rack: null)
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[], controller=worker1.cluster:9092 (id: 1 rack: null)}
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 will start reading the following 1 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic='metric_topic', partition=0}]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (7b5c91023318775d3ec0c77cd1e51073) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (7b5c91023318775d3ec0c77cd1e51073) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='metric_topic', partition=0}=-915623761773}.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initializing the Kafka consumer
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655009891632
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer initialized
[INFO][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Subscribed to partition(s): metric_topic-0
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending FindCoordinator request to broker worker1:9092 (id: -1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker1:9092 (id: -1 rack: null) using address worker1/47.104.89.120
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -1
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Completed connection to node -1. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating API versions fetch from node -1.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Recorded API versions for node -1: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='metric_topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1:9092 (id: -1 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updating last seen epoch from null to 0 for partition metric_topic-0
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[PartitionInfoAndEpoch{partitionInfo=Partition(topic = metric_topic, partition = 0, leader = 2, replicas = [2], isr = [2], offlineReplicas = []), epoch=0}], controller=worker1.cluster:9092 (id: 1 rack: null)}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Received FindCoordinator response ClientResponse(receivedTimeMs=1655009891806, latencyMs=118, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=3, clientId=consumer-metric_consumer_g-2, correlationId=0), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='NONE', nodeId=2, host='worker2cluster', port=9092))
[INFO][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Discovered group coordinator worker2cluster:9092 (id: 2147483645 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker2cluster:9092 (id: 2147483645 rack: null) using address worker2cluster/47.104.108.98
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Fetching committed offsets for partitions: [metric_topic-0]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 99ddec078f410adbf7d6d0f86efb0e7e.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 99ddec078f410adbf7d6d0f86efb0e7e.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 4874fa23a682faa8bcf19ec8eea2814a.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from e2f82294-c83e-43b2-8a52-8f8bc70229f3.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 2d6d9699f4e43f8b629763b6229bfa49: SlotReport{SlotStatus{slotID=e2f82294-c83e-43b2-8a52-8f8bc70229f3_0, allocationID=316ee10e1a8ba9282bcb72347733cb56, jobID=8903afdeabb4299f1f07ffcfdd3f2827, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor e2f82294-c83e-43b2-8a52-8f8bc70229f3: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 4874fa23a682faa8bcf19ec8eea2814a.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from e2f82294-c83e-43b2-8a52-8f8bc70229f3.
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-4b544960-d5a5-479f-b338-ddbbcc1bb8a3
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-4953d168-fb41-4760-aae4-4272ac0d83d1
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-dcc3cf1d-cd82-4115-b9a9-68d9fd77c848
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:52239
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=2, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '7df19f87deec5680128845fd9a6ca18d' for node 'Sink: Print to Std. Out-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-43c390cc-bb21-437f-b297-6b1e15d0b4c2
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:52295 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-2829f9b9-06ca-4099-998a-16e9633099a6
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-fca97e42-2f48-48aa-931c-c866d798865a
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: fe66ff2c-068c-4e59-8267-641cd585480c
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 369 GB (80.22% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-45696ee3-ea1d-45b2-b1fc-bad928fd6090 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-531d81df-9c2a-4fa3-94fb-534b01298c20 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-37413865-e461-4e11-9fa0-824d2624569b
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2908 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:52296
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:52296
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:52296.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:52296 was granted leadership with leaderSessionID=cbc4bd27-d97f-410d-8917-93bed76ba57d
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:52296 , session=cbc4bd27-d97f-410d-8917-93bed76ba57d
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-531d81df-9c2a-4fa3-94fb-534b01298c20
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-45696ee3-ea1d-45b2-b1fc-bad928fd6090
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:52295
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-37413865-e461-4e11-9fa0-824d2624569b
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=2, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '7df19f87deec5680128845fd9a6ca18d' for node 'Sink: Print to Std. Out-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-4140b4dc-a0dc-4b57-87d8-ccad76b25874
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:52300 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-280f4504-6676-444b-96b6-2b41b81df84e
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-35c1da14-8187-4adf-9ae3-ba8ac7aea1cb
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: d9969f1e-ca4e-4932-8042-4546a01627f5
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 369 GB (80.22% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-b8aa133d-cd99-47c3-a9d3-1c1232abcd6a for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-15b8cf98-912d-40a5-baca-0bf60703185c for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-dd39f726-3cc8-4fd9-8eff-29c3b8e172f3
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cff0139 under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@b25b095 under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@5cb042da under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@59c33386 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@571a9686 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@719d35e8 under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@2f651f93 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@6ffa56fa under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6c575325 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@747d1932 under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@736309a9 under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e93dcb9 under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@188b6035 under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4a34e9f under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@6f6621e3 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3fc05ea2 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@7c891ba7 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@6240651f under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3cf7298d under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@1ff55ff under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@351f2244 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@5496c165 under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51a8313b under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@2a03d65c under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@6642dc5a under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@43da41e under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@148c7c4b under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@2009f9b0 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@50d951e7 under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@39ad12b6 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@4eb45fec under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@211febf3 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3bd3d05e under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@6aba5d30 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@61d34b4 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@588307f7 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@7df76d99 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@459cfcca under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@2acbc859 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@6ab7ce48 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2c6aed22 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@e322ec9 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@7acfb656 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@2e5ee2c9 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@55a609dd under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@4afd21c6 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@4d0753c9 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@1416a80a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@719bb3b4 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@52cb4f50 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@25a5c7db under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@4d27d9d under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@28f878a0 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@20411320 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2b5183ec under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3c782d8e under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@572e6fd9 under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@7f5eae0f under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@58b71ceb under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@255e5e2e under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@12abdfb under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0e5507 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@6bbe50c9 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@3c46dcbe under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@68577ba8 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1108adc8 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@8a98f38 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@20011bf under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2922 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:52301
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:52301
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:52301.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:52301 was granted leadership with leaderSessionID=923e778b-4aba-4014-97a5-5cfe73c65603
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:52301 , session=923e778b-4aba-4014-97a5-5cfe73c65603
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id f1639d1b-fe2f-4c43-a796-f35ae97b21c6. Creating new DispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 82c5bce5f7af7a87deda52a2b2a4483b
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=deda52a2-b2a4-483b-82c5-bce5f7af7a87
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(82c5bce5f7af7a87deda52a2b2a4483b).
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=f1639d1b-fe2f-4c43-a796-f35ae97b21c6
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID d9969f1e-ca4e-4932-8042-4546a01627f5 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission 279202112f7a674d8b8fbe4607ab3b75 (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job 279202112f7a674d8b8fbe4607ab3b75 (metricStream).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id b86e8e41825ac50110e05c3a0d1c3154.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor d9969f1e-ca4e-4932-8042-4546a01627f5 under b86e8e41825ac50110e05c3a0d1c3154 at the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job 279202112f7a674d8b8fbe4607ab3b75.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under 8921a6de-b015-42a7-8b5a-9a9dda8b7419.
[DEBUG][22-06-12][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (279202112f7a674d8b8fbe4607ab3b75).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (279202112f7a674d8b8fbe4607ab3b75).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (279202112f7a674d8b8fbe4607ab3b75).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 0 ms.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (279202112f7a674d8b8fbe4607ab3b75).
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Sink: Print to Std. Out) to 0 predecessors.
[INFO][22-06-12][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 2 ms
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (279202112f7a674d8b8fbe4607ab3b75).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1aa87dc5
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job 279202112f7a674d8b8fbe4607ab3b75 after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@db2c45d for metricStream (279202112f7a674d8b8fbe4607ab3b75).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job 279202112f7a674d8b8fbe4607ab3b75 under leader id 8921a6de-b015-42a7-8b5a-9a9dda8b7419.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership 8921a6de-b015-42a7-8b5a-9a9dda8b7419.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=8921a6de-b015-42a7-8b5a-9a9dda8b7419
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (279202112f7a674d8b8fbe4607ab3b75) under job master id 8b5a9a9dda8b74198921a6deb01542a7.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (279202112f7a674d8b8fbe4607ab3b75) switched from state CREATED to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (991cb9485c84d546b86bcc69681f72e0) switched from CREATED to SCHEDULED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 279202112f7a674d8b8fbe4607ab3b75.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{0525d5952f04a4131a33d897f8b88750}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(82c5bce5f7af7a87deda52a2b2a4483b)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job 279202112f7a674d8b8fbe4607ab3b75 to job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager 8b5a9a9dda8b74198921a6deb01542a7@akka://flink/user/rpc/jobmanager_3 for job 279202112f7a674d8b8fbe4607ab3b75.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job 279202112f7a674d8b8fbe4607ab3b75 has a new job leader 8921a6de-b015-42a7-8b5a-9a9dda8b7419@akka://flink/user/rpc/jobmanager_3.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager 8b5a9a9dda8b74198921a6deb01542a7@akka://flink/user/rpc/jobmanager_3 for job 279202112f7a674d8b8fbe4607ab3b75.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: 82c5bce5f7af7a87deda52a2b2a4483b.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job 279202112f7a674d8b8fbe4607ab3b75: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 279202112f7a674d8b8fbe4607ab3b75.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot d9969f1e-ca4e-4932-8042-4546a01627f5_0 for job 279202112f7a674d8b8fbe4607ab3b75 with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request 5723123a0650e4ceaddd2f6bf8494f9a for job 279202112f7a674d8b8fbe4607ab3b75 from resource manager with leader id 82c5bce5f7af7a87deda52a2b2a4483b.
[DEBUG][22-06-12][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for 5723123a0650e4ceaddd2f6bf8494f9a.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job 279202112f7a674d8b8fbe4607ab3b75 for job leader monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job 279202112f7a674d8b8fbe4607ab3b75. Address: akka://flink/user/rpc/jobmanager_3, leader id: 8b5a9a9dda8b74198921a6deb01542a7.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 8921a6de-b015-42a7-8b5a-9a9dda8b7419.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 279202112f7a674d8b8fbe4607ab3b75.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job 279202112f7a674d8b8fbe4607ab3b75.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job 279202112f7a674d8b8fbe4607ab3b75.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor d9969f1e-ca4e-4932-8042-4546a01627f5 @ localhost (dataPort=-1).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer 5723123a0650e4ceaddd2f6bf8494f9a to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot 5723123a0650e4ceaddd2f6bf8494f9a @ d9969f1e-ca4e-4932-8042-4546a01627f5 @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot 5723123a0650e4ceaddd2f6bf8494f9a for slot request id SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id 5723123a0650e4ceaddd2f6bf8494f9a.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{0525d5952f04a4131a33d897f8b88750}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e})
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (991cb9485c84d546b86bcc69681f72e0) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id 991cb9485c84d546b86bcc69681f72e0 to d9969f1e-ca4e-4932-8042-4546a01627f5 @ localhost (dataPort=-1) with allocation id 5723123a0650e4ceaddd2f6bf8494f9a
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 5723123a0650e4ceaddd2f6bf8494f9a.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id 5723123a0650e4ceaddd2f6bf8494f9a for local state stores for job 279202112f7a674d8b8fbe4607ab3b75.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_5723123a0650e4ceaddd2f6bf8494f9a], jobID=279202112f7a674d8b8fbe4607ab3b75, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for 279202112f7a674d8b8fbe4607ab3b75 - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id 5723123a0650e4ceaddd2f6bf8494f9a.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (991cb9485c84d546b86bcc69681f72e0), deploy into slot with allocation id 5723123a0650e4ceaddd2f6bf8494f9a.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (991cb9485c84d546b86bcc69681f72e0) switched from CREATED to DEPLOYING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (991cb9485c84d546b86bcc69681f72e0) [DEPLOYING]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (991cb9485c84d546b86bcc69681f72e0) [DEPLOYING].
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 991cb9485c84d546b86bcc69681f72e0 at library cache manager took 1 milliseconds
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (991cb9485c84d546b86bcc69681f72e0) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 5723123a0650e4ceaddd2f6bf8494f9a.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@36074c2a
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (991cb9485c84d546b86bcc69681f72e0) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Sink: Print to Std. Out (1/1)#0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (991cb9485c84d546b86bcc69681f72e0) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Sink: Print to Std. Out (1/1)#0
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655009987080
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance b86e8e41825ac50110e05c3a0d1c3154: SlotReport{SlotStatus{slotID=d9969f1e-ca4e-4932-8042-4546a01627f5_0, allocationID=5723123a0650e4ceaddd2f6bf8494f9a, jobID=279202112f7a674d8b8fbe4607ab3b75, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor d9969f1e-ca4e-4932-8042-4546a01627f5: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance b86e8e41825ac50110e05c3a0d1c3154: SlotReport{SlotStatus{slotID=d9969f1e-ca4e-4932-8042-4546a01627f5_0, allocationID=5723123a0650e4ceaddd2f6bf8494f9a, jobID=279202112f7a674d8b8fbe4607ab3b75, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor d9969f1e-ca4e-4932-8042-4546a01627f5: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance b86e8e41825ac50110e05c3a0d1c3154: SlotReport{SlotStatus{slotID=d9969f1e-ca4e-4932-8042-4546a01627f5_0, allocationID=5723123a0650e4ceaddd2f6bf8494f9a, jobID=279202112f7a674d8b8fbe4607ab3b75, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor d9969f1e-ca4e-4932-8042-4546a01627f5: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance b86e8e41825ac50110e05c3a0d1c3154: SlotReport{SlotStatus{slotID=d9969f1e-ca4e-4932-8042-4546a01627f5_0, allocationID=5723123a0650e4ceaddd2f6bf8494f9a, jobID=279202112f7a674d8b8fbe4607ab3b75, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor d9969f1e-ca4e-4932-8042-4546a01627f5: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance b86e8e41825ac50110e05c3a0d1c3154: SlotReport{SlotStatus{slotID=d9969f1e-ca4e-4932-8042-4546a01627f5_0, allocationID=5723123a0650e4ceaddd2f6bf8494f9a, jobID=279202112f7a674d8b8fbe4607ab3b75, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor d9969f1e-ca4e-4932-8042-4546a01627f5: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from bcea2cef0a47e4fb19cfbad14e7ce8a3.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance b86e8e41825ac50110e05c3a0d1c3154: SlotReport{SlotStatus{slotID=d9969f1e-ca4e-4932-8042-4546a01627f5_0, allocationID=5723123a0650e4ceaddd2f6bf8494f9a, jobID=279202112f7a674d8b8fbe4607ab3b75, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor d9969f1e-ca4e-4932-8042-4546a01627f5: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 1d903d28e5e92dbfcbd147d3f8b31e34.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (991cb9485c84d546b86bcc69681f72e0) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (991cb9485c84d546b86bcc69681f72e0).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 (991cb9485c84d546b86bcc69681f72e0) [FAILED]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Print to Std. Out (1/1)#0 991cb9485c84d546b86bcc69681f72e0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Sink: Print to Std. Out (1/1) (991cb9485c84d546b86bcc69681f72e0) switched from INITIALIZING to FAILED on d9969f1e-ca4e-4932-8042-4546a01627f5 @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{0525d5952f04a4131a33d897f8b88750}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 5723123a0650e4ceaddd2f6bf8494f9a.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{5ea8bae4f9c0a0f2891daa954cec688e})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 5723123a0650e4ceaddd2f6bf8494f9a @ d9969f1e-ca4e-4932-8042-4546a01627f5 @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 279202112f7a674d8b8fbe4607ab3b75.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job 279202112f7a674d8b8fbe4607ab3b75
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job 279202112f7a674d8b8fbe4607ab3b75: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]1 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0. 
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (279202112f7a674d8b8fbe4607ab3b75) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (279202112f7a674d8b8fbe4607ab3b75) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph 279202112f7a674d8b8fbe4607ab3b75 reached terminal state FAILED.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job 279202112f7a674d8b8fbe4607ab3b75.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job 279202112f7a674d8b8fbe4607ab3b75 under leader id 8921a6de-b015-42a7-8b5a-9a9dda8b7419 reached a globally terminal state FAILED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job 279202112f7a674d8b8fbe4607ab3b75.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job 279202112f7a674d8b8fbe4607ab3b75 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job 279202112f7a674d8b8fbe4607ab3b75.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection bcea2cef0a47e4fb19cfbad14e7ce8a3.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job 279202112f7a674d8b8fbe4607ab3b75 under leader id 8921a6de-b015-42a7-8b5a-9a9dda8b7419.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection d9969f1e-ca4e-4932-8042-4546a01627f5 because: The TaskExecutor is shutting down.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor b86e8e41825ac50110e05c3a0d1c3154 from the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 279202112f7a674d8b8fbe4607ab3b75.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor d9969f1e-ca4e-4932-8042-4546a01627f5.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job 279202112f7a674d8b8fbe4607ab3b75.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(279202112f7a674d8b8fbe4607ab3b75).
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor d9969f1e-ca4e-4932-8042-4546a01627f5 because: Stopping JobMaster for job metricStream(279202112f7a674d8b8fbe4607ab3b75).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [5723123a0650e4ceaddd2f6bf8494f9a].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(279202112f7a674d8b8fbe4607ab3b75).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: 5723123a0650e4ceaddd2f6bf8494f9a, jobId: 279202112f7a674d8b8fbe4607ab3b75).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection bcea2cef0a47e4fb19cfbad14e7ce8a3.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(279202112f7a674d8b8fbe4607ab3b75).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job 279202112f7a674d8b8fbe4607ab3b75 from job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager 8b5a9a9dda8b74198921a6deb01542a7@akka://flink/user/rpc/jobmanager_3 for job 279202112f7a674d8b8fbe4607ab3b75 from the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 279202112f7a674d8b8fbe4607ab3b75.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 279202112f7a674d8b8fbe4607ab3b75.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 5723123a0650e4ceaddd2f6bf8494f9a because: Stopping JobMaster for job metricStream(279202112f7a674d8b8fbe4607ab3b75).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 5723123a0650e4ceaddd2f6bf8494f9a.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 5723123a0650e4ceaddd2f6bf8494f9a.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 5723123a0650e4ceaddd2f6bf8494f9a.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-b8aa133d-cd99-47c3-a9d3-1c1232abcd6a
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job 279202112f7a674d8b8fbe4607ab3b75 has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job 279202112f7a674d8b8fbe4607ab3b75 under leader id 8921a6de-b015-42a7-8b5a-9a9dda8b7419 has been terminated.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-12][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-15b8cf98-912d-40a5-baca-0bf60703185c
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-dd39f726-3cc8-4fd9-8eff-29c3b8e172f3
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-12][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:52300
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming OneInputTransformation{id=2, name='Map', outputType=PojoType<renaissance.bean.MetricBean, fields = [hostName: String, id: String, name: String, timestamp: String, value: String]>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=3, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 3
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '570f707193e0fe32f4d86d067aba243b' for node 'Map-2' {id: 2, parallelism: 1, user function: renaissance.profunc.MetricBeanMap}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'b728d985904d42b0fdd945a9e3253fca' for node 'Sink: Print to Std. Out-3' {id: 3, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-5699da17-b224-4fae-bd35-03bf1a2d983a
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:54253 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-fb52184b-eddb-447c-8d60-3b36ea747d01
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-b2667fdc-17fe-4481-b442-1296f7699b8a
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: 483ab34e-dc91-40be-b81e-0dfecc6a86d1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 368 GB (80.00% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-d48c9e23-6b2d-4368-af22-67a47876dbcd for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-31b58b75-64f2-4b53-90d0-7a0ac186b62c for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-e40f033b-884b-4591-96a9-da62f4644c66
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@2cc04358 under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@2cc04358 under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@12ffd1de under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@12ffd1de under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@3d278b4d under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@3d278b4d under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@4096aa05 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@4096aa05 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@9d3c67 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@9d3c67 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@6c806c8b under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@6c806c8b under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@6dfcffb5 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@6dfcffb5 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@184fb68d under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@184fb68d under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@71d8cfe7 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@71d8cfe7 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@1e530163 under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@1e530163 under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@14d8444b under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@14d8444b under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@71466383 under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@71466383 under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@46d63dbb under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@46d63dbb under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4088741b under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@4088741b under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@16a49a5d under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@16a49a5d under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@54bca971 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@54bca971 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@23706db8 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@23706db8 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@205bed61 under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@205bed61 under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@129fed45 under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@129fed45 under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@23592946 under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@23592946 under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@3e48d38 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@3e48d38 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@7c2b58c0 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@7c2b58c0 under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@11b377c5 under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@11b377c5 under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@7bca6fac under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@7bca6fac under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@5c60b0a0 under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@5c60b0a0 under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@7a2b1eb4 under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@7a2b1eb4 under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@702c436b under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@702c436b under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@5833f5cd under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@5833f5cd under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@10fbbdb under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@10fbbdb under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@23f3dbf0 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@23f3dbf0 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@31d6f3fe under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@31d6f3fe under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@760cf594 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@760cf594 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@aa149ed under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@aa149ed under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@37303f12 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@37303f12 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@31ff6309 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@31ff6309 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@204e90f7 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@204e90f7 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@20a05b32 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@20a05b32 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@165e389b under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@165e389b under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@5c73f672 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@5c73f672 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@8ee0c23 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@8ee0c23 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2ab5afc7 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@2ab5afc7 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@4dc8c0ea under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@4dc8c0ea under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@e4b6f47 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@e4b6f47 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@763cf5b9 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@763cf5b9 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@71f0b72e under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@71f0b72e under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@7a34f66a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@7a34f66a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@2f508f3c under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@2f508f3c under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@3ed03652 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@3ed03652 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@4aedaf61 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@4aedaf61 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@173797f0 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@173797f0 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@3c35c345 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@3c35c345 under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@3681037 under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@3681037 under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@2459319c under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@2459319c under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@ffaaaf0 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@ffaaaf0 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@1dc76fa1 under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@1dc76fa1 under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@5eed2d86 under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@5eed2d86 under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@33d53216 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@33d53216 under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@69a2b3b6 under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@69a2b3b6 under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@4f3e7344 under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@4f3e7344 under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@7808f638 under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@7808f638 under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@62d73ead under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@62d73ead under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@1e141e42 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@1e141e42 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@228cea97 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@228cea97 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@1d0a61c8 under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@1d0a61c8 under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@46731692 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@46731692 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@782bf610 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@782bf610 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@3db663d0 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@3db663d0 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@73fc518f under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@73fc518f under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 4485 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:54254
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:54254
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:54254.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:54254 was granted leadership with leaderSessionID=5e28203c-c144-464a-8d9b-9955489fce80
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:54254 , session=5e28203c-c144-464a-8d9b-9955489fce80
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id 600b6725-a51c-42fb-b5ba-c9db0216d6e6. Creating new DispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token b4e4b3c37e58d44dd7db72f333164e42
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=d7db72f3-3316-4e42-b4e4-b3c37e58d44d
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b4e4b3c37e58d44dd7db72f333164e42).
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=600b6725-a51c-42fb-b5ba-c9db0216d6e6
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission 71e2a55c32c250d2c80f9beee0a1e5a9 (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job 71e2a55c32c250d2c80f9beee0a1e5a9 (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID 483ab34e-dc91-40be-b81e-0dfecc6a86d1 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 4295a053c79154dfe0f4c108280c7b08.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor 483ab34e-dc91-40be-b81e-0dfecc6a86d1 under 4295a053c79154dfe0f4c108280c7b08 at the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under e70a5fd6-a093-4997-b81c-0c053116dee4.
[DEBUG][22-06-12][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (71e2a55c32c250d2c80f9beee0a1e5a9).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (71e2a55c32c250d2c80f9beee0a1e5a9).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (71e2a55c32c250d2c80f9beee0a1e5a9).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 1 ms.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (71e2a55c32c250d2c80f9beee0a1e5a9).
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Map -> Sink: Print to Std. Out) to 0 predecessors.
[INFO][22-06-12][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 1 ms
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (71e2a55c32c250d2c80f9beee0a1e5a9).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6b278a35
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job 71e2a55c32c250d2c80f9beee0a1e5a9 after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@74a12b9 for metricStream (71e2a55c32c250d2c80f9beee0a1e5a9).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job 71e2a55c32c250d2c80f9beee0a1e5a9 under leader id e70a5fd6-a093-4997-b81c-0c053116dee4.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership e70a5fd6-a093-4997-b81c-0c053116dee4.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=e70a5fd6-a093-4997-b81c-0c053116dee4
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (71e2a55c32c250d2c80f9beee0a1e5a9) under job master id b81c0c053116dee4e70a5fd6a0934997.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (71e2a55c32c250d2c80f9beee0a1e5a9) switched from state CREATED to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1) (1569c71b14cc1b3caca16c81a3285e8b) switched from CREATED to SCHEDULED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{990c91f279e34438973d62062aa688f7}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{990c91f279e34438973d62062aa688f7} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 71e2a55c32c250d2c80f9beee0a1e5a9.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{128c6bda31a7cde246efcd8b5e1cdad1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{990c91f279e34438973d62062aa688f7})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b4e4b3c37e58d44dd7db72f333164e42)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job 71e2a55c32c250d2c80f9beee0a1e5a9 to job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager b81c0c053116dee4e70a5fd6a0934997@akka://flink/user/rpc/jobmanager_3 for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job 71e2a55c32c250d2c80f9beee0a1e5a9 has a new job leader e70a5fd6-a093-4997-b81c-0c053116dee4@akka://flink/user/rpc/jobmanager_3.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager b81c0c053116dee4e70a5fd6a0934997@akka://flink/user/rpc/jobmanager_3 for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: b4e4b3c37e58d44dd7db72f333164e42.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job 71e2a55c32c250d2c80f9beee0a1e5a9: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot 483ab34e-dc91-40be-b81e-0dfecc6a86d1_0 for job 71e2a55c32c250d2c80f9beee0a1e5a9 with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request 420106f0c06e502c24175f1df3c0a1ae for job 71e2a55c32c250d2c80f9beee0a1e5a9 from resource manager with leader id b4e4b3c37e58d44dd7db72f333164e42.
[DEBUG][22-06-12][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for 420106f0c06e502c24175f1df3c0a1ae.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job 71e2a55c32c250d2c80f9beee0a1e5a9 for job leader monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job 71e2a55c32c250d2c80f9beee0a1e5a9. Address: akka://flink/user/rpc/jobmanager_3, leader id: b81c0c053116dee4e70a5fd6a0934997.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id e70a5fd6-a093-4997-b81c-0c053116dee4.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job 71e2a55c32c250d2c80f9beee0a1e5a9.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor 483ab34e-dc91-40be-b81e-0dfecc6a86d1 @ localhost (dataPort=-1).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer 420106f0c06e502c24175f1df3c0a1ae to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot 420106f0c06e502c24175f1df3c0a1ae @ 483ab34e-dc91-40be-b81e-0dfecc6a86d1 @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{990c91f279e34438973d62062aa688f7}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot 420106f0c06e502c24175f1df3c0a1ae for slot request id SlotRequestId{990c91f279e34438973d62062aa688f7}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id 420106f0c06e502c24175f1df3c0a1ae.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{128c6bda31a7cde246efcd8b5e1cdad1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{990c91f279e34438973d62062aa688f7})
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1) (1569c71b14cc1b3caca16c81a3285e8b) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id 1569c71b14cc1b3caca16c81a3285e8b to 483ab34e-dc91-40be-b81e-0dfecc6a86d1 @ localhost (dataPort=-1) with allocation id 420106f0c06e502c24175f1df3c0a1ae
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 420106f0c06e502c24175f1df3c0a1ae.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id 420106f0c06e502c24175f1df3c0a1ae for local state stores for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_420106f0c06e502c24175f1df3c0a1ae], jobID=71e2a55c32c250d2c80f9beee0a1e5a9, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for 71e2a55c32c250d2c80f9beee0a1e5a9 - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id 420106f0c06e502c24175f1df3c0a1ae.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b), deploy into slot with allocation id 420106f0c06e502c24175f1df3c0a1ae.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b) switched from CREATED to DEPLOYING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b) [DEPLOYING]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 420106f0c06e502c24175f1df3c0a1ae.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b) [DEPLOYING].
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 1569c71b14cc1b3caca16c81a3285e8b at library cache manager took 0 milliseconds
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@17d22830
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1) (1569c71b14cc1b3caca16c81a3285e8b) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_b728d985904d42b0fdd945a9e3253fca_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamMap_570f707193e0fe32f4d86d067aba243b_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655019688220
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker1:9092 (id: -1 rack: null) using address worker1/47.104.89.120
[DEBUG][22-06-12][org.apache.kafka.common.network.Selector][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Created socket with SO_RCVBUF = 65800, SO_SNDBUF = 131600, SO_TIMEOUT = 0 to node -1
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Completed connection to node -1. Fetching API versions.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating API versions fetch from node -1.
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Recorded API versions for node -1: (Produce(0): 0 to 8 [usable: 8], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 9 [usable: 9], LeaderAndIsr(4): 0 to 4 [usable: 4], StopReplica(5): 0 to 2 [usable: 2], UpdateMetadata(6): 0 to 6 [usable: 6], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 6 [usable: 6], FindCoordinator(10): 0 to 3 [usable: 3], JoinGroup(11): 0 to 6 [usable: 6], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 4 [usable: 4], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 3 [usable: 3], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 5 [usable: 5], DeleteTopics(20): 0 to 4 [usable: 4], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 2 [usable: 2], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0])
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending metadata request MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node worker1:9092 (id: -1 rack: null)
[INFO][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Cluster ID: w6wKIiBSR8Sv1AUq2edUOA
[DEBUG][22-06-12][org.apache.kafka.clients.Metadata][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='w6wKIiBSR8Sv1AUq2edUOA', nodes=[worker1.cluster:9092 (id: 1 rack: null), worker2cluster:9092 (id: 2 rack: null), worker3.cluster:9092 (id: 3 rack: null)], partitions=[], controller=worker1.cluster:9092 (id: 1 rack: null)}
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 will start reading the following 1 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic='mem.used', partition=0}]
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1) (1569c71b14cc1b3caca16c81a3285e8b) switched from INITIALIZING to RUNNING.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='mem.used', partition=0}=-915623761773}.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initializing the Kafka consumer
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655019688567
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer initialized
[INFO][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Subscribed to partition(s): mem.used-0
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending FindCoordinator request to broker worker2:9092 (id: -2 rack: null)
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4295a053c79154dfe0f4c108280c7b08: SlotReport{SlotStatus{slotID=483ab34e-dc91-40be-b81e-0dfecc6a86d1_0, allocationID=420106f0c06e502c24175f1df3c0a1ae, jobID=71e2a55c32c250d2c80f9beee0a1e5a9, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 483ab34e-dc91-40be-b81e-0dfecc6a86d1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4295a053c79154dfe0f4c108280c7b08: SlotReport{SlotStatus{slotID=483ab34e-dc91-40be-b81e-0dfecc6a86d1_0, allocationID=420106f0c06e502c24175f1df3c0a1ae, jobID=71e2a55c32c250d2c80f9beee0a1e5a9, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 483ab34e-dc91-40be-b81e-0dfecc6a86d1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4295a053c79154dfe0f4c108280c7b08: SlotReport{SlotStatus{slotID=483ab34e-dc91-40be-b81e-0dfecc6a86d1_0, allocationID=420106f0c06e502c24175f1df3c0a1ae, jobID=71e2a55c32c250d2c80f9beee0a1e5a9, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 483ab34e-dc91-40be-b81e-0dfecc6a86d1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Coordinator discovery failed, refreshing metadata
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4295a053c79154dfe0f4c108280c7b08: SlotReport{SlotStatus{slotID=483ab34e-dc91-40be-b81e-0dfecc6a86d1_0, allocationID=420106f0c06e502c24175f1df3c0a1ae, jobID=71e2a55c32c250d2c80f9beee0a1e5a9, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 483ab34e-dc91-40be-b81e-0dfecc6a86d1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4295a053c79154dfe0f4c108280c7b08: SlotReport{SlotStatus{slotID=483ab34e-dc91-40be-b81e-0dfecc6a86d1_0, allocationID=420106f0c06e502c24175f1df3c0a1ae, jobID=71e2a55c32c250d2c80f9beee0a1e5a9, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 483ab34e-dc91-40be-b81e-0dfecc6a86d1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 9dda468198ec4b184575220f750e58e2.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 4295a053c79154dfe0f4c108280c7b08: SlotReport{SlotStatus{slotID=483ab34e-dc91-40be-b81e-0dfecc6a86d1_0, allocationID=420106f0c06e502c24175f1df3c0a1ae, jobID=71e2a55c32c250d2c80f9beee0a1e5a9, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 483ab34e-dc91-40be-b81e-0dfecc6a86d1: PartitionReport{entries=[]}.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from fe8e7f414f3e55fed43328d22ea2c2d0.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[WARN][22-06-12][org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher]Error while closing Kafka consumer
java.lang.NullPointerException
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.run(KafkaConsumerThread.java:283)
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b) switched from RUNNING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-0 could be determined

[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 (1569c71b14cc1b3caca16c81a3285e8b) [FAILED]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1)#0 1569c71b14cc1b3caca16c81a3285e8b.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> Sink: Print to Std. Out (1/1) (1569c71b14cc1b3caca16c81a3285e8b) switched from RUNNING to FAILED on 483ab34e-dc91-40be-b81e-0dfecc6a86d1 @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{128c6bda31a7cde246efcd8b5e1cdad1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{990c91f279e34438973d62062aa688f7})
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{990c91f279e34438973d62062aa688f7})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{990c91f279e34438973d62062aa688f7}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 420106f0c06e502c24175f1df3c0a1ae.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{990c91f279e34438973d62062aa688f7})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 420106f0c06e502c24175f1df3c0a1ae @ 483ab34e-dc91-40be-b81e-0dfecc6a86d1 @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 71e2a55c32c250d2c80f9beee0a1e5a9.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job 71e2a55c32c250d2c80f9beee0a1e5a9
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job 71e2a55c32c250d2c80f9beee0a1e5a9: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]1 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0. 
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (71e2a55c32c250d2c80f9beee0a1e5a9) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-0 could be determined
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (71e2a55c32c250d2c80f9beee0a1e5a9) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph 71e2a55c32c250d2c80f9beee0a1e5a9 reached terminal state FAILED.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job 71e2a55c32c250d2c80f9beee0a1e5a9 under leader id e70a5fd6-a093-4997-b81c-0c053116dee4 reached a globally terminal state FAILED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job 71e2a55c32c250d2c80f9beee0a1e5a9 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition mem.used-0 could be determined
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job 71e2a55c32c250d2c80f9beee0a1e5a9 under leader id e70a5fd6-a093-4997-b81c-0c053116dee4.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(71e2a55c32c250d2c80f9beee0a1e5a9).
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection 9dda468198ec4b184575220f750e58e2.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection 483ab34e-dc91-40be-b81e-0dfecc6a86d1 because: The TaskExecutor is shutting down.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor 4295a053c79154dfe0f4c108280c7b08 from the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor 483ab34e-dc91-40be-b81e-0dfecc6a86d1.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job 71e2a55c32c250d2c80f9beee0a1e5a9.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor 483ab34e-dc91-40be-b81e-0dfecc6a86d1 because: Stopping JobMaster for job metricStream(71e2a55c32c250d2c80f9beee0a1e5a9).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [420106f0c06e502c24175f1df3c0a1ae].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(71e2a55c32c250d2c80f9beee0a1e5a9).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: 420106f0c06e502c24175f1df3c0a1ae, jobId: 71e2a55c32c250d2c80f9beee0a1e5a9).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection 9dda468198ec4b184575220f750e58e2.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(71e2a55c32c250d2c80f9beee0a1e5a9).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job 71e2a55c32c250d2c80f9beee0a1e5a9 from job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager b81c0c053116dee4e70a5fd6a0934997@akka://flink/user/rpc/jobmanager_3 for job 71e2a55c32c250d2c80f9beee0a1e5a9 from the resource manager.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 71e2a55c32c250d2c80f9beee0a1e5a9.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 420106f0c06e502c24175f1df3c0a1ae because: Stopping JobMaster for job metricStream(71e2a55c32c250d2c80f9beee0a1e5a9).
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 420106f0c06e502c24175f1df3c0a1ae.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 420106f0c06e502c24175f1df3c0a1ae.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 420106f0c06e502c24175f1df3c0a1ae.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job 71e2a55c32c250d2c80f9beee0a1e5a9 has been terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job 71e2a55c32c250d2c80f9beee0a1e5a9 under leader id e70a5fd6-a093-4997-b81c-0c053116dee4 has been terminated.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-d48c9e23-6b2d-4368-af22-67a47876dbcd
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-31b58b75-64f2-4b53-90d0-7a0ac186b62c
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-12][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-e40f033b-884b-4591-96a9-da62f4644c66
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-12][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:54253
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Boolean
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.String
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the [C
[DEBUG][22-06-12][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Integer
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming OneInputTransformation{id=2, name='Map', outputType=PojoType<renaissance.bean.MetricBean, fields = [hostName: String, id: String, name: String, timestamp: String, value: String]>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=3, name='Unnamed', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 3
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=4, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 4
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '8b66bce9f80f19736cb554745e27f15e' for node 'Map-2' {id: 2, parallelism: 1, user function: renaissance.profunc.MetricBeanMap}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '6b41151dfba2a5f165b47cdbc7b8eaaf' for node 'Sink: Unnamed-3' {id: 3, parallelism: 1, user function: renaissance.sink.MetricSink}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '0c23e62ea319711b24530a38c267707c' for node 'Sink: Print to Std. Out-4' {id: 4, parallelism: 1, user function: org.apache.flink.streaming.api.functions.sink.PrintSinkFunction}
[DEBUG][22-06-12][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-12][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-12][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-12][akka.event.EventStream]Default Loggers started
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-3e4e255c-22fd-4dfd-8554-c360c320e7da
[DEBUG][22-06-12][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:54665 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-9c09050b-871d-4b41-88f8-87fa16e3096f
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-3644852a-22aa-472f-a93a-49fedd14a0d1
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: e6839b05-7dad-488d-855a-45659e00388d
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 368 GB (80.00% usable)
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-758777ac-82ae-43a3-b51d-712a57ffe416 for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-7a910c7f-4377-4b69-a893-17b37be0d34b for spill files.
[INFO][22-06-12][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-3e62d0f9-6337-4f56-a1dc-61481ebd3183
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-12][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cf7298d under DELETE@/v1/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@3cf7298d under DELETE@/cluster.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@782bf610 under GET@/v1/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@782bf610 under GET@/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@3db663d0 under GET@/v1/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@3db663d0 under GET@/datasets.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@73fc518f under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@73fc518f under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@2de50ee4 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@2de50ee4 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@ad9e63e under GET@/v1/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@ad9e63e under GET@/jars.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@47fbc56 under POST@/v1/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@47fbc56 under POST@/jars/upload.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@151ef57f under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@151ef57f under DELETE@/jars/:jarid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@10895b16 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@10895b16 under GET@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@5524b72f under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@5524b72f under POST@/jars/:jarid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@2cc03cd1 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@2cc03cd1 under POST@/jars/:jarid/run.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e17913b under GET@/v1/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@4e17913b under GET@/jobmanager/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@149c3204 under GET@/v1/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@149c3204 under GET@/jobmanager/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@64f16277 under GET@/v1/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@64f16277 under GET@/jobmanager/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@497aec8c under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@497aec8c under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3b9632d1 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@3b9632d1 under GET@/jobmanager/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@4e6f2bb5 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@4e6f2bb5 under GET@/jobmanager/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@21e20ad5 under GET@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@21e20ad5 under GET@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3f628ce9 under POST@/v1/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@3f628ce9 under POST@/jobs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@35e8316e under GET@/v1/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@35e8316e under GET@/jobs/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@26d96e5 under GET@/v1/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@26d96e5 under GET@/jobs/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@336880df under GET@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@336880df under GET@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@1846579f under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@1846579f under PATCH@/jobs/:jobid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@6cd166b8 under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@6cd166b8 under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@2650f79 under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@2650f79 under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@75fc1992 under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@75fc1992 under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@5fac521d under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@5fac521d under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@38af1bf6 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@38af1bf6 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@129bd55d under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@129bd55d under GET@/jobs/:jobid/config.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@7be7e15 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@7be7e15 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@3abfe845 under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@3abfe845 under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@7a0f244f under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@7a0f244f under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3672276e under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@3672276e under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@4248b963 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@4248b963 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@7f08caf under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@7f08caf under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@4defd42 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@4defd42 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@2330e3e0 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@2330e3e0 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@24b4d544 under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@24b4d544 under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@27a2a089 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@27a2a089 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@54657dd2 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@54657dd2 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@706eab5d under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@706eab5d under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@72725ee1 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@72725ee1 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@40e60ece under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@40e60ece under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@3f9270ed under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@3f9270ed under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@3a230001 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@3a230001 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@5ac6c4f2 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@5ac6c4f2 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@2aa6311a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@2aa6311a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@61f39bb under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@61f39bb under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@249e0271 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@249e0271 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@4893b344 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@4893b344 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@53a665ad under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@53a665ad under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@2c0b4c83 under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@2c0b4c83 under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@78525ef9 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@78525ef9 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2d0ecb24 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2d0ecb24 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@4d654825 under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@4d654825 under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3bfc6a5e under GET@/v1/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@3bfc6a5e under GET@/overview.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@51b35e4e under POST@/v1/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@51b35e4e under POST@/savepoint-disposal.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@abff8b7 under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@abff8b7 under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@6d7cada5 under GET@/v1/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@6d7cada5 under GET@/taskmanagers.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@350a94ce under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@350a94ce under GET@/taskmanagers/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@7e00ed0f under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@7e00ed0f under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0fc838 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@b0fc838 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@3964d79 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@3964d79 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@62db0521 under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@62db0521 under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@1b4ae4e0 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@1b4ae4e0 under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@6ef1a1b9 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@6ef1a1b9 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@5fbdc49b under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@5fbdc49b under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@65753040 under GET@/v1/:*.
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@65753040 under GET@/:*.
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 5072 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-12][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:54666
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:54666
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:54666.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:54666 was granted leadership with leaderSessionID=d8c51ad9-5793-4144-8049-7d3064ac743e
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:54666 , session=d8c51ad9-5793-4144-8049-7d3064ac743e
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-12][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id ecdbbc6b-5348-4760-bd72-db2c99a566d9. Creating new DispatcherLeaderProcess.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 8248bb579b81077b8df40ad8926940fb
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-12][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=8df40ad8-9269-40fb-8248-bb579b81077b
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(8248bb579b81077b8df40ad8926940fb).
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=ecdbbc6b-5348-4760-bd72-db2c99a566d9
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission 2b88faefe20a577a6f96e3e980e94175 (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job 2b88faefe20a577a6f96e3e980e94175 (metricStream).
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID e6839b05-7dad-488d-855a-45659e00388d (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 010c27b31eebd21e35683ccfe0ff0d36.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor e6839b05-7dad-488d-855a-45659e00388d under 010c27b31eebd21e35683ccfe0ff0d36 at the slot manager.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job 2b88faefe20a577a6f96e3e980e94175.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under a39b001c-56db-4420-9bc5-6b782d10db3b.
[DEBUG][22-06-12][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (2b88faefe20a577a6f96e3e980e94175).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (2b88faefe20a577a6f96e3e980e94175).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (2b88faefe20a577a6f96e3e980e94175).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 0 ms.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (2b88faefe20a577a6f96e3e980e94175).
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out)) to 0 predecessors.
[INFO][22-06-12][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 1 ms
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (2b88faefe20a577a6f96e3e980e94175).
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@28069be5
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job 2b88faefe20a577a6f96e3e980e94175 after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-12][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@1caced73 for metricStream (2b88faefe20a577a6f96e3e980e94175).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job 2b88faefe20a577a6f96e3e980e94175 under leader id a39b001c-56db-4420-9bc5-6b782d10db3b.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership a39b001c-56db-4420-9bc5-6b782d10db3b.
[INFO][22-06-12][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=a39b001c-56db-4420-9bc5-6b782d10db3b
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (2b88faefe20a577a6f96e3e980e94175) under job master id 9bc56b782d10db3ba39b001c56db4420.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (2b88faefe20a577a6f96e3e980e94175) switched from state CREATED to RUNNING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1) (a8cb20c9154e477b7c3062bff73cb7b4) switched from CREATED to SCHEDULED.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{80772b11e01a9c6b00ecd0972f335429}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{80772b11e01a9c6b00ecd0972f335429} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 2b88faefe20a577a6f96e3e980e94175.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{aa51b48dbc58cc44e4c33560c938d2e7}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{80772b11e01a9c6b00ecd0972f335429})
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(8248bb579b81077b8df40ad8926940fb)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job 2b88faefe20a577a6f96e3e980e94175 to job leader id monitoring.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager 9bc56b782d10db3ba39b001c56db4420@akka://flink/user/rpc/jobmanager_3 for job 2b88faefe20a577a6f96e3e980e94175.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job 2b88faefe20a577a6f96e3e980e94175 has a new job leader a39b001c-56db-4420-9bc5-6b782d10db3b@akka://flink/user/rpc/jobmanager_3.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager 9bc56b782d10db3ba39b001c56db4420@akka://flink/user/rpc/jobmanager_3 for job 2b88faefe20a577a6f96e3e980e94175.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-12][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: 8248bb579b81077b8df40ad8926940fb.
[INFO][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job 2b88faefe20a577a6f96e3e980e94175: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 2b88faefe20a577a6f96e3e980e94175.
[DEBUG][22-06-12][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot e6839b05-7dad-488d-855a-45659e00388d_0 for job 2b88faefe20a577a6f96e3e980e94175 with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request 7f679c406f1c60f4c0d6bb0848ab88f4 for job 2b88faefe20a577a6f96e3e980e94175 from resource manager with leader id 8248bb579b81077b8df40ad8926940fb.
[DEBUG][22-06-12][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for 7f679c406f1c60f4c0d6bb0848ab88f4.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job 2b88faefe20a577a6f96e3e980e94175 for job leader monitoring.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job 2b88faefe20a577a6f96e3e980e94175. Address: akka://flink/user/rpc/jobmanager_3, leader id: 9bc56b782d10db3ba39b001c56db4420.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id a39b001c-56db-4420-9bc5-6b782d10db3b.
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-12][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor e6839b05-7dad-488d-855a-45659e00388d.
[DEBUG][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 2b88faefe20a577a6f96e3e980e94175.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job 2b88faefe20a577a6f96e3e980e94175.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job 2b88faefe20a577a6f96e3e980e94175.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor e6839b05-7dad-488d-855a-45659e00388d @ localhost (dataPort=-1).
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer 7f679c406f1c60f4c0d6bb0848ab88f4 to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot 7f679c406f1c60f4c0d6bb0848ab88f4 @ e6839b05-7dad-488d-855a-45659e00388d @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{80772b11e01a9c6b00ecd0972f335429}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot 7f679c406f1c60f4c0d6bb0848ab88f4 for slot request id SlotRequestId{80772b11e01a9c6b00ecd0972f335429}
[DEBUG][22-06-12][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id 7f679c406f1c60f4c0d6bb0848ab88f4.
[DEBUG][22-06-12][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{aa51b48dbc58cc44e4c33560c938d2e7}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{80772b11e01a9c6b00ecd0972f335429})
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1) (a8cb20c9154e477b7c3062bff73cb7b4) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1) (attempt #0) with attempt id a8cb20c9154e477b7c3062bff73cb7b4 to e6839b05-7dad-488d-855a-45659e00388d @ localhost (dataPort=-1) with allocation id 7f679c406f1c60f4c0d6bb0848ab88f4
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 7f679c406f1c60f4c0d6bb0848ab88f4.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id 7f679c406f1c60f4c0d6bb0848ab88f4 for local state stores for job 2b88faefe20a577a6f96e3e980e94175.
[DEBUG][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_7f679c406f1c60f4c0d6bb0848ab88f4], jobID=2b88faefe20a577a6f96e3e980e94175, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for 2b88faefe20a577a6f96e3e980e94175 - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id 7f679c406f1c60f4c0d6bb0848ab88f4.
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1)#0 (a8cb20c9154e477b7c3062bff73cb7b4), deploy into slot with allocation id 7f679c406f1c60f4c0d6bb0848ab88f4.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1)#0 (a8cb20c9154e477b7c3062bff73cb7b4) switched from CREATED to DEPLOYING.
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1)#0 (a8cb20c9154e477b7c3062bff73cb7b4) [DEPLOYING]
[INFO][22-06-12][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 7f679c406f1c60f4c0d6bb0848ab88f4.
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1)#0 (a8cb20c9154e477b7c3062bff73cb7b4) [DEPLOYING].
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task a8cb20c9154e477b7c3062bff73cb7b4 at library cache manager took 0 milliseconds
[DEBUG][22-06-12][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1)#0 (a8cb20c9154e477b7c3062bff73cb7b4) [DEPLOYING].
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@d62a648
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-12][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1)#0 (a8cb20c9154e477b7c3062bff73cb7b4) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1)#0.
[INFO][22-06-12][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1) (a8cb20c9154e477b7c3062bff73cb7b4) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-12][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Print to Std. Out) (1/1)#0
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_6b41151dfba2a5f165b47cdbc7b8eaaf_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.hadoop.util.Shell]setsid is not available on this machine. So not using it.
[DEBUG][22-06-12][org.apache.hadoop.util.Shell]setsid exited with exit code 0
[DEBUG][22-06-12][org.apache.hadoop.security.Groups] Creating new Groups object
[DEBUG][22-06-12][org.apache.hadoop.util.NativeCodeLoader]Trying to load the custom-built native-hadoop library...
[DEBUG][22-06-12][org.apache.hadoop.util.NativeCodeLoader]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[DEBUG][22-06-12][org.apache.hadoop.util.NativeCodeLoader]java.library.path=/Users/fabivs/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[WARN][22-06-12][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[DEBUG][22-06-12][org.apache.hadoop.util.PerformanceAdvisory]Falling back to shell based
[DEBUG][22-06-12][org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[DEBUG][22-06-12][org.apache.hadoop.security.Groups]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[DEBUG][22-06-12][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
[DEBUG][22-06-12][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
[DEBUG][22-06-12][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
[DEBUG][22-06-12][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
[DEBUG][22-06-12][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
[DEBUG][22-06-12][org.apache.hadoop.metrics2.impl.MetricsSystemImpl]UgiMetrics, User and group related metrics
[DEBUG][22-06-12][org.apache.hadoop.security.SecurityUtil]Setting hadoop.security.token.service.use_ip to true
[DEBUG][22-06-12][org.apache.hadoop.security.UserGroupInformation]hadoop login
[DEBUG][22-06-12][org.apache.hadoop.security.UserGroupInformation]hadoop login commit
[DEBUG][22-06-12][org.apache.hadoop.security.UserGroupInformation]using local user:UnixPrincipal: fabivs
[DEBUG][22-06-12][org.apache.hadoop.security.UserGroupInformation]Using user: "UnixPrincipal: fabivs" with name fabivs
[DEBUG][22-06-12][org.apache.hadoop.security.UserGroupInformation]User entry: "fabivs"
[DEBUG][22-06-12][org.apache.hadoop.security.UserGroupInformation]UGI loginUser:fabivs (auth:SIMPLE)
[DEBUG][22-06-12][org.apache.hadoop.security.UserGroupInformation]PrivilegedAction as:fabivs (auth:SIMPLE) from:org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:347)
[DEBUG][22-06-12][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]Connect 0x334f4b92 to master1:2181,master2:2181,utility1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:host.name=localhost
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_291
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/tools.jar:/Users/fabivs/myfile/code/bdp-flink/metricflow-flink/target/classes:/Users/fabivs/.m2/repository/org/apache/flink/flink-streaming-java_2.11/1.13.6/flink-streaming-java_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-core/1.13.6/flink-core-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-annotations/1.13.6/flink-annotations-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-metrics-core/1.13.6/flink-metrics-core-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-asm-7/7.1-13.0/flink-shaded-asm-7-7.1-13.0.jar:/Users/fabivs/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar:/Users/fabivs/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/fabivs/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-file-sink-common/1.13.6/flink-file-sink-common-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-runtime_2.11/1.13.6/flink-runtime_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-queryable-state-client-java/1.13.6/flink-queryable-state-client-java-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-hadoop-fs/1.13.6/flink-hadoop-fs-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-zookeeper-3/3.4.14-13.0/flink-shaded-zookeeper-3-3.4.14-13.0.jar:/Users/fabivs/.m2/repository/org/javassist/javassist/3.24.0-GA/javassist-3.24.0-GA.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-actor_2.11/2.5.21/akka-actor_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/com/typesafe/config/1.3.3/config-1.3.3.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-stream_2.11/2.5.21/akka-stream_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/org/reactivestreams/reactive-streams/1.0.2/reactive-streams-1.0.2.jar:/Users/fabivs/.m2/repository/com/typesafe/ssl-config-core_2.11/0.3.7/ssl-config-core_2.11-0.3.7.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.1/scala-parser-combinators_2.11-1.1.1.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-protobuf_2.11/2.5.21/akka-protobuf_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-slf4j_2.11/2.5.21/akka-slf4j_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/org/clapper/grizzled-slf4j_2.11/1.3.2/grizzled-slf4j_2.11-1.3.2.jar:/Users/fabivs/.m2/repository/com/github/scopt/scopt_2.11/3.5.0/scopt_2.11-3.5.0.jar:/Users/fabivs/.m2/repository/com/twitter/chill_2.11/0.7.6/chill_2.11-0.7.6.jar:/Users/fabivs/.m2/repository/com/twitter/chill-java/0.7.6/chill-java-0.7.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-guava/18.0-13.0/flink-shaded-guava-18.0-13.0.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-math3/3.5/commons-math3-3.5.jar:/Users/fabivs/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/fabivs/.m2/repository/org/apache/flink/force-shading/1.13.6/force-shading-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-java/1.13.6/flink-java-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-connector-base/1.13.6/flink-connector-base-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka_2.11/2.4.1/kafka_2.11-2.4.1.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.10.0/jackson-databind-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.10.0/jackson-annotations-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.10.0/jackson-core-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.10.0/jackson-module-scala_2.11-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.10.0/jackson-module-paranamer-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.10.0/jackson-dataformat-csv-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.10.0/jackson-datatype-jdk8-2.10.0.jar:/Users/fabivs/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/Users/fabivs/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.11/2.1.2/scala-collection-compat_2.11-2.1.2.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.11/0.9.0/scala-java8-compat_2.11-0.9.0.jar:/Users/fabivs/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/fabivs/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/fabivs/.m2/repository/com/typesafe/scala-logging/scala-logging_2.11/3.9.2/scala-logging_2.11-3.9.2.jar:/Users/fabivs/.m2/repository/org/apache/zookeeper/zookeeper/3.5.7/zookeeper-3.5.7.jar:/Users/fabivs/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.7/zookeeper-jute-3.5.7.jar:/Users/fabivs/.m2/repository/io/netty/netty-handler/4.1.45.Final/netty-handler-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-common/4.1.45.Final/netty-common-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-buffer/4.1.45.Final/netty-buffer-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport/4.1.45.Final/netty-transport-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-resolver/4.1.45.Final/netty-resolver-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-codec/4.1.45.Final/netty-codec-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport-native-epoll/4.1.45.Final/netty-transport-native-epoll-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.45.Final/netty-transport-native-unix-common-4.1.45.Final.jar:/Users/fabivs/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka-streams/2.4.1/kafka-streams-2.4.1.jar:/Users/fabivs/.m2/repository/org/apache/kafka/connect-json/2.4.1/connect-json-2.4.1.jar:/Users/fabivs/.m2/repository/org/apache/kafka/connect-api/2.4.1/connect-api-2.4.1.jar:/Users/fabivs/.m2/repository/org/rocksdb/rocksdbjni/5.18.3/rocksdbjni-5.18.3.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar:/Users/fabivs/.m2/repository/com/github/luben/zstd-jni/1.4.3-1/zstd-jni-1.4.3-1.jar:/Users/fabivs/.m2/repository/org/lz4/lz4-java/1.6.0/lz4-java-1.6.0.jar:/Users/fabivs/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-clients_2.11/1.13.6/flink-clients_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-optimizer_2.11/1.13.6/flink-optimizer_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-runtime-web_2.11/1.13.6/flink-runtime-web_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-netty/4.1.49.Final-13.0/flink-shaded-netty-4.1.49.Final-13.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-jackson/2.12.1-13.0/flink-shaded-jackson-2.12.1-13.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-connector-kafka_2.11/1.13.6/flink-connector-kafka_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar:/Users/fabivs/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/fabivs/.m2/repository/org/springframework/kafka/spring-kafka/2.7.8/spring-kafka-2.7.8.jar:/Users/fabivs/.m2/repository/org/springframework/spring-context/5.3.11/spring-context-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-aop/5.3.11/spring-aop-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-beans/5.3.11/spring-beans-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-core/5.3.11/spring-core-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-jcl/5.3.11/spring-jcl-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-expression/5.3.11/spring-expression-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-messaging/5.3.11/spring-messaging-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-tx/5.3.11/spring-tx-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/retry/spring-retry/1.3.1/spring-retry-1.3.1.jar:/Users/fabivs/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/Users/fabivs/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/Users/fabivs/.m2/repository/org/apache/logging/log4j/log4j-to-slf4j/2.14.0/log4j-to-slf4j-2.14.0.jar:/Users/fabivs/.m2/repository/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-common/3.2.2/hadoop-common-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-annotations/3.2.2/hadoop-annotations-3.2.2.jar:/Users/fabivs/.m2/repository/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar:/Users/fabivs/.m2/repository/com/google/guava/failureaccess/1.0/failureaccess-1.0.jar:/Users/fabivs/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/fabivs/.m2/repository/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar:/Users/fabivs/.m2/repository/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.jar:/Users/fabivs/.m2/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/Users/fabivs/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.jar:/Users/fabivs/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/Users/fabivs/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/Users/fabivs/.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar:/Users/fabivs/.m2/repository/commons-io/commons-io/2.5/commons-io-2.5.jar:/Users/fabivs/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/Users/fabivs/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/fabivs/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/fabivs/.m2/repository/javax/activation/javax.activation-api/1.2.0/javax.activation-api-1.2.0.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-server/9.4.20.v20190813/jetty-server-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-http/9.4.20.v20190813/jetty-http-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-io/9.4.20.v20190813/jetty-io-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-util/9.4.20.v20190813/jetty-util-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.20.v20190813/jetty-servlet-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-security/9.4.20.v20190813/jetty-security-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.20.v20190813/jetty-webapp-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.20.v20190813/jetty-xml-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/Users/fabivs/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/Users/fabivs/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/fabivs/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.2/jackson-core-asl-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.2/jackson-mapper-asl-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/Users/fabivs/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/fabivs/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar:/Users/fabivs/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/Users/fabivs/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/fabivs/.m2/repository/com/google/re2j/re2j/1.1/re2j-1.1.jar:/Users/fabivs/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/fabivs/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-auth/3.2.2/hadoop-auth-3.2.2.jar:/Users/fabivs/.m2/repository/com/nimbusds/nimbus-jose-jwt/7.9/nimbus-jose-jwt-7.9.jar:/Users/fabivs/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/Users/fabivs/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/Users/fabivs/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/Users/fabivs/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar:/Users/fabivs/.m2/repository/com/jcraft/jsch/0.1.55/jsch-0.1.55.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar:/Users/fabivs/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-compress/1.19/commons-compress-1.19.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/Users/fabivs/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/Users/fabivs/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/Users/fabivs/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-client/3.2.2/hadoop-client-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.2.2/hadoop-hdfs-client-3.2.2.jar:/Users/fabivs/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/Users/fabivs/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.2.2/hadoop-yarn-api-3.2.2.jar:/Users/fabivs/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.2.2/hadoop-yarn-client-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.2.2/hadoop-mapreduce-client-core-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.2.2/hadoop-yarn-common-3.2.2.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.10/jackson-module-jaxb-annotations-2.9.10.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.10/jackson-jaxrs-json-provider-2.9.10.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.10/jackson-jaxrs-base-2.9.10.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.2.2/hadoop-mapreduce-client-jobclient-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.2.2/hadoop-mapreduce-client-common-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-common/2.3.5/hbase-common-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-logging/2.3.5/hbase-logging-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-miscellaneous/3.3.0/hbase-shaded-miscellaneous-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-gson/3.3.0/hbase-shaded-gson-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-netty/3.3.0/hbase-shaded-netty-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/fabivs/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-hbase_2.11/1.10.3/flink-hbase_2.11-1.10.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-server/1.4.3/hbase-server-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-procedure/1.4.3/hbase-procedure-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-common/1.4.3/hbase-common-1.4.3-tests.jar:/Users/fabivs/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-prefix-tree/1.4.3/hbase-prefix-tree-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-metrics-api/1.4.3/hbase-metrics-api-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-metrics/1.4.3/hbase-metrics-1.4.3.jar:/Users/fabivs/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/fabivs/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/Users/fabivs/.m2/repository/io/netty/netty-all/4.1.8.Final/netty-all-4.1.8.Final.jar:/Users/fabivs/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/Users/fabivs/.m2/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/fabivs/.m2/repository/junit/junit/4.12/junit-4.12.jar:/Users/fabivs/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-client/2.3.5/hbase-client-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-protobuf/3.3.0/hbase-shaded-protobuf-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-hadoop-compat/2.3.5/hbase-hadoop-compat-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/2.3.5/hbase-hadoop2-compat-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-protocol-shaded/2.3.5/hbase-protocol-shaded-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-protocol/2.3.5/hbase-protocol-2.3.5.jar:/Users/fabivs/.m2/repository/org/jruby/jcodings/jcodings/1.0.18/jcodings-1.0.18.jar:/Users/fabivs/.m2/repository/org/jruby/joni/joni/2.1.11/joni-2.1.11.jar:/Users/fabivs/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.6/metrics-core-3.2.6.jar:/Users/fabivs/.m2/repository/com/alibaba/fastjson/1.2.75/fastjson-1.2.75.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/fabivs/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.16
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:user.name=fabivs
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/fabivs
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/fabivs/myfile/code/bdp-flink
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.free=93MB
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.max=1820MB
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.total=214MB
[INFO][22-06-12][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=master1:2181,master2:2181,utility1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$567/1241819321@1dae2fd2
[INFO][22-06-12][org.apache.zookeeper.common.X509Util]Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[INFO][22-06-12][org.apache.zookeeper.ClientCnxnSocket]jute.maxbuffer value is 4194304 Bytes
[INFO][22-06-12][org.apache.zookeeper.ClientCnxn]zookeeper.request.timeout value is 0. feature enabled=
[DEBUG][22-06-12][org.apache.zookeeper.SaslServerPrincipal]Canonicalized address to utility1.cluster
[INFO][22-06-12][org.apache.zookeeper.ClientCnxn]Opening socket connection to server utility1/118.190.207.81:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][22-06-12][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /192.168.3.92:54667, server: utility1/118.190.207.81:2181
[DEBUG][22-06-12][org.apache.zookeeper.ClientCnxn]Session establishment request sent on utility1/118.190.207.81:2181
[INFO][22-06-12][org.apache.zookeeper.ClientCnxn]Session establishment complete on server utility1/118.190.207.81:2181, sessionid = 0x30085a06e27000b, negotiated timeout = 40000
[DEBUG][22-06-12][org.apache.zookeeper.ClientCnxn]Reading reply sessionid:0x30085a06e27000b, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,81605000362,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a31363030306effffffd2ffffffac57ffffffa31c1bffffffd450425546a2435623632316435312d376363372d343162362d396438632d336662646463326230303833,s{47244640348,73014444056,1646900261205,1650525792431,20,0,0,0,67,0,47244640348} 
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector]-Dorg.apache.hbase.thirdparty.io.netty.leakDetection.level: simple
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector]-Dorg.apache.hbase.thirdparty.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetectorFactory]Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@30d4fad0
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-12][org.apache.hadoop.hbase.util.ClassSize]Using Unsafe to estimate memory layout
[DEBUG][22-06-12][org.apache.hadoop.hbase.ipc.AbstractRpcClient]Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@7155b81f, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-12][org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_0c23e62ea319711b24530a38c267707c_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamMap_8b66bce9f80f19736cb554745e27f15e_(1/1) with empty state.
[DEBUG][22-06-12][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-12][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[WARN][22-06-12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-12][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655021452793
[DEBUG][22-06-12][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-12][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-12][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-12][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[INFO][22-06-12][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-12][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-3e62d0f9-6337-4f56-a1dc-61481ebd3183
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-758777ac-82ae-43a3-b51d-712a57ffe416
[INFO][22-06-12][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:54665
[INFO][22-06-12][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-7a910c7f-4377-4b69-a893-17b37be0d34b
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming OneInputTransformation{id=2, name='Map', outputType=PojoType<renaissance.bean.MetricBean, fields = [hostName: String, id: String, name: String, timestamp: String, value: String]>, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=3, name='Unnamed', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 3
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '570f707193e0fe32f4d86d067aba243b' for node 'Map-2' {id: 2, parallelism: 1, user function: renaissance.profunc.MetricBeanMap}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'b728d985904d42b0fdd945a9e3253fca' for node 'Sink: Unnamed-3' {id: 3, parallelism: 1, user function: renaissance.sink.MetricSink}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-18][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-18][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]Default Loggers started
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-18][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]Default Loggers started
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-18][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-d14f13d8-0a55-40a9-ab36-1b8762a706b8
[DEBUG][22-06-18][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-18][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:50862 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-18][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-288582a7-5bf8-4374-b5fd-ac58674b8477
[INFO][22-06-18][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-87c3cf17-747b-4345-8c63-672feaf40ab4
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: 35927a05-b360-4185-a7b5-b48982d7939a
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 367 GB (79.78% usable)
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-97b0e51e-dbca-4313-ae31-403579066dfe for spill files.
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-1cd0b795-b2ef-4d53-994b-8a25731589f8 for spill files.
[INFO][22-06-18][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-18][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-fdad5464-d9a1-4cbf-813e-7c0c4621e450
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Upload directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-upload does not exist. 
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Created directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-upload for file uploads.
[DEBUG][22-06-18][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-18][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-18][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@37ed010a under DELETE@/v1/cluster.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@37ed010a under DELETE@/cluster.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@11b377c5 under GET@/v1/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@11b377c5 under GET@/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@7bca6fac under GET@/v1/datasets.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@7bca6fac under GET@/datasets.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@5c60b0a0 under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@5c60b0a0 under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@7a2b1eb4 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@7a2b1eb4 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@702c436b under GET@/v1/jars.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@702c436b under GET@/jars.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@5833f5cd under POST@/v1/jars/upload.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@5833f5cd under POST@/jars/upload.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@10fbbdb under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@10fbbdb under DELETE@/jars/:jarid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@23f3dbf0 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@23f3dbf0 under GET@/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@31d6f3fe under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@31d6f3fe under POST@/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@760cf594 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@760cf594 under POST@/jars/:jarid/run.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@aa149ed under GET@/v1/jobmanager/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@aa149ed under GET@/jobmanager/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@37303f12 under GET@/v1/jobmanager/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@37303f12 under GET@/jobmanager/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@31ff6309 under GET@/v1/jobmanager/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@31ff6309 under GET@/jobmanager/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@204e90f7 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@204e90f7 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@20a05b32 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@20a05b32 under GET@/jobmanager/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@165e389b under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@165e389b under GET@/jobmanager/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@5c73f672 under GET@/v1/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@5c73f672 under GET@/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@8ee0c23 under POST@/v1/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@8ee0c23 under POST@/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@2ab5afc7 under GET@/v1/jobs/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@2ab5afc7 under GET@/jobs/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@4dc8c0ea under GET@/v1/jobs/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@4dc8c0ea under GET@/jobs/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@e4b6f47 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@e4b6f47 under GET@/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@763cf5b9 under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@763cf5b9 under PATCH@/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@71f0b72e under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@71f0b72e under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@7a34f66a under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@7a34f66a under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@2f508f3c under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@2f508f3c under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@3ed03652 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@3ed03652 under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@4aedaf61 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@4aedaf61 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@173797f0 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@173797f0 under GET@/jobs/:jobid/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@3c35c345 under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@3c35c345 under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@3681037 under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@3681037 under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@2459319c under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@2459319c under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@ffaaaf0 under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@ffaaaf0 under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@1dc76fa1 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@1dc76fa1 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@5eed2d86 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@5eed2d86 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@33d53216 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@33d53216 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@69a2b3b6 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@69a2b3b6 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@4f3e7344 under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@4f3e7344 under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@7808f638 under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@7808f638 under POST@/jobs/:jobid/stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@62d73ead under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@62d73ead under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@1e141e42 under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@1e141e42 under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@228cea97 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@228cea97 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@1d0a61c8 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@1d0a61c8 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@46731692 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@46731692 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@782bf610 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@782bf610 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@3db663d0 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@3db663d0 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@73fc518f under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@73fc518f under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@2de50ee4 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@2de50ee4 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@ad9e63e under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@ad9e63e under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@47fbc56 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@47fbc56 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@151ef57f under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@151ef57f under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@10895b16 under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@10895b16 under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@5524b72f under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@5524b72f under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2cc03cd1 under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2cc03cd1 under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@4e17913b under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@4e17913b under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@149c3204 under GET@/v1/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@149c3204 under GET@/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@64f16277 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@64f16277 under POST@/savepoint-disposal.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@497aec8c under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@497aec8c under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@3b9632d1 under GET@/v1/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@3b9632d1 under GET@/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@4e6f2bb5 under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@4e6f2bb5 under GET@/taskmanagers/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@21e20ad5 under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@21e20ad5 under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@3f628ce9 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@3f628ce9 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@35e8316e under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@35e8316e under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@26d96e5 under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@26d96e5 under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@336880df under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@336880df under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1846579f under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@1846579f under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@6cd166b8 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@6cd166b8 under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@2650f79 under GET@/v1/:*.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@2650f79 under GET@/:*.
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2201 (auto-detected)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:50863
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:50863
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:50863.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:50863 was granted leadership with leaderSessionID=d755d8cc-47cd-4fe1-93eb-8e445b2fb7d4
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:50863 , session=d755d8cc-47cd-4fe1-93eb-8e445b2fb7d4
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-18][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-18][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id 1d1b3539-656a-4aea-bf63-aef58f755362. Creating new DispatcherLeaderProcess.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 9a5ca0c5757fe92c693a2dafac5042f6
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=693a2daf-ac50-42f6-9a5c-a0c5757fe92c
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(9a5ca0c5757fe92c693a2dafac5042f6).
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=1d1b3539-656a-4aea-bf63-aef58f755362
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID 35927a05-b360-4185-a7b5-b48982d7939a (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission 8c603fa0e11f98d9f77eb90c2d6cc097 (metricStream).
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job 8c603fa0e11f98d9f77eb90c2d6cc097 (metricStream).
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 0a23a7803ac718aefa75a3bdd12f9516.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor 35927a05-b360-4185-a7b5-b48982d7939a under 0a23a7803ac718aefa75a3bdd12f9516 at the slot manager.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under c68a2ded-e945-4978-94f8-56b52f7fcefe.
[DEBUG][22-06-18][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (8c603fa0e11f98d9f77eb90c2d6cc097).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (8c603fa0e11f98d9f77eb90c2d6cc097).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (8c603fa0e11f98d9f77eb90c2d6cc097).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 0 ms.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Adding 1 vertices from job graph metricStream (8c603fa0e11f98d9f77eb90c2d6cc097).
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Map -> Sink: Unnamed) to 0 predecessors.
[INFO][22-06-18][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 4 ms
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (8c603fa0e11f98d9f77eb90c2d6cc097).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@22f43641
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job 8c603fa0e11f98d9f77eb90c2d6cc097 after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@710ca8c for metricStream (8c603fa0e11f98d9f77eb90c2d6cc097).
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job 8c603fa0e11f98d9f77eb90c2d6cc097 under leader id c68a2ded-e945-4978-94f8-56b52f7fcefe.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership c68a2ded-e945-4978-94f8-56b52f7fcefe.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=c68a2ded-e945-4978-94f8-56b52f7fcefe
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (8c603fa0e11f98d9f77eb90c2d6cc097) under job master id 94f856b52f7fcefec68a2dede9454978.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (8c603fa0e11f98d9f77eb90c2d6cc097) switched from state CREATED to RUNNING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> Sink: Unnamed (1/1) (45cab0f240fe2d8f8fabde7a2f484c4c) switched from CREATED to SCHEDULED.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 8c603fa0e11f98d9f77eb90c2d6cc097.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{3d8c384e5939485c5dc0bd301ed30570}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc})
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(9a5ca0c5757fe92c693a2dafac5042f6)
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job 8c603fa0e11f98d9f77eb90c2d6cc097 to job leader id monitoring.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job 8c603fa0e11f98d9f77eb90c2d6cc097 has a new job leader c68a2ded-e945-4978-94f8-56b52f7fcefe@akka://flink/user/rpc/jobmanager_3.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager 94f856b52f7fcefec68a2dede9454978@akka://flink/user/rpc/jobmanager_3 for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager 94f856b52f7fcefec68a2dede9454978@akka://flink/user/rpc/jobmanager_3 for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: 9a5ca0c5757fe92c693a2dafac5042f6.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job 8c603fa0e11f98d9f77eb90c2d6cc097: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot 35927a05-b360-4185-a7b5-b48982d7939a_0 for job 8c603fa0e11f98d9f77eb90c2d6cc097 with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request a74142005ca0510f151a04915019dc7a for job 8c603fa0e11f98d9f77eb90c2d6cc097 from resource manager with leader id 9a5ca0c5757fe92c693a2dafac5042f6.
[DEBUG][22-06-18][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for a74142005ca0510f151a04915019dc7a.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job 8c603fa0e11f98d9f77eb90c2d6cc097 for job leader monitoring.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job 8c603fa0e11f98d9f77eb90c2d6cc097. Address: akka://flink/user/rpc/jobmanager_3, leader id: 94f856b52f7fcefec68a2dede9454978.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id c68a2ded-e945-4978-94f8-56b52f7fcefe.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job 8c603fa0e11f98d9f77eb90c2d6cc097.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor 35927a05-b360-4185-a7b5-b48982d7939a @ localhost (dataPort=-1).
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer a74142005ca0510f151a04915019dc7a to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot a74142005ca0510f151a04915019dc7a @ 35927a05-b360-4185-a7b5-b48982d7939a @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot a74142005ca0510f151a04915019dc7a for slot request id SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id a74142005ca0510f151a04915019dc7a.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{3d8c384e5939485c5dc0bd301ed30570}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc})
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> Sink: Unnamed (1/1) (45cab0f240fe2d8f8fabde7a2f484c4c) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source -> Map -> Sink: Unnamed (1/1) (attempt #0) with attempt id 45cab0f240fe2d8f8fabde7a2f484c4c to 35927a05-b360-4185-a7b5-b48982d7939a @ localhost (dataPort=-1) with allocation id a74142005ca0510f151a04915019dc7a
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot a74142005ca0510f151a04915019dc7a.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot a74142005ca0510f151a04915019dc7a.
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id a74142005ca0510f151a04915019dc7a for local state stores for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_a74142005ca0510f151a04915019dc7a], jobID=8c603fa0e11f98d9f77eb90c2d6cc097, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for 8c603fa0e11f98d9f77eb90c2d6cc097 - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id a74142005ca0510f151a04915019dc7a.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 (45cab0f240fe2d8f8fabde7a2f484c4c), deploy into slot with allocation id a74142005ca0510f151a04915019dc7a.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 (45cab0f240fe2d8f8fabde7a2f484c4c) switched from CREATED to DEPLOYING.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 (45cab0f240fe2d8f8fabde7a2f484c4c) [DEPLOYING]
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 (45cab0f240fe2d8f8fabde7a2f484c4c) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 45cab0f240fe2d8f8fabde7a2f484c4c at library cache manager took 1 milliseconds
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 (45cab0f240fe2d8f8fabde7a2f484c4c) [DEPLOYING].
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7ba4def8
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 (45cab0f240fe2d8f8fabde7a2f484c4c) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> Sink: Unnamed (1/1) (45cab0f240fe2d8f8fabde7a2f484c4c) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_b728d985904d42b0fdd945a9e3253fca_(1/1) with empty state.
[DEBUG][22-06-18][org.apache.hadoop.util.Shell]setsid is not available on this machine. So not using it.
[DEBUG][22-06-18][org.apache.hadoop.util.Shell]setsid exited with exit code 0
[DEBUG][22-06-18][org.apache.hadoop.security.Groups] Creating new Groups object
[DEBUG][22-06-18][org.apache.hadoop.util.NativeCodeLoader]Trying to load the custom-built native-hadoop library...
[DEBUG][22-06-18][org.apache.hadoop.util.NativeCodeLoader]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[DEBUG][22-06-18][org.apache.hadoop.util.NativeCodeLoader]java.library.path=/Users/fabivs/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[WARN][22-06-18][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[DEBUG][22-06-18][org.apache.hadoop.util.PerformanceAdvisory]Falling back to shell based
[DEBUG][22-06-18][org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[DEBUG][22-06-18][org.apache.hadoop.security.Groups]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.impl.MetricsSystemImpl]UgiMetrics, User and group related metrics
[DEBUG][22-06-18][org.apache.hadoop.security.SecurityUtil]Setting hadoop.security.token.service.use_ip to true
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]hadoop login
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]hadoop login commit
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]using local user:UnixPrincipal: fabivs
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]Using user: "UnixPrincipal: fabivs" with name fabivs
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]User entry: "fabivs"
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]UGI loginUser:fabivs (auth:SIMPLE)
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]PrivilegedAction as:fabivs (auth:SIMPLE) from:org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:347)
[DEBUG][22-06-18][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]Connect 0x5f2b0b0b to master1:2181,master2:2181,utility1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:host.name=localhost
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_291
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/tools.jar:/Users/fabivs/myfile/code/bdp-flink/metricflow-flink/target/classes:/Users/fabivs/.m2/repository/org/apache/flink/flink-streaming-java_2.11/1.13.6/flink-streaming-java_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-core/1.13.6/flink-core-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-annotations/1.13.6/flink-annotations-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-metrics-core/1.13.6/flink-metrics-core-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-asm-7/7.1-13.0/flink-shaded-asm-7-7.1-13.0.jar:/Users/fabivs/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar:/Users/fabivs/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/fabivs/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-file-sink-common/1.13.6/flink-file-sink-common-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-runtime_2.11/1.13.6/flink-runtime_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-queryable-state-client-java/1.13.6/flink-queryable-state-client-java-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-hadoop-fs/1.13.6/flink-hadoop-fs-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-zookeeper-3/3.4.14-13.0/flink-shaded-zookeeper-3-3.4.14-13.0.jar:/Users/fabivs/.m2/repository/org/javassist/javassist/3.24.0-GA/javassist-3.24.0-GA.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-actor_2.11/2.5.21/akka-actor_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/com/typesafe/config/1.3.3/config-1.3.3.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-stream_2.11/2.5.21/akka-stream_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/org/reactivestreams/reactive-streams/1.0.2/reactive-streams-1.0.2.jar:/Users/fabivs/.m2/repository/com/typesafe/ssl-config-core_2.11/0.3.7/ssl-config-core_2.11-0.3.7.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.1/scala-parser-combinators_2.11-1.1.1.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-protobuf_2.11/2.5.21/akka-protobuf_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-slf4j_2.11/2.5.21/akka-slf4j_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/org/clapper/grizzled-slf4j_2.11/1.3.2/grizzled-slf4j_2.11-1.3.2.jar:/Users/fabivs/.m2/repository/com/github/scopt/scopt_2.11/3.5.0/scopt_2.11-3.5.0.jar:/Users/fabivs/.m2/repository/com/twitter/chill_2.11/0.7.6/chill_2.11-0.7.6.jar:/Users/fabivs/.m2/repository/com/twitter/chill-java/0.7.6/chill-java-0.7.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-guava/18.0-13.0/flink-shaded-guava-18.0-13.0.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-math3/3.5/commons-math3-3.5.jar:/Users/fabivs/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/fabivs/.m2/repository/org/apache/flink/force-shading/1.13.6/force-shading-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-java/1.13.6/flink-java-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-connector-base/1.13.6/flink-connector-base-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka_2.11/2.4.1/kafka_2.11-2.4.1.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.10.0/jackson-databind-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.10.0/jackson-annotations-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.10.0/jackson-core-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.10.0/jackson-module-scala_2.11-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.10.0/jackson-module-paranamer-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.10.0/jackson-dataformat-csv-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.10.0/jackson-datatype-jdk8-2.10.0.jar:/Users/fabivs/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/Users/fabivs/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.11/2.1.2/scala-collection-compat_2.11-2.1.2.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.11/0.9.0/scala-java8-compat_2.11-0.9.0.jar:/Users/fabivs/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/fabivs/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/fabivs/.m2/repository/com/typesafe/scala-logging/scala-logging_2.11/3.9.2/scala-logging_2.11-3.9.2.jar:/Users/fabivs/.m2/repository/org/apache/zookeeper/zookeeper/3.5.7/zookeeper-3.5.7.jar:/Users/fabivs/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.7/zookeeper-jute-3.5.7.jar:/Users/fabivs/.m2/repository/io/netty/netty-handler/4.1.45.Final/netty-handler-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-common/4.1.45.Final/netty-common-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-buffer/4.1.45.Final/netty-buffer-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport/4.1.45.Final/netty-transport-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-resolver/4.1.45.Final/netty-resolver-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-codec/4.1.45.Final/netty-codec-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport-native-epoll/4.1.45.Final/netty-transport-native-epoll-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.45.Final/netty-transport-native-unix-common-4.1.45.Final.jar:/Users/fabivs/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka-streams/2.4.1/kafka-streams-2.4.1.jar:/Users/fabivs/.m2/repository/org/apache/kafka/connect-json/2.4.1/connect-json-2.4.1.jar:/Users/fabivs/.m2/repository/org/apache/kafka/connect-api/2.4.1/connect-api-2.4.1.jar:/Users/fabivs/.m2/repository/org/rocksdb/rocksdbjni/5.18.3/rocksdbjni-5.18.3.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar:/Users/fabivs/.m2/repository/com/github/luben/zstd-jni/1.4.3-1/zstd-jni-1.4.3-1.jar:/Users/fabivs/.m2/repository/org/lz4/lz4-java/1.6.0/lz4-java-1.6.0.jar:/Users/fabivs/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-clients_2.11/1.13.6/flink-clients_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-optimizer_2.11/1.13.6/flink-optimizer_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-runtime-web_2.11/1.13.6/flink-runtime-web_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-netty/4.1.49.Final-13.0/flink-shaded-netty-4.1.49.Final-13.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-jackson/2.12.1-13.0/flink-shaded-jackson-2.12.1-13.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-connector-kafka_2.11/1.13.6/flink-connector-kafka_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar:/Users/fabivs/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/fabivs/.m2/repository/org/springframework/kafka/spring-kafka/2.7.8/spring-kafka-2.7.8.jar:/Users/fabivs/.m2/repository/org/springframework/spring-context/5.3.11/spring-context-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-aop/5.3.11/spring-aop-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-beans/5.3.11/spring-beans-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-core/5.3.11/spring-core-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-jcl/5.3.11/spring-jcl-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-expression/5.3.11/spring-expression-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-messaging/5.3.11/spring-messaging-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-tx/5.3.11/spring-tx-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/retry/spring-retry/1.3.1/spring-retry-1.3.1.jar:/Users/fabivs/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/Users/fabivs/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/Users/fabivs/.m2/repository/org/apache/logging/log4j/log4j-to-slf4j/2.14.0/log4j-to-slf4j-2.14.0.jar:/Users/fabivs/.m2/repository/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-common/3.2.2/hadoop-common-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-annotations/3.2.2/hadoop-annotations-3.2.2.jar:/Users/fabivs/.m2/repository/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar:/Users/fabivs/.m2/repository/com/google/guava/failureaccess/1.0/failureaccess-1.0.jar:/Users/fabivs/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/fabivs/.m2/repository/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar:/Users/fabivs/.m2/repository/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.jar:/Users/fabivs/.m2/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/Users/fabivs/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.jar:/Users/fabivs/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/Users/fabivs/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/Users/fabivs/.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar:/Users/fabivs/.m2/repository/commons-io/commons-io/2.5/commons-io-2.5.jar:/Users/fabivs/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/Users/fabivs/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/fabivs/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/fabivs/.m2/repository/javax/activation/javax.activation-api/1.2.0/javax.activation-api-1.2.0.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-server/9.4.20.v20190813/jetty-server-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-http/9.4.20.v20190813/jetty-http-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-io/9.4.20.v20190813/jetty-io-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-util/9.4.20.v20190813/jetty-util-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.20.v20190813/jetty-servlet-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-security/9.4.20.v20190813/jetty-security-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.20.v20190813/jetty-webapp-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.20.v20190813/jetty-xml-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/Users/fabivs/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/Users/fabivs/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/fabivs/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.2/jackson-core-asl-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.2/jackson-mapper-asl-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/Users/fabivs/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/fabivs/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar:/Users/fabivs/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/Users/fabivs/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/fabivs/.m2/repository/com/google/re2j/re2j/1.1/re2j-1.1.jar:/Users/fabivs/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/fabivs/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-auth/3.2.2/hadoop-auth-3.2.2.jar:/Users/fabivs/.m2/repository/com/nimbusds/nimbus-jose-jwt/7.9/nimbus-jose-jwt-7.9.jar:/Users/fabivs/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/Users/fabivs/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/Users/fabivs/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/Users/fabivs/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar:/Users/fabivs/.m2/repository/com/jcraft/jsch/0.1.55/jsch-0.1.55.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar:/Users/fabivs/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-compress/1.19/commons-compress-1.19.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/Users/fabivs/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/Users/fabivs/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/Users/fabivs/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-client/3.2.2/hadoop-client-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.2.2/hadoop-hdfs-client-3.2.2.jar:/Users/fabivs/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/Users/fabivs/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.2.2/hadoop-yarn-api-3.2.2.jar:/Users/fabivs/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.2.2/hadoop-yarn-client-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.2.2/hadoop-mapreduce-client-core-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.2.2/hadoop-yarn-common-3.2.2.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.10/jackson-module-jaxb-annotations-2.9.10.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.10/jackson-jaxrs-json-provider-2.9.10.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.10/jackson-jaxrs-base-2.9.10.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.2.2/hadoop-mapreduce-client-jobclient-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.2.2/hadoop-mapreduce-client-common-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-common/2.3.5/hbase-common-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-logging/2.3.5/hbase-logging-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-miscellaneous/3.3.0/hbase-shaded-miscellaneous-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-gson/3.3.0/hbase-shaded-gson-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-netty/3.3.0/hbase-shaded-netty-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/fabivs/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-hbase_2.11/1.10.3/flink-hbase_2.11-1.10.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-server/1.4.3/hbase-server-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-procedure/1.4.3/hbase-procedure-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-common/1.4.3/hbase-common-1.4.3-tests.jar:/Users/fabivs/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-prefix-tree/1.4.3/hbase-prefix-tree-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-metrics-api/1.4.3/hbase-metrics-api-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-metrics/1.4.3/hbase-metrics-1.4.3.jar:/Users/fabivs/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/fabivs/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/Users/fabivs/.m2/repository/io/netty/netty-all/4.1.8.Final/netty-all-4.1.8.Final.jar:/Users/fabivs/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/Users/fabivs/.m2/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/fabivs/.m2/repository/junit/junit/4.12/junit-4.12.jar:/Users/fabivs/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-client/2.3.5/hbase-client-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-protobuf/3.3.0/hbase-shaded-protobuf-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-hadoop-compat/2.3.5/hbase-hadoop-compat-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/2.3.5/hbase-hadoop2-compat-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-protocol-shaded/2.3.5/hbase-protocol-shaded-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-protocol/2.3.5/hbase-protocol-2.3.5.jar:/Users/fabivs/.m2/repository/org/jruby/jcodings/jcodings/1.0.18/jcodings-1.0.18.jar:/Users/fabivs/.m2/repository/org/jruby/joni/joni/2.1.11/joni-2.1.11.jar:/Users/fabivs/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.6/metrics-core-3.2.6.jar:/Users/fabivs/.m2/repository/com/alibaba/fastjson/1.2.75/fastjson-1.2.75.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/fabivs/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.16
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:user.name=fabivs
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/fabivs
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/fabivs/myfile/code/bdp-flink
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.free=94MB
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.max=1820MB
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.total=215MB
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=master1:2181,master2:2181,utility1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$569/7440890@f3d94ac
[INFO][22-06-18][org.apache.zookeeper.common.X509Util]Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[INFO][22-06-18][org.apache.zookeeper.ClientCnxnSocket]jute.maxbuffer value is 4194304 Bytes
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]zookeeper.request.timeout value is 0. feature enabled=
[DEBUG][22-06-18][org.apache.zookeeper.SaslServerPrincipal]Canonicalized address to master2.cluster
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master2/118.190.145.3:2181. Will not attempt to authenticate using SASL (unknown error)
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0a23a7803ac718aefa75a3bdd12f9516: SlotReport{SlotStatus{slotID=35927a05-b360-4185-a7b5-b48982d7939a_0, allocationID=a74142005ca0510f151a04915019dc7a, jobID=8c603fa0e11f98d9f77eb90c2d6cc097, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 35927a05-b360-4185-a7b5-b48982d7939a: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0a23a7803ac718aefa75a3bdd12f9516: SlotReport{SlotStatus{slotID=35927a05-b360-4185-a7b5-b48982d7939a_0, allocationID=a74142005ca0510f151a04915019dc7a, jobID=8c603fa0e11f98d9f77eb90c2d6cc097, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 35927a05-b360-4185-a7b5-b48982d7939a: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0a23a7803ac718aefa75a3bdd12f9516: SlotReport{SlotStatus{slotID=35927a05-b360-4185-a7b5-b48982d7939a_0, allocationID=a74142005ca0510f151a04915019dc7a, jobID=8c603fa0e11f98d9f77eb90c2d6cc097, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 35927a05-b360-4185-a7b5-b48982d7939a: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[WARN][22-06-18][org.apache.zookeeper.ClientCnxn]Client session timed out, have not heard from server in 30002ms for sessionid 0x0
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Client session timed out, have not heard from server in 30002ms for sessionid 0x0, closing socket connection and attempting reconnect
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxnSocketNIO]Ignoring exception during shutdown input
java.net.SocketException: Socket is not connected
	at sun.nio.ch.Net.translateToSocketException(Net.java:122)
	at sun.nio.ch.Net.translateException(Net.java:156)
	at sun.nio.ch.Net.translateException(Net.java:162)
	at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:401)
	at org.apache.zookeeper.ClientCnxnSocketNIO.cleanup(ClientCnxnSocketNIO.java:198)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1338)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanAndNotifyState(ClientCnxn.java:1276)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1254)
Caused by: java.nio.channels.NotYetConnectedException
	at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:780)
	at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:399)
	... 4 more
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxnSocketNIO]Ignoring exception during shutdown output
java.net.SocketException: Socket is not connected
	at sun.nio.ch.Net.translateToSocketException(Net.java:122)
	at sun.nio.ch.Net.translateException(Net.java:156)
	at sun.nio.ch.Net.translateException(Net.java:162)
	at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:409)
	at org.apache.zookeeper.ClientCnxnSocketNIO.cleanup(ClientCnxnSocketNIO.java:205)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1338)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanAndNotifyState(ClientCnxn.java:1276)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1254)
Caused by: java.nio.channels.NotYetConnectedException
	at sun.nio.ch.SocketChannelImpl.shutdownOutput(SocketChannelImpl.java:797)
	at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:407)
	... 4 more
[WARN][22-06-18][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]0x5f2b0b0b to master1:2181,master2:2181,utility1:2181 failed for get of /hbase/hbaseid, code = CONNECTIONLOSS, retries = 1
[DEBUG][22-06-18][org.apache.zookeeper.SaslServerPrincipal]Canonicalized address to utility1.cluster
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Opening socket connection to server utility1/118.190.207.81:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /192.168.3.92:50867, server: utility1/118.190.207.81:2181
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Session establishment request sent on utility1/118.190.207.81:2181
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Session establishment complete on server utility1/118.190.207.81:2181, sessionid = 0x30085a06e270017, negotiated timeout = 40000
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Reading reply sessionid:0x30085a06e270017, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,81605000463,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a31363030306effffffd2ffffffac57ffffffa31c1bffffffd450425546a2435623632316435312d376363372d343162362d396438632d336662646463326230303833,s{47244640348,73014444056,1646900261205,1650525792431,20,0,0,0,67,0,47244640348} 
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector]-Dorg.apache.hbase.thirdparty.io.netty.leakDetection.level: simple
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector]-Dorg.apache.hbase.thirdparty.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetectorFactory]Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@3eab602f
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-18][org.apache.hadoop.hbase.util.ClassSize]Using Unsafe to estimate memory layout
[DEBUG][22-06-18][org.apache.hadoop.hbase.ipc.AbstractRpcClient]Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@757fb548, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamMap_570f707193e0fe32f4d86d067aba243b_(1/1) with empty state.
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) with empty state.
[INFO][22-06-18][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[WARN][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655524553802
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-18][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0a23a7803ac718aefa75a3bdd12f9516: SlotReport{SlotStatus{slotID=35927a05-b360-4185-a7b5-b48982d7939a_0, allocationID=a74142005ca0510f151a04915019dc7a, jobID=8c603fa0e11f98d9f77eb90c2d6cc097, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 35927a05-b360-4185-a7b5-b48982d7939a: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Got ping response for sessionid: 0x30085a06e270017 after 33ms
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0a23a7803ac718aefa75a3bdd12f9516: SlotReport{SlotStatus{slotID=35927a05-b360-4185-a7b5-b48982d7939a_0, allocationID=a74142005ca0510f151a04915019dc7a, jobID=8c603fa0e11f98d9f77eb90c2d6cc097, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 35927a05-b360-4185-a7b5-b48982d7939a: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0a23a7803ac718aefa75a3bdd12f9516: SlotReport{SlotStatus{slotID=35927a05-b360-4185-a7b5-b48982d7939a_0, allocationID=a74142005ca0510f151a04915019dc7a, jobID=8c603fa0e11f98d9f77eb90c2d6cc097, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 35927a05-b360-4185-a7b5-b48982d7939a: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Got ping response for sessionid: 0x30085a06e270017 after 25ms
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0a23a7803ac718aefa75a3bdd12f9516: SlotReport{SlotStatus{slotID=35927a05-b360-4185-a7b5-b48982d7939a_0, allocationID=a74142005ca0510f151a04915019dc7a, jobID=8c603fa0e11f98d9f77eb90c2d6cc097, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 35927a05-b360-4185-a7b5-b48982d7939a: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Got ping response for sessionid: 0x30085a06e270017 after 35ms
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0a23a7803ac718aefa75a3bdd12f9516: SlotReport{SlotStatus{slotID=35927a05-b360-4185-a7b5-b48982d7939a_0, allocationID=a74142005ca0510f151a04915019dc7a, jobID=8c603fa0e11f98d9f77eb90c2d6cc097, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 35927a05-b360-4185-a7b5-b48982d7939a: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Got ping response for sessionid: 0x30085a06e270017 after 28ms
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 3bf48d01f36944efc960d463c4af7b0f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0a23a7803ac718aefa75a3bdd12f9516: SlotReport{SlotStatus{slotID=35927a05-b360-4185-a7b5-b48982d7939a_0, allocationID=a74142005ca0510f151a04915019dc7a, jobID=8c603fa0e11f98d9f77eb90c2d6cc097, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 35927a05-b360-4185-a7b5-b48982d7939a: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 640b08b02a75b7eba852d55ec9792a75.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.zookeeper.ZooKeeper]Closing session: 0x30085a06e270017
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Closing client for session: 0x30085a06e270017
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Reading reply sessionid:0x30085a06e270017, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,81605000464,0  request:: null response:: null
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Disconnecting client for session: 0x30085a06e270017
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]An exception was thrown while closing send thread for session 0x30085a06e270017 : Unable to read additional data from server sessionid 0x30085a06e270017, likely server has closed socket
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Session: 0x30085a06e270017 closed
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]EventThread shut down for session: 0x30085a06e270017
[DEBUG][22-06-18][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]Close zookeeper connection 0x5f2b0b0b to master1:2181,master2:2181,utility1:2181
[DEBUG][22-06-18][org.apache.hadoop.hbase.ipc.AbstractRpcClient]Stopping rpc client
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 (45cab0f240fe2d8f8fabde7a2f484c4c) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 (45cab0f240fe2d8f8fabde7a2f484c4c).
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 (45cab0f240fe2d8f8fabde7a2f484c4c) [FAILED]
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Map -> Sink: Unnamed (1/1)#0 45cab0f240fe2d8f8fabde7a2f484c4c.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source -> Map -> Sink: Unnamed (1/1) (45cab0f240fe2d8f8fabde7a2f484c4c) switched from INITIALIZING to FAILED on 35927a05-b360-4185-a7b5-b48982d7939a @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{3d8c384e5939485c5dc0bd301ed30570}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc})
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot a74142005ca0510f151a04915019dc7a.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{b86d34e9bfa443cb5cad8f86368252dc})
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot a74142005ca0510f151a04915019dc7a @ 35927a05-b360-4185-a7b5-b48982d7939a @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 8c603fa0e11f98d9f77eb90c2d6cc097.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job 8c603fa0e11f98d9f77eb90c2d6cc097
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job 8c603fa0e11f98d9f77eb90c2d6cc097: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]1 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0. 
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (8c603fa0e11f98d9f77eb90c2d6cc097) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (8c603fa0e11f98d9f77eb90c2d6cc097) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph 8c603fa0e11f98d9f77eb90c2d6cc097 reached terminal state FAILED.
[INFO][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[INFO][22-06-18][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job 8c603fa0e11f98d9f77eb90c2d6cc097 under leader id c68a2ded-e945-4978-94f8-56b52f7fcefe reached a globally terminal state FAILED.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job 8c603fa0e11f98d9f77eb90c2d6cc097 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job 8c603fa0e11f98d9f77eb90c2d6cc097 under leader id c68a2ded-e945-4978-94f8-56b52f7fcefe.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection 3bf48d01f36944efc960d463c4af7b0f.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(8c603fa0e11f98d9f77eb90c2d6cc097).
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection 35927a05-b360-4185-a7b5-b48982d7939a because: The TaskExecutor is shutting down.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor 0a23a7803ac718aefa75a3bdd12f9516 from the slot manager.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job 8c603fa0e11f98d9f77eb90c2d6cc097.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID cbc357ccb763df2852fee8c4fc7d55f2_0
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor 35927a05-b360-4185-a7b5-b48982d7939a.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor 35927a05-b360-4185-a7b5-b48982d7939a because: Stopping JobMaster for job metricStream(8c603fa0e11f98d9f77eb90c2d6cc097).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [a74142005ca0510f151a04915019dc7a].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(8c603fa0e11f98d9f77eb90c2d6cc097).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection 3bf48d01f36944efc960d463c4af7b0f.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(8c603fa0e11f98d9f77eb90c2d6cc097).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: a74142005ca0510f151a04915019dc7a, jobId: 8c603fa0e11f98d9f77eb90c2d6cc097).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job 8c603fa0e11f98d9f77eb90c2d6cc097 from job leader id monitoring.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager 94f856b52f7fcefec68a2dede9454978@akka://flink/user/rpc/jobmanager_3 for job 8c603fa0e11f98d9f77eb90c2d6cc097 from the resource manager.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 8c603fa0e11f98d9f77eb90c2d6cc097.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id a74142005ca0510f151a04915019dc7a because: Stopping JobMaster for job metricStream(8c603fa0e11f98d9f77eb90c2d6cc097).
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id a74142005ca0510f151a04915019dc7a.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for a74142005ca0510f151a04915019dc7a.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id a74142005ca0510f151a04915019dc7a.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job 8c603fa0e11f98d9f77eb90c2d6cc097 has been terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job 8c603fa0e11f98d9f77eb90c2d6cc097 under leader id c68a2ded-e945-4978-94f8-56b52f7fcefe has been terminated.
[INFO][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-18][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-97b0e51e-dbca-4313-ae31-403579066dfe
[INFO][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-1cd0b795-b2ef-4d53-994b-8a25731589f8
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-18][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-fdad5464-d9a1-4cbf-813e-7c0c4621e450
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-18][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-18][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-18][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-18][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-18][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:50862
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming OneInputTransformation{id=4, name='Map', outputType=PojoType<renaissance.bean.MetricBean, fields = [hostName: String, id: String, name: String, timestamp: String, value: String]>, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming UnionTransformation{id=3, name='Union', outputType=String, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=2, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 4
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=5, name='Unnamed', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 5
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'bc764cd8ddf7a0cff126f51c16239658' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'feca28aff5a3958840bee985ee7de4d3' for node 'Source: Custom Source-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '4bf7c1955ffe56e2106d666433eaf137' for node 'Map-4' {id: 4, parallelism: 1, user function: renaissance.profunc.MetricBeanMap}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'ccb29b5204e83e8a588b3828afaa7015' for node 'Sink: Unnamed-5' {id: 5, parallelism: 1, user function: renaissance.sink.MetricSink}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 4
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]CONNECTED: ForwardPartitioner - 1 -> 4
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 2
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]CONNECTED: ForwardPartitioner - 2 -> 4
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-18][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-18][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]Default Loggers started
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-18][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]Default Loggers started
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-18][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-5be0111f-0349-4569-8f92-28f7eceeb543
[DEBUG][22-06-18][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-18][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:51770 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-18][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-191bb994-571a-4967-b30c-791d925cd98f
[INFO][22-06-18][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-877fe85f-56b0-4bb4-b72e-8f9f9c89e364
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: 942f515d-aa56-4aaf-86bc-2639a3f0e668
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 367 GB (79.78% usable)
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-de511f4b-c251-4744-bc19-332024b7cb4e for spill files.
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-f839713b-c4a2-4789-a72c-d8566cd31ea2 for spill files.
[INFO][22-06-18][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-18][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-152382d6-cb29-464a-a445-3eeab4d424bc
[DEBUG][22-06-18][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-18][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-18][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@351f2244 under DELETE@/v1/cluster.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@351f2244 under DELETE@/cluster.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@73fc518f under GET@/v1/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@73fc518f under GET@/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@2de50ee4 under GET@/v1/datasets.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@2de50ee4 under GET@/datasets.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@ad9e63e under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@ad9e63e under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@47fbc56 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@47fbc56 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@151ef57f under GET@/v1/jars.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@151ef57f under GET@/jars.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@10895b16 under POST@/v1/jars/upload.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@10895b16 under POST@/jars/upload.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@5524b72f under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@5524b72f under DELETE@/jars/:jarid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@2cc03cd1 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@2cc03cd1 under GET@/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@4e17913b under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@4e17913b under POST@/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@149c3204 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@149c3204 under POST@/jars/:jarid/run.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@64f16277 under GET@/v1/jobmanager/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@64f16277 under GET@/jobmanager/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@497aec8c under GET@/v1/jobmanager/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@497aec8c under GET@/jobmanager/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@3b9632d1 under GET@/v1/jobmanager/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@3b9632d1 under GET@/jobmanager/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@4e6f2bb5 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@4e6f2bb5 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@21e20ad5 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@21e20ad5 under GET@/jobmanager/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@3f628ce9 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@3f628ce9 under GET@/jobmanager/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@35e8316e under GET@/v1/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@35e8316e under GET@/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@26d96e5 under POST@/v1/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@26d96e5 under POST@/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@336880df under GET@/v1/jobs/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@336880df under GET@/jobs/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@1846579f under GET@/v1/jobs/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@1846579f under GET@/jobs/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@6cd166b8 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@6cd166b8 under GET@/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2650f79 under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2650f79 under PATCH@/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@75fc1992 under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@75fc1992 under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@5fac521d under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@5fac521d under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@38af1bf6 under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@38af1bf6 under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@129bd55d under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@129bd55d under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@7be7e15 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@7be7e15 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@3abfe845 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@3abfe845 under GET@/jobs/:jobid/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@7a0f244f under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@7a0f244f under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@3672276e under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@3672276e under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@4248b963 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@4248b963 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@7f08caf under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@7f08caf under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@4defd42 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@4defd42 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@2330e3e0 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@2330e3e0 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@24b4d544 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@24b4d544 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@27a2a089 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@27a2a089 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@54657dd2 under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@54657dd2 under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@706eab5d under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@706eab5d under POST@/jobs/:jobid/stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@72725ee1 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@72725ee1 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@40e60ece under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@40e60ece under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@3f9270ed under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@3f9270ed under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@3a230001 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@3a230001 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@5ac6c4f2 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@5ac6c4f2 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@2aa6311a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@2aa6311a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@61f39bb under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@61f39bb under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@249e0271 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@249e0271 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@4893b344 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@4893b344 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@53a665ad under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@53a665ad under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@2c0b4c83 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@2c0b4c83 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@78525ef9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@78525ef9 under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@2d0ecb24 under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@2d0ecb24 under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@4d654825 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@4d654825 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@3bfc6a5e under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@3bfc6a5e under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51b35e4e under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51b35e4e under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@abff8b7 under GET@/v1/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@abff8b7 under GET@/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@6d7cada5 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@6d7cada5 under POST@/savepoint-disposal.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@350a94ce under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@350a94ce under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@7e00ed0f under GET@/v1/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@7e00ed0f under GET@/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@b0fc838 under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@b0fc838 under GET@/taskmanagers/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@3964d79 under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@3964d79 under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@62db0521 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@62db0521 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@1b4ae4e0 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@1b4ae4e0 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@6ef1a1b9 under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@6ef1a1b9 under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@5fbdc49b under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@5fbdc49b under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@65753040 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@65753040 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@2954b5ea under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@2954b5ea under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@4acb2510 under GET@/v1/:*.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@4acb2510 under GET@/:*.
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 2909 (auto-detected)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:51771
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:51771
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:51771.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:51771 was granted leadership with leaderSessionID=13dd9b2f-3dd8-41fa-a950-f7fd2ef0eb17
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:51771 , session=13dd9b2f-3dd8-41fa-a950-f7fd2ef0eb17
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-18][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-18][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id e9a1ff6a-96be-492c-a6be-a6c5ce177937. Creating new DispatcherLeaderProcess.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token b0308fb161fad467c8adad722779434e
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=c8adad72-2779-434e-b030-8fb161fad467
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b0308fb161fad467c8adad722779434e).
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=e9a1ff6a-96be-492c-a6be-a6c5ce177937
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID 942f515d-aa56-4aaf-86bc-2639a3f0e668 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission 0500955fa50ddbac2a57592e8dbe688f (metricStream).
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job 0500955fa50ddbac2a57592e8dbe688f (metricStream).
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 310e2c6d8836963b6bbd065c196008a0.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor 942f515d-aa56-4aaf-86bc-2639a3f0e668 under 310e2c6d8836963b6bbd065c196008a0 at the slot manager.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job 0500955fa50ddbac2a57592e8dbe688f.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under 5ec62ac0-40ed-4d5b-b7e7-dd59760d4198.
[DEBUG][22-06-18][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (0500955fa50ddbac2a57592e8dbe688f).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (0500955fa50ddbac2a57592e8dbe688f).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (0500955fa50ddbac2a57592e8dbe688f).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 0 ms.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Adding 3 vertices from job graph metricStream (0500955fa50ddbac2a57592e8dbe688f).
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 3 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex bc764cd8ddf7a0cff126f51c16239658 (Source: Custom Source) to 0 predecessors.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex feca28aff5a3958840bee985ee7de4d3 (Source: Custom Source) to 0 predecessors.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex 4bf7c1955ffe56e2106d666433eaf137 (Map -> Sink: Unnamed) to 2 predecessors.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting input 0 of vertex 4bf7c1955ffe56e2106d666433eaf137 (Map -> Sink: Unnamed) to intermediate result referenced via predecessor bc764cd8ddf7a0cff126f51c16239658 (Source: Custom Source).
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting input 1 of vertex 4bf7c1955ffe56e2106d666433eaf137 (Map -> Sink: Unnamed) to intermediate result referenced via predecessor feca28aff5a3958840bee985ee7de4d3 (Source: Custom Source).
[INFO][22-06-18][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 7 ms
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (0500955fa50ddbac2a57592e8dbe688f).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@274244b6
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job 0500955fa50ddbac2a57592e8dbe688f after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@6270b341 for metricStream (0500955fa50ddbac2a57592e8dbe688f).
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job 0500955fa50ddbac2a57592e8dbe688f under leader id 5ec62ac0-40ed-4d5b-b7e7-dd59760d4198.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership 5ec62ac0-40ed-4d5b-b7e7-dd59760d4198.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=5ec62ac0-40ed-4d5b-b7e7-dd59760d4198
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (0500955fa50ddbac2a57592e8dbe688f) under job master id b7e7dd59760d41985ec62ac040ed4d5b.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (0500955fa50ddbac2a57592e8dbe688f) switched from state CREATED to RUNNING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (e4ab122db48ab6a4b51fd7a6dfa282bd) switched from CREATED to SCHEDULED.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (4ec3d330a1c1884412e2a640c7e8f985) switched from CREATED to SCHEDULED.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (d8769874f50b21d0e1971ee07ae725af) switched from CREATED to SCHEDULED.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 0500955fa50ddbac2a57592e8dbe688f.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{2ab7c46231ed632da50337739e3269ad}) for execution vertex (id bc764cd8ddf7a0cff126f51c16239658_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{9014d4e9b6ae202382643a33324aae52}) for execution vertex (id feca28aff5a3958840bee985ee7de4d3_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{9049d2f8d8e9aa3c85be51547106db22}) for execution vertex (id 4bf7c1955ffe56e2106d666433eaf137_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b0308fb161fad467c8adad722779434e)
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job 0500955fa50ddbac2a57592e8dbe688f to job leader id monitoring.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager b7e7dd59760d41985ec62ac040ed4d5b@akka://flink/user/rpc/jobmanager_3 for job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job 0500955fa50ddbac2a57592e8dbe688f has a new job leader 5ec62ac0-40ed-4d5b-b7e7-dd59760d4198@akka://flink/user/rpc/jobmanager_3.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager b7e7dd59760d41985ec62ac040ed4d5b@akka://flink/user/rpc/jobmanager_3 for job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: b0308fb161fad467c8adad722779434e.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job 0500955fa50ddbac2a57592e8dbe688f: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot 942f515d-aa56-4aaf-86bc-2639a3f0e668_0 for job 0500955fa50ddbac2a57592e8dbe688f with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request 8d911e7232644aa9027f1b97d57481da for job 0500955fa50ddbac2a57592e8dbe688f from resource manager with leader id b0308fb161fad467c8adad722779434e.
[DEBUG][22-06-18][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for 8d911e7232644aa9027f1b97d57481da.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job 0500955fa50ddbac2a57592e8dbe688f for job leader monitoring.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job 0500955fa50ddbac2a57592e8dbe688f. Address: akka://flink/user/rpc/jobmanager_3, leader id: b7e7dd59760d41985ec62ac040ed4d5b.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 5ec62ac0-40ed-4d5b-b7e7-dd59760d4198.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 0500955fa50ddbac2a57592e8dbe688f.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job 0500955fa50ddbac2a57592e8dbe688f.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor 942f515d-aa56-4aaf-86bc-2639a3f0e668 @ localhost (dataPort=-1).
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer 8d911e7232644aa9027f1b97d57481da to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot 8d911e7232644aa9027f1b97d57481da @ 942f515d-aa56-4aaf-86bc-2639a3f0e668 @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot 8d911e7232644aa9027f1b97d57481da for slot request id SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id 8d911e7232644aa9027f1b97d57481da.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{2ab7c46231ed632da50337739e3269ad}) for execution vertex (id bc764cd8ddf7a0cff126f51c16239658_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{9014d4e9b6ae202382643a33324aae52}) for execution vertex (id feca28aff5a3958840bee985ee7de4d3_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{9049d2f8d8e9aa3c85be51547106db22}) for execution vertex (id 4bf7c1955ffe56e2106d666433eaf137_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (e4ab122db48ab6a4b51fd7a6dfa282bd) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source (1/1) (attempt #0) with attempt id e4ab122db48ab6a4b51fd7a6dfa282bd to 942f515d-aa56-4aaf-86bc-2639a3f0e668 @ localhost (dataPort=-1) with allocation id 8d911e7232644aa9027f1b97d57481da
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (4ec3d330a1c1884412e2a640c7e8f985) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source (1/1) (attempt #0) with attempt id 4ec3d330a1c1884412e2a640c7e8f985 to 942f515d-aa56-4aaf-86bc-2639a3f0e668 @ localhost (dataPort=-1) with allocation id 8d911e7232644aa9027f1b97d57481da
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (d8769874f50b21d0e1971ee07ae725af) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Map -> Sink: Unnamed (1/1) (attempt #0) with attempt id d8769874f50b21d0e1971ee07ae725af to 942f515d-aa56-4aaf-86bc-2639a3f0e668 @ localhost (dataPort=-1) with allocation id 8d911e7232644aa9027f1b97d57481da
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 8d911e7232644aa9027f1b97d57481da.
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id 8d911e7232644aa9027f1b97d57481da for local state stores for job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_8d911e7232644aa9027f1b97d57481da], jobID=0500955fa50ddbac2a57592e8dbe688f, jobVertexID=bc764cd8ddf7a0cff126f51c16239658, subtaskIndex=0}} for 0500955fa50ddbac2a57592e8dbe688f - bc764cd8ddf7a0cff126f51c16239658 - 0 under allocation id 8d911e7232644aa9027f1b97d57481da.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionFactory]Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd): Initialized org.apache.flink.runtime.io.network.partition.ResultPartitionFactory@469520ee
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd), deploy into slot with allocation id 8d911e7232644aa9027f1b97d57481da.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd) switched from CREATED to DEPLOYING.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd) [DEPLOYING]
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd) [DEPLOYING].
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 8d911e7232644aa9027f1b97d57481da.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task e4ab122db48ab6a4b51fd7a6dfa282bd at library cache manager took 0 milliseconds
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_8d911e7232644aa9027f1b97d57481da], jobID=0500955fa50ddbac2a57592e8dbe688f, jobVertexID=feca28aff5a3958840bee985ee7de4d3, subtaskIndex=0}} for 0500955fa50ddbac2a57592e8dbe688f - feca28aff5a3958840bee985ee7de4d3 - 0 under allocation id 8d911e7232644aa9027f1b97d57481da.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionFactory]Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985): Initialized org.apache.flink.runtime.io.network.partition.ResultPartitionFactory@469520ee
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985), deploy into slot with allocation id 8d911e7232644aa9027f1b97d57481da.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985) switched from CREATED to DEPLOYING.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985) [DEPLOYING]
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985) [DEPLOYING].
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 8d911e7232644aa9027f1b97d57481da.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 4ec3d330a1c1884412e2a640c7e8f985 at library cache manager took 8 milliseconds
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_8d911e7232644aa9027f1b97d57481da], jobID=0500955fa50ddbac2a57592e8dbe688f, jobVertexID=4bf7c1955ffe56e2106d666433eaf137, subtaskIndex=0}} for 0500955fa50ddbac2a57592e8dbe688f - 4bf7c1955ffe56e2106d666433eaf137 - 0 under allocation id 8d911e7232644aa9027f1b97d57481da.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.buffer.LocalBufferPool]Using a local buffer pool with 2-10 buffers
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.buffer.LocalBufferPool]Using a local buffer pool with 2-10 buffers
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Registered PipelinedResultPartition 2219a273afafd5602c4ca36fc46bd412#0@e4ab122db48ab6a4b51fd7a6dfa282bd [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.TaskEventDispatcher]registering 2219a273afafd5602c4ca36fc46bd412#0@e4ab122db48ab6a4b51fd7a6dfa282bd
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Registered PipelinedResultPartition 382dfbd7bdf08c91f4fc319e7c43c9ee#0@4ec3d330a1c1884412e2a640c7e8f985 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.TaskEventDispatcher]registering 382dfbd7bdf08c91f4fc319e7c43c9ee#0@4ec3d330a1c1884412e2a640c7e8f985
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af): Created 1 input channels (local: 1, remote: 0, unknown: 0).
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af): Created 1 input channels (local: 1, remote: 0, unknown: 0).
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Using partitioner FORWARD for output 0 of task Source: Custom Source
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Using partitioner FORWARD for output 0 of task Source: Custom Source
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af), deploy into slot with allocation id 8d911e7232644aa9027f1b97d57481da.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af) switched from CREATED to DEPLOYING.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af) [DEPLOYING]
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task d8769874f50b21d0e1971ee07ae725af at library cache manager took 0 milliseconds
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.buffer.LocalBufferPool]Using a local buffer pool with 1-8 buffers
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.buffer.LocalBufferPool]Using a local buffer pool with 1-8 buffers
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6bb462a4
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot 8d911e7232644aa9027f1b97d57481da.
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@60b2ce3a
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@69a8b379
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985) switched from DEPLOYING to INITIALIZING.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af) switched from DEPLOYING to INITIALIZING.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd) switched from DEPLOYING to INITIALIZING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (e4ab122db48ab6a4b51fd7a6dfa282bd) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Map -> Sink: Unnamed (1/1)#0.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source (1/1)#0.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source (1/1)#0.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (d8769874f50b21d0e1971ee07ae725af) switched from DEPLOYING to INITIALIZING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (4ec3d330a1c1884412e2a640c7e8f985) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source (1/1)#0
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source (1/1)#0
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf]-Dorg.apache.flink.shaded.netty4.io.netty.buffer.checkAccessible: true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf]-Dorg.apache.flink.shaded.netty4.io.netty.buffer.checkBounds: true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetectorFactory]Loaded default ResourceLeakDetector: org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector@41a52a8c
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Map -> Sink: Unnamed (1/1)#0
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_bc764cd8ddf7a0cff126f51c16239658_(1/1) with empty state.
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_feca28aff5a3958840bee985ee7de4d3_(1/1) with empty state.
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_ccb29b5204e83e8a588b3828afaa7015_(1/1) with empty state.
[INFO][22-06-18][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-18][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[INFO][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-18][org.apache.hadoop.util.Shell]setsid is not available on this machine. So not using it.
[WARN][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[DEBUG][22-06-18][org.apache.hadoop.util.Shell]setsid exited with exit code 0
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655528148634
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655528148634
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-18][org.apache.hadoop.security.Groups] Creating new Groups object
[DEBUG][22-06-18][org.apache.hadoop.util.NativeCodeLoader]Trying to load the custom-built native-hadoop library...
[DEBUG][22-06-18][org.apache.hadoop.util.NativeCodeLoader]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[DEBUG][22-06-18][org.apache.hadoop.util.NativeCodeLoader]java.library.path=/Users/fabivs/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[WARN][22-06-18][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[DEBUG][22-06-18][org.apache.hadoop.util.PerformanceAdvisory]Falling back to shell based
[DEBUG][22-06-18][org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[DEBUG][22-06-18][org.apache.hadoop.security.Groups]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[DEBUG][22-06-18][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-18][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.impl.MetricsSystemImpl]UgiMetrics, User and group related metrics
[DEBUG][22-06-18][org.apache.hadoop.security.SecurityUtil]Setting hadoop.security.token.service.use_ip to true
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]hadoop login
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]hadoop login commit
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]using local user:UnixPrincipal: fabivs
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]Using user: "UnixPrincipal: fabivs" with name fabivs
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]User entry: "fabivs"
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]UGI loginUser:fabivs (auth:SIMPLE)
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]PrivilegedAction as:fabivs (auth:SIMPLE) from:org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:347)
[DEBUG][22-06-18][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]Connect 0x73efed9f to master1:2181,master2:2181,utility1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:host.name=localhost
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_291
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/tools.jar:/Users/fabivs/myfile/code/bdp-flink/metricflow-flink/target/classes:/Users/fabivs/.m2/repository/org/apache/flink/flink-streaming-java_2.11/1.13.6/flink-streaming-java_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-core/1.13.6/flink-core-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-annotations/1.13.6/flink-annotations-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-metrics-core/1.13.6/flink-metrics-core-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-asm-7/7.1-13.0/flink-shaded-asm-7-7.1-13.0.jar:/Users/fabivs/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar:/Users/fabivs/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/fabivs/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-file-sink-common/1.13.6/flink-file-sink-common-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-runtime_2.11/1.13.6/flink-runtime_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-queryable-state-client-java/1.13.6/flink-queryable-state-client-java-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-hadoop-fs/1.13.6/flink-hadoop-fs-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-zookeeper-3/3.4.14-13.0/flink-shaded-zookeeper-3-3.4.14-13.0.jar:/Users/fabivs/.m2/repository/org/javassist/javassist/3.24.0-GA/javassist-3.24.0-GA.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-actor_2.11/2.5.21/akka-actor_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/com/typesafe/config/1.3.3/config-1.3.3.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-stream_2.11/2.5.21/akka-stream_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/org/reactivestreams/reactive-streams/1.0.2/reactive-streams-1.0.2.jar:/Users/fabivs/.m2/repository/com/typesafe/ssl-config-core_2.11/0.3.7/ssl-config-core_2.11-0.3.7.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.1/scala-parser-combinators_2.11-1.1.1.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-protobuf_2.11/2.5.21/akka-protobuf_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-slf4j_2.11/2.5.21/akka-slf4j_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/org/clapper/grizzled-slf4j_2.11/1.3.2/grizzled-slf4j_2.11-1.3.2.jar:/Users/fabivs/.m2/repository/com/github/scopt/scopt_2.11/3.5.0/scopt_2.11-3.5.0.jar:/Users/fabivs/.m2/repository/com/twitter/chill_2.11/0.7.6/chill_2.11-0.7.6.jar:/Users/fabivs/.m2/repository/com/twitter/chill-java/0.7.6/chill-java-0.7.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-guava/18.0-13.0/flink-shaded-guava-18.0-13.0.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-math3/3.5/commons-math3-3.5.jar:/Users/fabivs/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/fabivs/.m2/repository/org/apache/flink/force-shading/1.13.6/force-shading-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-java/1.13.6/flink-java-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-connector-base/1.13.6/flink-connector-base-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka_2.11/2.4.1/kafka_2.11-2.4.1.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.10.0/jackson-databind-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.10.0/jackson-annotations-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.10.0/jackson-core-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.10.0/jackson-module-scala_2.11-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.10.0/jackson-module-paranamer-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.10.0/jackson-dataformat-csv-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.10.0/jackson-datatype-jdk8-2.10.0.jar:/Users/fabivs/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/Users/fabivs/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.11/2.1.2/scala-collection-compat_2.11-2.1.2.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.11/0.9.0/scala-java8-compat_2.11-0.9.0.jar:/Users/fabivs/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/fabivs/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/fabivs/.m2/repository/com/typesafe/scala-logging/scala-logging_2.11/3.9.2/scala-logging_2.11-3.9.2.jar:/Users/fabivs/.m2/repository/org/apache/zookeeper/zookeeper/3.5.7/zookeeper-3.5.7.jar:/Users/fabivs/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.7/zookeeper-jute-3.5.7.jar:/Users/fabivs/.m2/repository/io/netty/netty-handler/4.1.45.Final/netty-handler-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-common/4.1.45.Final/netty-common-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-buffer/4.1.45.Final/netty-buffer-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport/4.1.45.Final/netty-transport-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-resolver/4.1.45.Final/netty-resolver-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-codec/4.1.45.Final/netty-codec-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport-native-epoll/4.1.45.Final/netty-transport-native-epoll-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.45.Final/netty-transport-native-unix-common-4.1.45.Final.jar:/Users/fabivs/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka-streams/2.4.1/kafka-streams-2.4.1.jar:/Users/fabivs/.m2/repository/org/apache/kafka/connect-json/2.4.1/connect-json-2.4.1.jar:/Users/fabivs/.m2/repository/org/apache/kafka/connect-api/2.4.1/connect-api-2.4.1.jar:/Users/fabivs/.m2/repository/org/rocksdb/rocksdbjni/5.18.3/rocksdbjni-5.18.3.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar:/Users/fabivs/.m2/repository/com/github/luben/zstd-jni/1.4.3-1/zstd-jni-1.4.3-1.jar:/Users/fabivs/.m2/repository/org/lz4/lz4-java/1.6.0/lz4-java-1.6.0.jar:/Users/fabivs/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-clients_2.11/1.13.6/flink-clients_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-optimizer_2.11/1.13.6/flink-optimizer_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-runtime-web_2.11/1.13.6/flink-runtime-web_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-netty/4.1.49.Final-13.0/flink-shaded-netty-4.1.49.Final-13.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-jackson/2.12.1-13.0/flink-shaded-jackson-2.12.1-13.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-connector-kafka_2.11/1.13.6/flink-connector-kafka_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar:/Users/fabivs/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/fabivs/.m2/repository/org/springframework/kafka/spring-kafka/2.7.8/spring-kafka-2.7.8.jar:/Users/fabivs/.m2/repository/org/springframework/spring-context/5.3.11/spring-context-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-aop/5.3.11/spring-aop-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-beans/5.3.11/spring-beans-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-core/5.3.11/spring-core-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-jcl/5.3.11/spring-jcl-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-expression/5.3.11/spring-expression-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-messaging/5.3.11/spring-messaging-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-tx/5.3.11/spring-tx-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/retry/spring-retry/1.3.1/spring-retry-1.3.1.jar:/Users/fabivs/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/Users/fabivs/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/Users/fabivs/.m2/repository/org/apache/logging/log4j/log4j-to-slf4j/2.14.0/log4j-to-slf4j-2.14.0.jar:/Users/fabivs/.m2/repository/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-common/3.2.2/hadoop-common-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-annotations/3.2.2/hadoop-annotations-3.2.2.jar:/Users/fabivs/.m2/repository/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar:/Users/fabivs/.m2/repository/com/google/guava/failureaccess/1.0/failureaccess-1.0.jar:/Users/fabivs/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/fabivs/.m2/repository/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar:/Users/fabivs/.m2/repository/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.jar:/Users/fabivs/.m2/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/Users/fabivs/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.jar:/Users/fabivs/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/Users/fabivs/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/Users/fabivs/.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar:/Users/fabivs/.m2/repository/commons-io/commons-io/2.5/commons-io-2.5.jar:/Users/fabivs/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/Users/fabivs/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/fabivs/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/fabivs/.m2/repository/javax/activation/javax.activation-api/1.2.0/javax.activation-api-1.2.0.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-server/9.4.20.v20190813/jetty-server-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-http/9.4.20.v20190813/jetty-http-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-io/9.4.20.v20190813/jetty-io-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-util/9.4.20.v20190813/jetty-util-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.20.v20190813/jetty-servlet-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-security/9.4.20.v20190813/jetty-security-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.20.v20190813/jetty-webapp-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.20.v20190813/jetty-xml-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/Users/fabivs/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/Users/fabivs/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/fabivs/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.2/jackson-core-asl-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.2/jackson-mapper-asl-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/Users/fabivs/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/fabivs/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar:/Users/fabivs/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/Users/fabivs/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/fabivs/.m2/repository/com/google/re2j/re2j/1.1/re2j-1.1.jar:/Users/fabivs/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/fabivs/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-auth/3.2.2/hadoop-auth-3.2.2.jar:/Users/fabivs/.m2/repository/com/nimbusds/nimbus-jose-jwt/7.9/nimbus-jose-jwt-7.9.jar:/Users/fabivs/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/Users/fabivs/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/Users/fabivs/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/Users/fabivs/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar:/Users/fabivs/.m2/repository/com/jcraft/jsch/0.1.55/jsch-0.1.55.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar:/Users/fabivs/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-compress/1.19/commons-compress-1.19.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/Users/fabivs/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/Users/fabivs/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/Users/fabivs/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-client/3.2.2/hadoop-client-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.2.2/hadoop-hdfs-client-3.2.2.jar:/Users/fabivs/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/Users/fabivs/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.2.2/hadoop-yarn-api-3.2.2.jar:/Users/fabivs/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.2.2/hadoop-yarn-client-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.2.2/hadoop-mapreduce-client-core-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.2.2/hadoop-yarn-common-3.2.2.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.10/jackson-module-jaxb-annotations-2.9.10.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.10/jackson-jaxrs-json-provider-2.9.10.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.10/jackson-jaxrs-base-2.9.10.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.2.2/hadoop-mapreduce-client-jobclient-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.2.2/hadoop-mapreduce-client-common-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-common/2.3.5/hbase-common-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-logging/2.3.5/hbase-logging-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-miscellaneous/3.3.0/hbase-shaded-miscellaneous-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-gson/3.3.0/hbase-shaded-gson-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-netty/3.3.0/hbase-shaded-netty-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/fabivs/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-hbase_2.11/1.10.3/flink-hbase_2.11-1.10.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-server/1.4.3/hbase-server-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-procedure/1.4.3/hbase-procedure-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-common/1.4.3/hbase-common-1.4.3-tests.jar:/Users/fabivs/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-prefix-tree/1.4.3/hbase-prefix-tree-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-metrics-api/1.4.3/hbase-metrics-api-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-metrics/1.4.3/hbase-metrics-1.4.3.jar:/Users/fabivs/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/fabivs/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/Users/fabivs/.m2/repository/io/netty/netty-all/4.1.8.Final/netty-all-4.1.8.Final.jar:/Users/fabivs/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/Users/fabivs/.m2/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/fabivs/.m2/repository/junit/junit/4.12/junit-4.12.jar:/Users/fabivs/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-client/2.3.5/hbase-client-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-protobuf/3.3.0/hbase-shaded-protobuf-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-hadoop-compat/2.3.5/hbase-hadoop-compat-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/2.3.5/hbase-hadoop2-compat-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-protocol-shaded/2.3.5/hbase-protocol-shaded-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-protocol/2.3.5/hbase-protocol-2.3.5.jar:/Users/fabivs/.m2/repository/org/jruby/jcodings/jcodings/1.0.18/jcodings-1.0.18.jar:/Users/fabivs/.m2/repository/org/jruby/joni/joni/2.1.11/joni-2.1.11.jar:/Users/fabivs/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.6/metrics-core-3.2.6.jar:/Users/fabivs/.m2/repository/com/alibaba/fastjson/1.2.75/fastjson-1.2.75.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/fabivs/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.16
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:user.name=fabivs
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/fabivs
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/fabivs/myfile/code/bdp-flink
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.free=148MB
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.max=1820MB
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.total=314MB
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=master1:2181,master2:2181,utility1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$668/263989984@528b0f2a
[INFO][22-06-18][org.apache.zookeeper.common.X509Util]Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[INFO][22-06-18][org.apache.zookeeper.ClientCnxnSocket]jute.maxbuffer value is 4194304 Bytes
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]zookeeper.request.timeout value is 0. feature enabled=
[DEBUG][22-06-18][org.apache.zookeeper.SaslServerPrincipal]Canonicalized address to master2.cluster
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master2/118.190.145.3:2181. Will not attempt to authenticate using SASL (unknown error)
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 310e2c6d8836963b6bbd065c196008a0: SlotReport{SlotStatus{slotID=942f515d-aa56-4aaf-86bc-2639a3f0e668_0, allocationID=8d911e7232644aa9027f1b97d57481da, jobID=0500955fa50ddbac2a57592e8dbe688f, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 942f515d-aa56-4aaf-86bc-2639a3f0e668: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 310e2c6d8836963b6bbd065c196008a0: SlotReport{SlotStatus{slotID=942f515d-aa56-4aaf-86bc-2639a3f0e668_0, allocationID=8d911e7232644aa9027f1b97d57481da, jobID=0500955fa50ddbac2a57592e8dbe688f, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 942f515d-aa56-4aaf-86bc-2639a3f0e668: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 310e2c6d8836963b6bbd065c196008a0: SlotReport{SlotStatus{slotID=942f515d-aa56-4aaf-86bc-2639a3f0e668_0, allocationID=8d911e7232644aa9027f1b97d57481da, jobID=0500955fa50ddbac2a57592e8dbe688f, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 942f515d-aa56-4aaf-86bc-2639a3f0e668: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[WARN][22-06-18][org.apache.zookeeper.ClientCnxn]Client session timed out, have not heard from server in 30002ms for sessionid 0x0
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Client session timed out, have not heard from server in 30002ms for sessionid 0x0, closing socket connection and attempting reconnect
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxnSocketNIO]Ignoring exception during shutdown input
java.net.SocketException: Socket is not connected
	at sun.nio.ch.Net.translateToSocketException(Net.java:122)
	at sun.nio.ch.Net.translateException(Net.java:156)
	at sun.nio.ch.Net.translateException(Net.java:162)
	at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:401)
	at org.apache.zookeeper.ClientCnxnSocketNIO.cleanup(ClientCnxnSocketNIO.java:198)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1338)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanAndNotifyState(ClientCnxn.java:1276)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1254)
Caused by: java.nio.channels.NotYetConnectedException
	at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:780)
	at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:399)
	... 4 more
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxnSocketNIO]Ignoring exception during shutdown output
java.net.SocketException: Socket is not connected
	at sun.nio.ch.Net.translateToSocketException(Net.java:122)
	at sun.nio.ch.Net.translateException(Net.java:156)
	at sun.nio.ch.Net.translateException(Net.java:162)
	at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:409)
	at org.apache.zookeeper.ClientCnxnSocketNIO.cleanup(ClientCnxnSocketNIO.java:205)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1338)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanAndNotifyState(ClientCnxn.java:1276)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1254)
Caused by: java.nio.channels.NotYetConnectedException
	at sun.nio.ch.SocketChannelImpl.shutdownOutput(SocketChannelImpl.java:797)
	at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:407)
	... 4 more
[WARN][22-06-18][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]0x73efed9f to master1:2181,master2:2181,utility1:2181 failed for get of /hbase/hbaseid, code = CONNECTIONLOSS, retries = 1
[DEBUG][22-06-18][org.apache.zookeeper.SaslServerPrincipal]Canonicalized address to utility1.cluster
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Opening socket connection to server utility1/118.190.207.81:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /192.168.3.92:51788, server: utility1/118.190.207.81:2181
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Session establishment request sent on utility1/118.190.207.81:2181
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Session establishment complete on server utility1/118.190.207.81:2181, sessionid = 0x30085a06e27001d, negotiated timeout = 40000
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Reading reply sessionid:0x30085a06e27001d, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,81605000548,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a31363030306effffffd2ffffffac57ffffffa31c1bffffffd450425546a2435623632316435312d376363372d343162362d396438632d336662646463326230303833,s{47244640348,73014444056,1646900261205,1650525792431,20,0,0,0,67,0,47244640348} 
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector]-Dorg.apache.hbase.thirdparty.io.netty.leakDetection.level: simple
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector]-Dorg.apache.hbase.thirdparty.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetectorFactory]Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@8aaac5e
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-18][org.apache.hadoop.hbase.util.ClassSize]Using Unsafe to estimate memory layout
[DEBUG][22-06-18][org.apache.hadoop.hbase.ipc.AbstractRpcClient]Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@63383e2d, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamMap_4bf7c1955ffe56e2106d666433eaf137_(1/1) with empty state.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af)/InputChannelInfo{gateIdx=0, inputChannelIdx=0} finished recovering input.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af)/InputChannelInfo{gateIdx=1, inputChannelIdx=0} finished recovering input.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af) switched from INITIALIZING to RUNNING.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate]Converting recovered input channels (1 channels)
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (d8769874f50b21d0e1971ee07ae725af) switched from INITIALIZING to RUNNING.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister]stopPersisting -1, lastSeenBarrier = -1 (COMPLETED) @ InputChannelInfo{gateIdx=0, inputChannelIdx=0}
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel]LocalInputChannel [2219a273afafd5602c4ca36fc46bd412#0@e4ab122db48ab6a4b51fd7a6dfa282bd]: Requesting LOCAL subpartition 0 of partition 2219a273afafd5602c4ca36fc46bd412#0@e4ab122db48ab6a4b51fd7a6dfa282bd. ChannelStatePersister(lastSeenBarrier=-1 (COMPLETED)}
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Requesting subpartition 0 of PipelinedResultPartition 2219a273afafd5602c4ca36fc46bd412#0@e4ab122db48ab6a4b51fd7a6dfa282bd [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.PipelinedSubpartition]Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd): Creating read view for subpartition 0 of partition 2219a273afafd5602c4ca36fc46bd412#0@e4ab122db48ab6a4b51fd7a6dfa282bd.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartition]Created PipelinedSubpartitionView(index: 0) of ResultPartition 2219a273afafd5602c4ca36fc46bd412#0@e4ab122db48ab6a4b51fd7a6dfa282bd
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate]Converting recovered input channels (1 channels)
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister]stopPersisting -1, lastSeenBarrier = -1 (COMPLETED) @ InputChannelInfo{gateIdx=1, inputChannelIdx=0}
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel]LocalInputChannel [382dfbd7bdf08c91f4fc319e7c43c9ee#0@4ec3d330a1c1884412e2a640c7e8f985]: Requesting LOCAL subpartition 0 of partition 382dfbd7bdf08c91f4fc319e7c43c9ee#0@4ec3d330a1c1884412e2a640c7e8f985. ChannelStatePersister(lastSeenBarrier=-1 (COMPLETED)}
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Requesting subpartition 0 of PipelinedResultPartition 382dfbd7bdf08c91f4fc319e7c43c9ee#0@4ec3d330a1c1884412e2a640c7e8f985 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.PipelinedSubpartition]Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985): Creating read view for subpartition 0 of partition 382dfbd7bdf08c91f4fc319e7c43c9ee#0@4ec3d330a1c1884412e2a640c7e8f985.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartition]Created PipelinedSubpartitionView(index: 0) of ResultPartition 382dfbd7bdf08c91f4fc319e7c43c9ee#0@4ec3d330a1c1884412e2a640c7e8f985
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 310e2c6d8836963b6bbd065c196008a0: SlotReport{SlotStatus{slotID=942f515d-aa56-4aaf-86bc-2639a3f0e668_0, allocationID=8d911e7232644aa9027f1b97d57481da, jobID=0500955fa50ddbac2a57592e8dbe688f, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 942f515d-aa56-4aaf-86bc-2639a3f0e668: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Got ping response for sessionid: 0x30085a06e27001d after 37ms
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 310e2c6d8836963b6bbd065c196008a0: SlotReport{SlotStatus{slotID=942f515d-aa56-4aaf-86bc-2639a3f0e668_0, allocationID=8d911e7232644aa9027f1b97d57481da, jobID=0500955fa50ddbac2a57592e8dbe688f, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 942f515d-aa56-4aaf-86bc-2639a3f0e668: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 89ef3b2353761a36311e9b82abeb1b76.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 310e2c6d8836963b6bbd065c196008a0: SlotReport{SlotStatus{slotID=942f515d-aa56-4aaf-86bc-2639a3f0e668_0, allocationID=8d911e7232644aa9027f1b97d57481da, jobID=0500955fa50ddbac2a57592e8dbe688f, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor 942f515d-aa56-4aaf-86bc-2639a3f0e668: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 572242d80383e84707edb8b0e28af2cb.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Got ping response for sessionid: 0x30085a06e27001d after 45ms
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[WARN][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985).
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd).
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.TaskEventDispatcher]unregistering 382dfbd7bdf08c91f4fc319e7c43c9ee#0@4ec3d330a1c1884412e2a640c7e8f985
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.TaskEventDispatcher]unregistering 2219a273afafd5602c4ca36fc46bd412#0@e4ab122db48ab6a4b51fd7a6dfa282bd
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartition]Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985): Releasing PipelinedResultPartition 382dfbd7bdf08c91f4fc319e7c43c9ee#0@4ec3d330a1c1884412e2a640c7e8f985 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.PipelinedSubpartition]Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985): Released PipelinedSubpartition#0 [number of buffers: 1 (4 bytes), number of buffers in backlog: 0, finished? false, read view? false].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Released partition 382dfbd7bdf08c91f4fc319e7c43c9ee#0 produced by 4ec3d330a1c1884412e2a640c7e8f985.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartition]Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd): Releasing PipelinedResultPartition 2219a273afafd5602c4ca36fc46bd412#0@e4ab122db48ab6a4b51fd7a6dfa282bd [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source (1/1)#0 (4ec3d330a1c1884412e2a640c7e8f985) [FAILED]
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.PipelinedSubpartition]Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd): Released PipelinedSubpartition#0 [number of buffers: 1 (4 bytes), number of buffers in backlog: 0, finished? false, read view? false].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Released partition 2219a273afafd5602c4ca36fc46bd412#0 produced by e4ab122db48ab6a4b51fd7a6dfa282bd.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source (1/1)#0 (e4ab122db48ab6a4b51fd7a6dfa282bd) [FAILED]
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source (1/1)#0 e4ab122db48ab6a4b51fd7a6dfa282bd.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source (1/1)#0 4ec3d330a1c1884412e2a640c7e8f985.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (e4ab122db48ab6a4b51fd7a6dfa282bd) switched from INITIALIZING to FAILED on 942f515d-aa56-4aaf-86bc-2639a3f0e668 @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{2ab7c46231ed632da50337739e3269ad}) for execution vertex (id bc764cd8ddf7a0cff126f51c16239658_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task bc764cd8ddf7a0cff126f51c16239658_0.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]3 tasks should be restarted to recover the failed task bc764cd8ddf7a0cff126f51c16239658_0. 
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]No SlotExecutionVertexAssignment for logical null from physical SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2}}
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{9014d4e9b6ae202382643a33324aae52}) for execution vertex (id feca28aff5a3958840bee985ee7de4d3_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2}) from feca28aff5a3958840bee985ee7de4d3_0
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{9049d2f8d8e9aa3c85be51547106db22}) for execution vertex (id 4bf7c1955ffe56e2106d666433eaf137_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2}) from 4bf7c1955ffe56e2106d666433eaf137_0
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (0500955fa50ddbac2a57592e8dbe688f) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (4ec3d330a1c1884412e2a640c7e8f985) switched from INITIALIZING to CANCELING.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 4ec3d330a1c1884412e2a640c7e8f985.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (d8769874f50b21d0e1971ee07ae725af) switched from RUNNING to CANCELING.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Attempting to cancel task Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af).
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af) switched from RUNNING to CANCELING.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Triggering cancellation of task code Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af).
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (4ec3d330a1c1884412e2a640c7e8f985) switched from CANCELING to CANCELED.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source (1/1) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{9014d4e9b6ae202382643a33324aae52}) for execution vertex (id feca28aff5a3958840bee985ee7de4d3_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Discarding the results produced by task execution 4ec3d330a1c1884412e2a640c7e8f985.
[DEBUG][22-06-18][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]Close zookeeper connection 0x73efed9f to master1:2181,master2:2181,utility1:2181
[DEBUG][22-06-18][org.apache.hadoop.hbase.ipc.AbstractRpcClient]Stopping rpc client
[DEBUG][22-06-18][org.apache.zookeeper.ZooKeeper]Closing session: 0x30085a06e27001d
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Closing client for session: 0x30085a06e27001d
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af) switched from CANCELING to CANCELED.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af).
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Release task Map -> Sink: Unnamed (1/1)#0 network resources (state: CANCELED).
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af): Releasing SingleInputGate{owningTaskName='Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af)', gateIndex=0}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate]Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af): Releasing SingleInputGate{owningTaskName='Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af)', gateIndex=1}.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Map -> Sink: Unnamed (1/1)#0 (d8769874f50b21d0e1971ee07ae725af) [CANCELED]
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Discarding the results produced by task execution 4ec3d330a1c1884412e2a640c7e8f985.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state CANCELED to JobManager for task Map -> Sink: Unnamed (1/1)#0 d8769874f50b21d0e1971ee07ae725af.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 4ec3d330a1c1884412e2a640c7e8f985.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (d8769874f50b21d0e1971ee07ae725af) switched from CANCELING to CANCELED.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Map -> Sink: Unnamed (1/1) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{9049d2f8d8e9aa3c85be51547106db22}) for execution vertex (id 4bf7c1955ffe56e2106d666433eaf137_0) from the physical slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot 8d911e7232644aa9027f1b97d57481da.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{dd136d8ad5f92000e4e3210c0f44f7b2})
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot 8d911e7232644aa9027f1b97d57481da @ 942f515d-aa56-4aaf-86bc-2639a3f0e668 @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job 0500955fa50ddbac2a57592e8dbe688f.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job 0500955fa50ddbac2a57592e8dbe688f
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (0500955fa50ddbac2a57592e8dbe688f) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job 0500955fa50ddbac2a57592e8dbe688f: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph 0500955fa50ddbac2a57592e8dbe688f reached terminal state FAILED.
[INFO][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-18][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Reading reply sessionid:0x30085a06e27001d, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,81605000549,0  request:: null response:: null
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Disconnecting client for session: 0x30085a06e27001d
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 4ec3d330a1c1884412e2a640c7e8f985.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job 0500955fa50ddbac2a57592e8dbe688f under leader id 5ec62ac0-40ed-4d5b-b7e7-dd59760d4198 reached a globally terminal state FAILED.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 4ec3d330a1c1884412e2a640c7e8f985.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job 0500955fa50ddbac2a57592e8dbe688f reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job 0500955fa50ddbac2a57592e8dbe688f under leader id 5ec62ac0-40ed-4d5b-b7e7-dd59760d4198.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection 89ef3b2353761a36311e9b82abeb1b76.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source (1/1) - execution #0 to FAILED while being CANCELED.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(0500955fa50ddbac2a57592e8dbe688f).
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection 942f515d-aa56-4aaf-86bc-2639a3f0e668 because: The TaskExecutor is shutting down.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor 310e2c6d8836963b6bbd065c196008a0 from the slot manager.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor 942f515d-aa56-4aaf-86bc-2639a3f0e668.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID bc764cd8ddf7a0cff126f51c16239658_0
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID feca28aff5a3958840bee985ee7de4d3_0
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID 4bf7c1955ffe56e2106d666433eaf137_0
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job 0500955fa50ddbac2a57592e8dbe688f.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor 942f515d-aa56-4aaf-86bc-2639a3f0e668 because: Stopping JobMaster for job metricStream(0500955fa50ddbac2a57592e8dbe688f).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [8d911e7232644aa9027f1b97d57481da].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(0500955fa50ddbac2a57592e8dbe688f).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: 8d911e7232644aa9027f1b97d57481da, jobId: 0500955fa50ddbac2a57592e8dbe688f).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection 89ef3b2353761a36311e9b82abeb1b76.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(0500955fa50ddbac2a57592e8dbe688f).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job 0500955fa50ddbac2a57592e8dbe688f from job leader id monitoring.
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Session: 0x30085a06e27001d closed
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]EventThread shut down for session: 0x30085a06e27001d
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager b7e7dd59760d41985ec62ac040ed4d5b@akka://flink/user/rpc/jobmanager_3 for job 0500955fa50ddbac2a57592e8dbe688f from the resource manager.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job 0500955fa50ddbac2a57592e8dbe688f.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job 0500955fa50ddbac2a57592e8dbe688f.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id 8d911e7232644aa9027f1b97d57481da because: Stopping JobMaster for job metricStream(0500955fa50ddbac2a57592e8dbe688f).
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id 8d911e7232644aa9027f1b97d57481da.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for 8d911e7232644aa9027f1b97d57481da.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id 8d911e7232644aa9027f1b97d57481da.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-18][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-de511f4b-c251-4744-bc19-332024b7cb4e
[INFO][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job 0500955fa50ddbac2a57592e8dbe688f has been terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job 0500955fa50ddbac2a57592e8dbe688f under leader id 5ec62ac0-40ed-4d5b-b7e7-dd59760d4198 has been terminated.
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-f839713b-c4a2-4789-a72c-d8566cd31ea2
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-18][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-152382d6-cb29-464a-a445-3eeab4d424bc
[INFO][22-06-18][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-18][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-18][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-18][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-18][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:51770
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.util.Properties
[DEBUG][22-06-18][org.apache.flink.api.java.ClosureCleaner]Dig to clean the java.lang.Long
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming OneInputTransformation{id=4, name='Map', outputType=PojoType<renaissance.bean.MetricBean, fields = [hostName: String, id: String, name: String, timestamp: String, value: String]>, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming UnionTransformation{id=3, name='Union', outputType=String, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=1, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 1
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySourceTransformation{id=2, name='Custom Source', outputType=String, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 2
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 4
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphGenerator]Transforming LegacySinkTransformation{id=5, name='Unnamed', outputType=GenericType<java.lang.Object>, parallelism=1}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraph]Vertex: 5
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'bc764cd8ddf7a0cff126f51c16239658' for node 'Source: Custom Source-1' {id: 1, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'feca28aff5a3958840bee985ee7de4d3' for node 'Source: Custom Source-2' {id: 2, parallelism: 1, user function: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash '4bf7c1955ffe56e2106d666433eaf137' for node 'Map-4' {id: 4, parallelism: 1, user function: renaissance.profunc.MetricBeanMap}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamGraphHasherV2]Generated hash 'ccb29b5204e83e8a588b3828afaa7015' for node 'Sink: Unnamed-5' {id: 5, parallelism: 1, user function: renaissance.sink.MetricSink}
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 4
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 1
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]CONNECTED: ForwardPartitioner - 1 -> 4
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]Parallelism set: 1 for 2
[DEBUG][22-06-18][org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator]CONNECTED: ForwardPartitioner - 2 -> 4
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils]The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting Flink Mini Cluster
[DEBUG][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=1, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=5 min}}
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting Metrics Registry
[INFO][22-06-18][org.apache.flink.runtime.metrics.MetricRegistryImpl]No metrics reporter configured, no metrics will be exposed/reported.
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting RPC Service(s)
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-18][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]Default Loggers started
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Trying to start local actor system
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"akka.dispatch.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"30s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
[INFO][22-06-18][akka.event.slf4j.Slf4jLogger]Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]logger log1-Slf4jLogger started
[DEBUG][22-06-18][akka.event.EventStream]Default Loggers started
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils]Actor system started at akka://flink-metrics
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name MetricQueryService.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting high-availability services
[INFO][22-06-18][org.apache.flink.runtime.blob.BlobServer]Created BLOB server storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-7693c658-1b06-4236-98a8-ff737289829d
[DEBUG][22-06-18][org.apache.flink.util.NetUtils]Trying to open socket on port 0
[INFO][22-06-18][org.apache.flink.runtime.blob.BlobServer]Started BLOB server at 0.0.0.0:52210 - max concurrent requests: 50 - max backlog: 1000
[INFO][22-06-18][org.apache.flink.runtime.blob.PermanentBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-8659ef52-5cb4-4dda-9772-06a7e3ae2c79
[INFO][22-06-18][org.apache.flink.runtime.blob.TransientBlobCache]Created BLOB cache storage directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/blobStore-4e43d72b-3d16-4457-9e74-bd13804b41c7
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Starting 1 TaskManger(s)
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskManagerRunner]Starting TaskManager with ResourceID: a6854902-73ff-42a3-9ee7-8543e53de45d
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskManagerServices]Temporary file directory '/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T': total 460 GB, usable 367 GB (79.78% usable)
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-1f5e2dda-c7cc-4d5a-8b95-fb9bcc52df99 for spill files.
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-9eaf041a-39a4-4fbc-9ffc-c6e6c30a5755 for spill files.
[INFO][22-06-18][org.apache.flink.runtime.io.network.buffer.NetworkBufferPool]Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
[INFO][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting the network environment and its components.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Starting network connection manager
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.KvStateService]Starting the kvState service and its components.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration]Messages have a max timeout of 300000 ms
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting AkkaRpcActor with name taskmanager_0.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Start job leader service.
[INFO][22-06-18][org.apache.flink.runtime.filecache.FileCache]User file cache uses directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-1c5f4eb0-a14a-4d91-b294-515ee8bde217
[DEBUG][22-06-18][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher REST endpoint.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Starting rest endpoint.
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[WARN][22-06-18][org.apache.flink.runtime.webmonitor.WebMonitorUtils]Log file environment variable 'log.file' is not set.
[WARN][22-06-18][org.apache.flink.runtime.webmonitor.WebMonitorUtils]JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@351f2244 under DELETE@/v1/cluster.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@351f2244 under DELETE@/cluster.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@73fc518f under GET@/v1/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@73fc518f under GET@/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@2de50ee4 under GET@/v1/datasets.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@2de50ee4 under GET@/datasets.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@ad9e63e under GET@/v1/datasets/delete/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@ad9e63e under GET@/datasets/delete/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@47fbc56 under DELETE@/v1/datasets/:datasetid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@47fbc56 under DELETE@/datasets/:datasetid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@151ef57f under GET@/v1/jars.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@151ef57f under GET@/jars.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@10895b16 under POST@/v1/jars/upload.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@10895b16 under POST@/jars/upload.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@5524b72f under DELETE@/v1/jars/:jarid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@5524b72f under DELETE@/jars/:jarid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@2cc03cd1 under GET@/v1/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@2cc03cd1 under GET@/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@4e17913b under POST@/v1/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@4e17913b under POST@/jars/:jarid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@149c3204 under POST@/v1/jars/:jarid/run.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@149c3204 under POST@/jars/:jarid/run.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@64f16277 under GET@/v1/jobmanager/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@64f16277 under GET@/jobmanager/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@497aec8c under GET@/v1/jobmanager/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@497aec8c under GET@/jobmanager/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@3b9632d1 under GET@/v1/jobmanager/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@3b9632d1 under GET@/jobmanager/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@4e6f2bb5 under GET@/v1/jobmanager/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@4e6f2bb5 under GET@/jobmanager/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@21e20ad5 under GET@/v1/jobmanager/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@21e20ad5 under GET@/jobmanager/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@3f628ce9 under GET@/v1/jobmanager/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@3f628ce9 under GET@/jobmanager/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@35e8316e under GET@/v1/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@35e8316e under GET@/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@26d96e5 under POST@/v1/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@26d96e5 under POST@/jobs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@336880df under GET@/v1/jobs/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@336880df under GET@/jobs/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@1846579f under GET@/v1/jobs/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@1846579f under GET@/jobs/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@6cd166b8 under GET@/v1/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@6cd166b8 under GET@/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2650f79 under PATCH@/v1/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@2650f79 under PATCH@/jobs/:jobid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@75fc1992 under GET@/v1/jobs/:jobid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@75fc1992 under GET@/jobs/:jobid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@5fac521d under GET@/v1/jobs/:jobid/checkpoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@5fac521d under GET@/jobs/:jobid/checkpoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@38af1bf6 under GET@/v1/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@38af1bf6 under GET@/jobs/:jobid/checkpoints/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@129bd55d under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@129bd55d under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@7be7e15 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@7be7e15 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@3abfe845 under GET@/v1/jobs/:jobid/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@3abfe845 under GET@/jobs/:jobid/config.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@7a0f244f under POST@/v1/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@7a0f244f under POST@/jobs/:jobid/coordinators/:operatorid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@3672276e under GET@/v1/jobs/:jobid/exceptions.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@3672276e under GET@/jobs/:jobid/exceptions.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@4248b963 under GET@/v1/jobs/:jobid/execution-result.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@4248b963 under GET@/jobs/:jobid/execution-result.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@7f08caf under GET@/v1/jobs/:jobid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@7f08caf under GET@/jobs/:jobid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@4defd42 under GET@/v1/jobs/:jobid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@4defd42 under GET@/jobs/:jobid/plan.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@2330e3e0 under PATCH@/v1/jobs/:jobid/rescaling.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@2330e3e0 under PATCH@/jobs/:jobid/rescaling.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@24b4d544 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@24b4d544 under GET@/jobs/:jobid/rescaling/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@27a2a089 under POST@/v1/jobs/:jobid/savepoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@27a2a089 under POST@/jobs/:jobid/savepoints.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@54657dd2 under GET@/v1/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@54657dd2 under GET@/jobs/:jobid/savepoints/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@706eab5d under POST@/v1/jobs/:jobid/stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@706eab5d under POST@/jobs/:jobid/stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@72725ee1 under GET@/v1/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@72725ee1 under GET@/jobs/:jobid/vertices/:vertexid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@40e60ece under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@40e60ece under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@3f9270ed under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@3f9270ed under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@3a230001 under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@3a230001 under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@5ac6c4f2 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@5ac6c4f2 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@2aa6311a under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@2aa6311a under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@61f39bb under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@61f39bb under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@249e0271 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@249e0271 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@4893b344 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@4893b344 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@53a665ad under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@53a665ad under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@2c0b4c83 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@2c0b4c83 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@78525ef9 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@78525ef9 under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@2d0ecb24 under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@2d0ecb24 under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@4d654825 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@4d654825 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@3bfc6a5e under GET@/v1/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@3bfc6a5e under GET@/jobs/:jobid/yarn-cancel.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51b35e4e under GET@/v1/jobs/:jobid/yarn-stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@51b35e4e under GET@/jobs/:jobid/yarn-stop.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@abff8b7 under GET@/v1/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@abff8b7 under GET@/overview.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@6d7cada5 under POST@/v1/savepoint-disposal.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@6d7cada5 under POST@/savepoint-disposal.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@350a94ce under GET@/v1/savepoint-disposal/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@350a94ce under GET@/savepoint-disposal/:triggerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@7e00ed0f under GET@/v1/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@7e00ed0f under GET@/taskmanagers.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@b0fc838 under GET@/v1/taskmanagers/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@b0fc838 under GET@/taskmanagers/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@3964d79 under GET@/v1/taskmanagers/:taskmanagerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@3964d79 under GET@/taskmanagers/:taskmanagerid.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@62db0521 under GET@/v1/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@62db0521 under GET@/taskmanagers/:taskmanagerid/log.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@1b4ae4e0 under GET@/v1/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@1b4ae4e0 under GET@/taskmanagers/:taskmanagerid/logs.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@6ef1a1b9 under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@6ef1a1b9 under GET@/taskmanagers/:taskmanagerid/logs/:filename.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@5fbdc49b under GET@/v1/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@5fbdc49b under GET@/taskmanagers/:taskmanagerid/metrics.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@65753040 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@65753040 under GET@/taskmanagers/:taskmanagerid/stdout.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@2954b5ea under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@2954b5ea under GET@/taskmanagers/:taskmanagerid/thread-dump.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@4acb2510 under GET@/v1/:*.
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@4acb2510 under GET@/:*.
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.processId: 3196 (auto-detected)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv4Stack: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]-Djava.net.preferIPv6Addresses: false
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.NetUtil]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelId]-Dio.netty.machineId: 3c:06:30:ff:fe:0a:2a:21 (auto-detected)
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector]-Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numHeapArenas: 16
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.numDirectArenas: 16
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.pageSize: 8192
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxOrder: 11
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.chunkSize: 16777216
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.tinyCacheSize: 512
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.smallCacheSize: 256
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.normalCacheSize: 64
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimInterval: 8192
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.cacheTrimIntervalMillis: 0
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.useCacheForAllThreads: true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator]-Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.allocator.type: pooled
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.threadLocalDirectBufferSize: 0
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.ByteBufUtil]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[DEBUG][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Binding rest endpoint to null:0.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Rest endpoint listening at localhost:52211
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender http://localhost:52211
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Web frontend listening at http://localhost:52211.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]http://localhost:52211 was granted leadership with leaderSessionID=3ef894b5-8c6a-4b81-a347-516e3577b0ae
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader http://localhost:52211 , session=3ef894b5-8c6a-4b81-a347-516e3577b0ae
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name resourcemanager_1.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[DEBUG][22-06-18][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting Dispatcher.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
[DEBUG][22-06-18][org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory]Starting ResourceManager.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Starting the resource manager.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: StandaloneResourceManager
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner]DefaultDispatcherRunner was granted leadership with leader id 30006f04-518c-4cca-b06b-e669c3283036. Creating new DispatcherLeaderProcess.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token a0dbb646a533227701a679c630cf4ced
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Flink Mini Cluster started successfully
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting the slot manager.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Start SessionDispatcherLeaderProcess.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Recover all persisted job graphs.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Successfully recovered 0 persisted job graphs.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=01a679c6-30cf-4ced-a0db-b646a5332277
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(a0dbb646a533227701a679c630cf4ced).
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=30006f04-518c-4cca-b06b-e669c3283036
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_2. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Received JobGraph submission f7e00431a191f50392988496d13be523 (metricStream).
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Submitting job f7e00431a191f50392988496d13be523 (metricStream).
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering TaskManager with ResourceID a6854902-73ff-42a3-9ee7-8543e53de45d (akka://flink/user/rpc/taskmanager_0) at ResourceManager
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 0779c89dc502b0bf9e8d189be92b4d8a.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Registering task executor a6854902-73ff-42a3-9ee7-8543e53de45d under 0779c89dc502b0bf9e8d189be92b4d8a at the slot manager.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Start leadership runner for job f7e00431a191f50392988496d13be523.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Create new JobMasterServiceProcess because we were granted leadership under f7a50145-0949-47e2-8b6d-208acb1cc72c.
[DEBUG][22-06-18][org.apache.flink.client.ClientUtils]Wait until job initialization is finished
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Starting FencedAkkaRpcActor with name jobmanager_3.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Initializing job metricStream (f7e00431a191f50392988496d13be523).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Using restart back off time strategy NoRestartBackoffTimeStrategy for metricStream (f7e00431a191f50392988496d13be523).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Running initialization on master for job metricStream (f7e00431a191f50392988496d13be523).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Successfully ran initialization on master in 1 ms.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Adding 3 vertices from job graph metricStream (f7e00431a191f50392988496d13be523).
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Attaching 3 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex bc764cd8ddf7a0cff126f51c16239658 (Source: Custom Source) to 0 predecessors.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex feca28aff5a3958840bee985ee7de4d3 (Source: Custom Source) to 0 predecessors.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting ExecutionJobVertex 4bf7c1955ffe56e2106d666433eaf137 (Map -> Sink: Unnamed) to 2 predecessors.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting input 0 of vertex 4bf7c1955ffe56e2106d666433eaf137 (Map -> Sink: Unnamed) to intermediate result referenced via predecessor bc764cd8ddf7a0cff126f51c16239658 (Source: Custom Source).
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Connecting input 1 of vertex 4bf7c1955ffe56e2106d666433eaf137 (Map -> Sink: Unnamed) to intermediate result referenced via predecessor feca28aff5a3958840bee985ee7de4d3 (Source: Custom Source).
[INFO][22-06-18][org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology]Built 1 pipelined regions in 2 ms
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Successfully created execution graph from job graph metricStream (f7e00431a191f50392988496d13be523).
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5adca5b3
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Checkpoint storage is set to 'jobmanager'
[DEBUG][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Status of the shared state registry of job f7e00431a191f50392988496d13be523 after restore: SharedStateRegistry{registeredStates={}}.
[INFO][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]No checkpoint found during restore.
[DEBUG][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Resetting the master hooks.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@6d608db1 for metricStream (f7e00431a191f50392988496d13be523).
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Successfully created the JobMasterService for job f7e00431a191f50392988496d13be523 under leader id f7a50145-0949-47e2-8b6d-208acb1cc72c.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Confirm leadership f7a50145-0949-47e2-8b6d-208acb1cc72c.
[INFO][22-06-18][org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService]Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=f7a50145-0949-47e2-8b6d-208acb1cc72c
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Starting execution of job metricStream (f7e00431a191f50392988496d13be523) under job master id 8b6d208acb1cc72cf7a50145094947e2.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (f7e00431a191f50392988496d13be523) switched from state CREATED to RUNNING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (2d0c20acbcd7e2278f1f759d97c10321) switched from CREATED to SCHEDULED.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (5a7eed33f1778c1406b738339da5a6b5) switched from CREATED to SCHEDULED.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (788fee4c04d621243b03fffef57b0772) switched from CREATED to SCHEDULED.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl]Received slot request [SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3}] with resource requirements: ResourceProfile{UNKNOWN}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Request new allocated slot with slot request id SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3} and resource profile ResourceProfile{UNKNOWN}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job f7e00431a191f50392988496d13be523.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{7288acfead59069363c79f000abfa0d2}) for execution vertex (id bc764cd8ddf7a0cff126f51c16239658_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{27aa936427b23e85018405c849c4875d}) for execution vertex (id feca28aff5a3958840bee985ee7de4d3_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Request a logical slot (SlotRequestId{85e436f2363dcf249dca732a2c078f88}) for execution vertex (id 4bf7c1955ffe56e2106d666433eaf137_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(a0dbb646a533227701a679c630cf4ced)
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_1. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Resolved ResourceManager address, beginning registration
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Registration at ResourceManager attempt 1 (timeout=100ms)
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Add job f7e00431a191f50392988496d13be523 to job leader id monitoring.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Job f7e00431a191f50392988496d13be523 has a new job leader f7a50145-0949-47e2-8b6d-208acb1cc72c@akka://flink/user/rpc/jobmanager_3.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registering job manager 8b6d208acb1cc72cf7a50145094947e2@akka://flink/user/rpc/jobmanager_3 for job f7e00431a191f50392988496d13be523.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Registered job manager 8b6d208acb1cc72cf7a50145094947e2@akka://flink/user/rpc/jobmanager_3 for job f7e00431a191f50392988496d13be523.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_1 was successful.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]JobManager successfully registered at ResourceManager, leader id: a0dbb646a533227701a679c630cf4ced.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received resource requirements from job f7e00431a191f50392988496d13be523: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job f7e00431a191f50392988496d13be523.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Starting allocation of slot a6854902-73ff-42a3-9ee7-8543e53de45d_0 for job f7e00431a191f50392988496d13be523 with resource profile ResourceProfile{UNKNOWN}.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Receive slot request a2d6392d18cb2c712b91cd29063ece26 for job f7e00431a191f50392988496d13be523 from resource manager with leader id a0dbb646a533227701a679c630cf4ced.
[DEBUG][22-06-18][org.apache.flink.runtime.memory.MemoryManager]Initialized MemoryManager with total memory size 134217728 and page size 32768.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Allocated slot for a2d6392d18cb2c712b91cd29063ece26.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Add job f7e00431a191f50392988496d13be523 for job leader monitoring.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]New leader information for job f7e00431a191f50392988496d13be523. Address: akka://flink/user/rpc/jobmanager_3, leader id: 8b6d208acb1cc72cf7a50145094947e2.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id f7a50145-0949-47e2-8b6d-208acb1cc72c.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Resolved JobManager address, beginning registration
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration at JobManager attempt 1 (timeout=100ms)
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Register new TaskExecutor a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job f7e00431a191f50392988496d13be523.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Establish JobManager connection for job f7e00431a191f50392988496d13be523.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Offer reserved slots to the leader of job f7e00431a191f50392988496d13be523.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Received 1 slot offers from TaskExecutor a6854902-73ff-42a3-9ee7-8543e53de45d @ localhost (dataPort=-1).
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Matched slot offer a2d6392d18cb2c712b91cd29063ece26 to requirement ResourceProfile{UNKNOWN}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Matched slot AllocatedSlot a2d6392d18cb2c712b91cd29063ece26 @ a6854902-73ff-42a3-9ee7-8543e53de45d @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=false, unfulfillableSince=9223372036854775807}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Reserve slot a2d6392d18cb2c712b91cd29063ece26 for slot request id SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool]Reserve free slot with allocation id a2d6392d18cb2c712b91cd29063ece26.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{7288acfead59069363c79f000abfa0d2}) for execution vertex (id bc764cd8ddf7a0cff126f51c16239658_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{27aa936427b23e85018405c849c4875d}) for execution vertex (id feca28aff5a3958840bee985ee7de4d3_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Allocated logical slot (SlotRequestId{85e436f2363dcf249dca732a2c078f88}) for execution vertex (id 4bf7c1955ffe56e2106d666433eaf137_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (2d0c20acbcd7e2278f1f759d97c10321) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source (1/1) (attempt #0) with attempt id 2d0c20acbcd7e2278f1f759d97c10321 to a6854902-73ff-42a3-9ee7-8543e53de45d @ localhost (dataPort=-1) with allocation id a2d6392d18cb2c712b91cd29063ece26
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (5a7eed33f1778c1406b738339da5a6b5) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Source: Custom Source (1/1) (attempt #0) with attempt id 5a7eed33f1778c1406b738339da5a6b5 to a6854902-73ff-42a3-9ee7-8543e53de45d @ localhost (dataPort=-1) with allocation id a2d6392d18cb2c712b91cd29063ece26
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot a2d6392d18cb2c712b91cd29063ece26.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (788fee4c04d621243b03fffef57b0772) switched from SCHEDULED to DEPLOYING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Deploying Map -> Sink: Unnamed (1/1) (attempt #0) with attempt id 788fee4c04d621243b03fffef57b0772 to a6854902-73ff-42a3-9ee7-8543e53de45d @ localhost (dataPort=-1) with allocation id a2d6392d18cb2c712b91cd29063ece26
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new allocation id a2d6392d18cb2c712b91cd29063ece26 for local state stores for job f7e00431a191f50392988496d13be523.
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_a2d6392d18cb2c712b91cd29063ece26], jobID=f7e00431a191f50392988496d13be523, jobVertexID=bc764cd8ddf7a0cff126f51c16239658, subtaskIndex=0}} for f7e00431a191f50392988496d13be523 - bc764cd8ddf7a0cff126f51c16239658 - 0 under allocation id a2d6392d18cb2c712b91cd29063ece26.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionFactory]Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321): Initialized org.apache.flink.runtime.io.network.partition.ResultPartitionFactory@43146246
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321), deploy into slot with allocation id a2d6392d18cb2c712b91cd29063ece26.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321) switched from CREATED to DEPLOYING.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321) [DEPLOYING]
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 2d0c20acbcd7e2278f1f759d97c10321 at library cache manager took 2 milliseconds
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot a2d6392d18cb2c712b91cd29063ece26.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot a2d6392d18cb2c712b91cd29063ece26.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_a2d6392d18cb2c712b91cd29063ece26], jobID=f7e00431a191f50392988496d13be523, jobVertexID=4bf7c1955ffe56e2106d666433eaf137, subtaskIndex=0}} for f7e00431a191f50392988496d13be523 - 4bf7c1955ffe56e2106d666433eaf137 - 0 under allocation id a2d6392d18cb2c712b91cd29063ece26.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.buffer.LocalBufferPool]Using a local buffer pool with 2-10 buffers
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Registered PipelinedResultPartition 23483146cfaa8f680c797cf8ebad07fc#0@2d0c20acbcd7e2278f1f759d97c10321 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.TaskEventDispatcher]registering 23483146cfaa8f680c797cf8ebad07fc#0@2d0c20acbcd7e2278f1f759d97c10321
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772): Created 1 input channels (local: 1, remote: 0, unknown: 0).
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772): Created 1 input channels (local: 1, remote: 0, unknown: 0).
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772), deploy into slot with allocation id a2d6392d18cb2c712b91cd29063ece26.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772) switched from CREATED to DEPLOYING.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Activate slot a2d6392d18cb2c712b91cd29063ece26.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772) [DEPLOYING]
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 788fee4c04d621243b03fffef57b0772 at library cache manager took 9 milliseconds
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Using partitioner FORWARD for output 0 of task Source: Custom Source
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.buffer.LocalBufferPool]Using a local buffer pool with 1-8 buffers
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.buffer.LocalBufferPool]Using a local buffer pool with 1-8 buffers
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/localState/aid_a2d6392d18cb2c712b91cd29063ece26], jobID=f7e00431a191f50392988496d13be523, jobVertexID=feca28aff5a3958840bee985ee7de4d3, subtaskIndex=0}} for f7e00431a191f50392988496d13be523 - feca28aff5a3958840bee985ee7de4d3 - 0 under allocation id a2d6392d18cb2c712b91cd29063ece26.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionFactory]Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5): Initialized org.apache.flink.runtime.io.network.partition.ResultPartitionFactory@43146246
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received task Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5), deploy into slot with allocation id a2d6392d18cb2c712b91cd29063ece26.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5) switched from CREATED to DEPLOYING.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Creating FileSystem stream leak safety net for task Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5) [DEPLOYING]
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Loading JAR files for task Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Getting user code class loader for task 5a7eed33f1778c1406b738339da5a6b5 at library cache manager took 0 milliseconds
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Registering task at network: Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5) [DEPLOYING].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.buffer.LocalBufferPool]Using a local buffer pool with 2-10 buffers
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Registered PipelinedResultPartition 31eb0ba5d3bdf88cf9cdcad0a71eea19#0@5a7eed33f1778c1406b738339da5a6b5 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.TaskEventDispatcher]registering 31eb0ba5d3bdf88cf9cdcad0a71eea19#0@5a7eed33f1778c1406b738339da5a6b5
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Using partitioner FORWARD for output 0 of task Source: Custom Source
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@71104e71
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@58dafef2
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@78b90966
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Checkpoint storage is set to 'jobmanager'
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772) switched from DEPLOYING to INITIALIZING.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321) switched from DEPLOYING to INITIALIZING.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source (1/1)#0.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Source: Custom Source (1/1)#0.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Initializing Map -> Sink: Unnamed (1/1)#0.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (2d0c20acbcd7e2278f1f759d97c10321) switched from DEPLOYING to INITIALIZING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (788fee4c04d621243b03fffef57b0772) switched from DEPLOYING to INITIALIZING.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (5a7eed33f1778c1406b738339da5a6b5) switched from DEPLOYING to INITIALIZING.
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source (1/1)#0
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Source: Custom Source (1/1)#0
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf]-Dorg.apache.flink.shaded.netty4.io.netty.buffer.checkAccessible: true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf]-Dorg.apache.flink.shaded.netty4.io.netty.buffer.checkBounds: true
[DEBUG][22-06-18][org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetectorFactory]Loaded default ResourceLeakDetector: org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector@248aa433
[DEBUG][22-06-18][org.apache.flink.streaming.runtime.tasks.StreamTask]Invoking Map -> Sink: Unnamed (1/1)#0
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSink_ccb29b5204e83e8a588b3828afaa7015_(1/1) with empty state.
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_bc764cd8ddf7a0cff126f51c16239658_(1/1) with empty state.
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamSource_feca28aff5a3958840bee985ee7de4d3_(1/1) with empty state.
[INFO][22-06-18][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-18][org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase]Consumer subtask 0 has no restore state.
[INFO][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[INFO][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [worker1:9092, worker2:9092, worker3:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = metric_consumer_g
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initializing the Kafka consumer
[DEBUG][22-06-18][org.apache.hadoop.util.Shell]setsid is not available on this machine. So not using it.
[DEBUG][22-06-18][org.apache.hadoop.util.Shell]setsid exited with exit code 0
[DEBUG][22-06-18][org.apache.hadoop.security.Groups] Creating new Groups object
[WARN][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN][22-06-18][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'zookeeper.connect' was supplied but isn't a known config.
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655529007889
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer initialized
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka version: 2.4.1
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId: c57222ae8cd7866b
[INFO][22-06-18][org.apache.kafka.common.utils.AppInfoParser]Kafka startTimeMs: 1655529007889
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer initialized
[DEBUG][22-06-18][org.apache.hadoop.util.NativeCodeLoader]Trying to load the custom-built native-hadoop library...
[DEBUG][22-06-18][org.apache.hadoop.util.NativeCodeLoader]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[DEBUG][22-06-18][org.apache.hadoop.util.NativeCodeLoader]java.library.path=/Users/fabivs/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[WARN][22-06-18][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[DEBUG][22-06-18][org.apache.hadoop.util.PerformanceAdvisory]Falling back to shell based
[DEBUG][22-06-18][org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[DEBUG][22-06-18][org.apache.hadoop.security.Groups]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[DEBUG][22-06-18][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-18][org.apache.kafka.clients.NetworkClient][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Initiating connection to node worker2:9092 (id: -2 rack: null) using address worker2/47.104.108.98
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.lib.MutableMetricsFactory]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
[DEBUG][22-06-18][org.apache.hadoop.metrics2.impl.MetricsSystemImpl]UgiMetrics, User and group related metrics
[DEBUG][22-06-18][org.apache.hadoop.security.SecurityUtil]Setting hadoop.security.token.service.use_ip to true
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]hadoop login
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]hadoop login commit
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]using local user:UnixPrincipal: fabivs
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]Using user: "UnixPrincipal: fabivs" with name fabivs
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]User entry: "fabivs"
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]UGI loginUser:fabivs (auth:SIMPLE)
[DEBUG][22-06-18][org.apache.hadoop.security.UserGroupInformation]PrivilegedAction as:fabivs (auth:SIMPLE) from:org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:347)
[DEBUG][22-06-18][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]Connect 0x3b7c1c4f to master1:2181,master2:2181,utility1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:host.name=localhost
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.version=1.8.0_291
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.vendor=Oracle Corporation
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.class.path=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home/lib/tools.jar:/Users/fabivs/myfile/code/bdp-flink/metricflow-flink/target/classes:/Users/fabivs/.m2/repository/org/apache/flink/flink-streaming-java_2.11/1.13.6/flink-streaming-java_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-core/1.13.6/flink-core-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-annotations/1.13.6/flink-annotations-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-metrics-core/1.13.6/flink-metrics-core-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-asm-7/7.1-13.0/flink-shaded-asm-7-7.1-13.0.jar:/Users/fabivs/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar:/Users/fabivs/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/fabivs/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-file-sink-common/1.13.6/flink-file-sink-common-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-runtime_2.11/1.13.6/flink-runtime_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-queryable-state-client-java/1.13.6/flink-queryable-state-client-java-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-hadoop-fs/1.13.6/flink-hadoop-fs-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-zookeeper-3/3.4.14-13.0/flink-shaded-zookeeper-3-3.4.14-13.0.jar:/Users/fabivs/.m2/repository/org/javassist/javassist/3.24.0-GA/javassist-3.24.0-GA.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-actor_2.11/2.5.21/akka-actor_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/com/typesafe/config/1.3.3/config-1.3.3.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-stream_2.11/2.5.21/akka-stream_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/org/reactivestreams/reactive-streams/1.0.2/reactive-streams-1.0.2.jar:/Users/fabivs/.m2/repository/com/typesafe/ssl-config-core_2.11/0.3.7/ssl-config-core_2.11-0.3.7.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.1/scala-parser-combinators_2.11-1.1.1.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-protobuf_2.11/2.5.21/akka-protobuf_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/com/typesafe/akka/akka-slf4j_2.11/2.5.21/akka-slf4j_2.11-2.5.21.jar:/Users/fabivs/.m2/repository/org/clapper/grizzled-slf4j_2.11/1.3.2/grizzled-slf4j_2.11-1.3.2.jar:/Users/fabivs/.m2/repository/com/github/scopt/scopt_2.11/3.5.0/scopt_2.11-3.5.0.jar:/Users/fabivs/.m2/repository/com/twitter/chill_2.11/0.7.6/chill_2.11-0.7.6.jar:/Users/fabivs/.m2/repository/com/twitter/chill-java/0.7.6/chill-java-0.7.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-guava/18.0-13.0/flink-shaded-guava-18.0-13.0.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-math3/3.5/commons-math3-3.5.jar:/Users/fabivs/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/fabivs/.m2/repository/org/apache/flink/force-shading/1.13.6/force-shading-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-java/1.13.6/flink-java-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-connector-base/1.13.6/flink-connector-base-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka_2.11/2.4.1/kafka_2.11-2.4.1.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.10.0/jackson-databind-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.10.0/jackson-annotations-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.10.0/jackson-core-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.10.0/jackson-module-scala_2.11-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.10.0/jackson-module-paranamer-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.10.0/jackson-dataformat-csv-2.10.0.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.10.0/jackson-datatype-jdk8-2.10.0.jar:/Users/fabivs/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/Users/fabivs/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.11/2.1.2/scala-collection-compat_2.11-2.1.2.jar:/Users/fabivs/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.11/0.9.0/scala-java8-compat_2.11-0.9.0.jar:/Users/fabivs/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/Users/fabivs/.m2/repository/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar:/Users/fabivs/.m2/repository/com/typesafe/scala-logging/scala-logging_2.11/3.9.2/scala-logging_2.11-3.9.2.jar:/Users/fabivs/.m2/repository/org/apache/zookeeper/zookeeper/3.5.7/zookeeper-3.5.7.jar:/Users/fabivs/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.7/zookeeper-jute-3.5.7.jar:/Users/fabivs/.m2/repository/io/netty/netty-handler/4.1.45.Final/netty-handler-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-common/4.1.45.Final/netty-common-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-buffer/4.1.45.Final/netty-buffer-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport/4.1.45.Final/netty-transport-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-resolver/4.1.45.Final/netty-resolver-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-codec/4.1.45.Final/netty-codec-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport-native-epoll/4.1.45.Final/netty-transport-native-epoll-4.1.45.Final.jar:/Users/fabivs/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.45.Final/netty-transport-native-unix-common-4.1.45.Final.jar:/Users/fabivs/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka-streams/2.4.1/kafka-streams-2.4.1.jar:/Users/fabivs/.m2/repository/org/apache/kafka/connect-json/2.4.1/connect-json-2.4.1.jar:/Users/fabivs/.m2/repository/org/apache/kafka/connect-api/2.4.1/connect-api-2.4.1.jar:/Users/fabivs/.m2/repository/org/rocksdb/rocksdbjni/5.18.3/rocksdbjni-5.18.3.jar:/Users/fabivs/.m2/repository/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar:/Users/fabivs/.m2/repository/com/github/luben/zstd-jni/1.4.3-1/zstd-jni-1.4.3-1.jar:/Users/fabivs/.m2/repository/org/lz4/lz4-java/1.6.0/lz4-java-1.6.0.jar:/Users/fabivs/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-clients_2.11/1.13.6/flink-clients_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-optimizer_2.11/1.13.6/flink-optimizer_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-runtime-web_2.11/1.13.6/flink-runtime-web_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-netty/4.1.49.Final-13.0/flink-shaded-netty-4.1.49.Final-13.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-shaded-jackson/2.12.1-13.0/flink-shaded-jackson-2.12.1-13.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-connector-kafka_2.11/1.13.6/flink-connector-kafka_2.11-1.13.6.jar:/Users/fabivs/.m2/repository/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar:/Users/fabivs/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/fabivs/.m2/repository/org/springframework/kafka/spring-kafka/2.7.8/spring-kafka-2.7.8.jar:/Users/fabivs/.m2/repository/org/springframework/spring-context/5.3.11/spring-context-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-aop/5.3.11/spring-aop-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-beans/5.3.11/spring-beans-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-core/5.3.11/spring-core-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-jcl/5.3.11/spring-jcl-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-expression/5.3.11/spring-expression-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-messaging/5.3.11/spring-messaging-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/spring-tx/5.3.11/spring-tx-5.3.11.jar:/Users/fabivs/.m2/repository/org/springframework/retry/spring-retry/1.3.1/spring-retry-1.3.1.jar:/Users/fabivs/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/Users/fabivs/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/Users/fabivs/.m2/repository/org/apache/logging/log4j/log4j-to-slf4j/2.14.0/log4j-to-slf4j-2.14.0.jar:/Users/fabivs/.m2/repository/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-common/3.2.2/hadoop-common-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-annotations/3.2.2/hadoop-annotations-3.2.2.jar:/Users/fabivs/.m2/repository/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar:/Users/fabivs/.m2/repository/com/google/guava/failureaccess/1.0/failureaccess-1.0.jar:/Users/fabivs/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/fabivs/.m2/repository/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar:/Users/fabivs/.m2/repository/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.jar:/Users/fabivs/.m2/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/Users/fabivs/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.jar:/Users/fabivs/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/Users/fabivs/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/Users/fabivs/.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar:/Users/fabivs/.m2/repository/commons-io/commons-io/2.5/commons-io-2.5.jar:/Users/fabivs/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/Users/fabivs/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/fabivs/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/fabivs/.m2/repository/javax/activation/javax.activation-api/1.2.0/javax.activation-api-1.2.0.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-server/9.4.20.v20190813/jetty-server-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-http/9.4.20.v20190813/jetty-http-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-io/9.4.20.v20190813/jetty-io-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-util/9.4.20.v20190813/jetty-util-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.20.v20190813/jetty-servlet-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-security/9.4.20.v20190813/jetty-security-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.20.v20190813/jetty-webapp-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.20.v20190813/jetty-xml-9.4.20.v20190813.jar:/Users/fabivs/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/Users/fabivs/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/Users/fabivs/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/fabivs/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.2/jackson-core-asl-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.2/jackson-mapper-asl-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/Users/fabivs/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/Users/fabivs/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/fabivs/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar:/Users/fabivs/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/Users/fabivs/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/fabivs/.m2/repository/com/google/re2j/re2j/1.1/re2j-1.1.jar:/Users/fabivs/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/fabivs/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-auth/3.2.2/hadoop-auth-3.2.2.jar:/Users/fabivs/.m2/repository/com/nimbusds/nimbus-jose-jwt/7.9/nimbus-jose-jwt-7.9.jar:/Users/fabivs/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/Users/fabivs/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/Users/fabivs/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/Users/fabivs/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar:/Users/fabivs/.m2/repository/com/jcraft/jsch/0.1.55/jsch-0.1.55.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar:/Users/fabivs/.m2/repository/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar:/Users/fabivs/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-compress/1.19/commons-compress-1.19.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/Users/fabivs/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/Users/fabivs/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/Users/fabivs/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/Users/fabivs/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-client/3.2.2/hadoop-client-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.2.2/hadoop-hdfs-client-3.2.2.jar:/Users/fabivs/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/Users/fabivs/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.2.2/hadoop-yarn-api-3.2.2.jar:/Users/fabivs/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.2.2/hadoop-yarn-client-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.2.2/hadoop-mapreduce-client-core-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.2.2/hadoop-yarn-common-3.2.2.jar:/Users/fabivs/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.10/jackson-module-jaxb-annotations-2.9.10.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.10/jackson-jaxrs-json-provider-2.9.10.jar:/Users/fabivs/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.10/jackson-jaxrs-base-2.9.10.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.2.2/hadoop-mapreduce-client-jobclient-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.2.2/hadoop-mapreduce-client-common-3.2.2.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-common/2.3.5/hbase-common-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-logging/2.3.5/hbase-logging-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-miscellaneous/3.3.0/hbase-shaded-miscellaneous-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-gson/3.3.0/hbase-shaded-gson-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-netty/3.3.0/hbase-shaded-netty-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/Users/fabivs/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/Users/fabivs/.m2/repository/org/apache/flink/flink-hbase_2.11/1.10.3/flink-hbase_2.11-1.10.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-server/1.4.3/hbase-server-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-procedure/1.4.3/hbase-procedure-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-common/1.4.3/hbase-common-1.4.3-tests.jar:/Users/fabivs/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-prefix-tree/1.4.3/hbase-prefix-tree-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-metrics-api/1.4.3/hbase-metrics-api-1.4.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-metrics/1.4.3/hbase-metrics-1.4.3.jar:/Users/fabivs/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/fabivs/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/fabivs/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/Users/fabivs/.m2/repository/io/netty/netty-all/4.1.8.Final/netty-all-4.1.8.Final.jar:/Users/fabivs/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/Users/fabivs/.m2/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/Users/fabivs/.m2/repository/junit/junit/4.12/junit-4.12.jar:/Users/fabivs/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-client/2.3.5/hbase-client-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-protobuf/3.3.0/hbase-shaded-protobuf-3.3.0.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-hadoop-compat/2.3.5/hbase-hadoop-compat-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/2.3.5/hbase-hadoop2-compat-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-protocol-shaded/2.3.5/hbase-protocol-shaded-2.3.5.jar:/Users/fabivs/.m2/repository/org/apache/hbase/hbase-protocol/2.3.5/hbase-protocol-2.3.5.jar:/Users/fabivs/.m2/repository/org/jruby/jcodings/jcodings/1.0.18/jcodings-1.0.18.jar:/Users/fabivs/.m2/repository/org/jruby/joni/joni/2.1.11/joni-2.1.11.jar:/Users/fabivs/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.6/metrics-core-3.2.6.jar:/Users/fabivs/.m2/repository/com/alibaba/fastjson/1.2.75/fastjson-1.2.75.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.library.path=/Users/fabivs/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.io.tmpdir=/var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:java.compiler=<NA>
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.name=Mac OS X
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.arch=x86_64
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.version=10.16
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:user.name=fabivs
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:user.home=/Users/fabivs
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:user.dir=/Users/fabivs/myfile/code/bdp-flink
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.free=235MB
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.max=1820MB
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Client environment:os.memory.total=289MB
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Initiating client connection, connectString=master1:2181,master2:2181,utility1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$671/64487180@4bb670f1
[INFO][22-06-18][org.apache.zookeeper.common.X509Util]Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[INFO][22-06-18][org.apache.zookeeper.ClientCnxnSocket]jute.maxbuffer value is 4194304 Bytes
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]zookeeper.request.timeout value is 0. feature enabled=
[DEBUG][22-06-18][org.apache.zookeeper.SaslServerPrincipal]Canonicalized address to master2.cluster
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Opening socket connection to server master2/118.190.145.3:2181. Will not attempt to authenticate using SASL (unknown error)
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0779c89dc502b0bf9e8d189be92b4d8a: SlotReport{SlotStatus{slotID=a6854902-73ff-42a3-9ee7-8543e53de45d_0, allocationID=a2d6392d18cb2c712b91cd29063ece26, jobID=f7e00431a191f50392988496d13be523, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor a6854902-73ff-42a3-9ee7-8543e53de45d: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0779c89dc502b0bf9e8d189be92b4d8a: SlotReport{SlotStatus{slotID=a6854902-73ff-42a3-9ee7-8543e53de45d_0, allocationID=a2d6392d18cb2c712b91cd29063ece26, jobID=f7e00431a191f50392988496d13be523, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor a6854902-73ff-42a3-9ee7-8543e53de45d: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0779c89dc502b0bf9e8d189be92b4d8a: SlotReport{SlotStatus{slotID=a6854902-73ff-42a3-9ee7-8543e53de45d_0, allocationID=a2d6392d18cb2c712b91cd29063ece26, jobID=f7e00431a191f50392988496d13be523, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor a6854902-73ff-42a3-9ee7-8543e53de45d: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[WARN][22-06-18][org.apache.zookeeper.ClientCnxn]Client session timed out, have not heard from server in 30002ms for sessionid 0x0
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Client session timed out, have not heard from server in 30002ms for sessionid 0x0, closing socket connection and attempting reconnect
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxnSocketNIO]Ignoring exception during shutdown input
java.net.SocketException: Socket is not connected
	at sun.nio.ch.Net.translateToSocketException(Net.java:122)
	at sun.nio.ch.Net.translateException(Net.java:156)
	at sun.nio.ch.Net.translateException(Net.java:162)
	at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:401)
	at org.apache.zookeeper.ClientCnxnSocketNIO.cleanup(ClientCnxnSocketNIO.java:198)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1338)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanAndNotifyState(ClientCnxn.java:1276)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1254)
Caused by: java.nio.channels.NotYetConnectedException
	at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:780)
	at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:399)
	... 4 more
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxnSocketNIO]Ignoring exception during shutdown output
java.net.SocketException: Socket is not connected
	at sun.nio.ch.Net.translateToSocketException(Net.java:122)
	at sun.nio.ch.Net.translateException(Net.java:156)
	at sun.nio.ch.Net.translateException(Net.java:162)
	at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:409)
	at org.apache.zookeeper.ClientCnxnSocketNIO.cleanup(ClientCnxnSocketNIO.java:205)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1338)
	at org.apache.zookeeper.ClientCnxn$SendThread.cleanAndNotifyState(ClientCnxn.java:1276)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1254)
Caused by: java.nio.channels.NotYetConnectedException
	at sun.nio.ch.SocketChannelImpl.shutdownOutput(SocketChannelImpl.java:797)
	at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:407)
	... 4 more
[WARN][22-06-18][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]0x3b7c1c4f to master1:2181,master2:2181,utility1:2181 failed for get of /hbase/hbaseid, code = CONNECTIONLOSS, retries = 1
[DEBUG][22-06-18][org.apache.zookeeper.SaslServerPrincipal]Canonicalized address to utility1.cluster
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Opening socket connection to server utility1/118.190.207.81:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Socket connection established, initiating session, client: /192.168.3.92:52218, server: utility1/118.190.207.81:2181
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Session establishment request sent on utility1/118.190.207.81:2181
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]Session establishment complete on server utility1/118.190.207.81:2181, sessionid = 0x30085a06e27001e, negotiated timeout = 40000
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Reading reply sessionid:0x30085a06e27001e, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,81605000550,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a31363030306effffffd2ffffffac57ffffffa31c1bffffffd450425546a2435623632316435312d376363372d343162362d396438632d336662646463326230303833,s{47244640348,73014444056,1646900261205,1650525792431,20,0,0,0,67,0,47244640348} 
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.logging.InternalLoggerFactory]Using SLF4J as the default logging framework
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector]-Dorg.apache.hbase.thirdparty.io.netty.leakDetection.level: simple
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector]-Dorg.apache.hbase.thirdparty.io.netty.leakDetection.targetRecords: 4
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetectorFactory]Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@fb49cd5
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]Platform: MacOS
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]-Dio.netty.noUnsafe: false
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]Java version: 8
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.theUnsafe: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]sun.misc.Unsafe.copyMemory: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.Buffer.address: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]direct buffer constructor: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.Bits.unaligned: available, true
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent0]java.nio.DirectByteBuffer.<init>(long, int): available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]sun.misc.Unsafe: available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.tmpdir: /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T (java.io.tmpdir)
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.maxDirectMemory: 1908932608 bytes
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.CleanerJava6]java.nio.ByteBuffer.cleaner(): available
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]-Dio.netty.noPreferDirect: false
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.PlatformDependent]org.jctools-core.MpscChunkedArrayQueue: available
[DEBUG][22-06-18][org.apache.hadoop.hbase.util.ClassSize]Using Unsafe to estimate memory layout
[DEBUG][22-06-18][org.apache.hadoop.hbase.ipc.AbstractRpcClient]Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@13b0fe60, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.channel.MultithreadEventLoopGroup]-Dio.netty.eventLoopThreads: 16
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.util.internal.InternalThreadLocalMap]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop]-Dio.netty.noKeySetOptimization: false
[DEBUG][22-06-18][org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop]-Dio.netty.selectorAutoRebuildThreshold: 512
[DEBUG][22-06-18][org.apache.flink.streaming.api.operators.BackendRestorerProcedure]Creating operator state backend for StreamMap_4bf7c1955ffe56e2106d666433eaf137_(1/1) with empty state.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772)/InputChannelInfo{gateIdx=0, inputChannelIdx=0} finished recovering input.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772)/InputChannelInfo{gateIdx=1, inputChannelIdx=0} finished recovering input.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772) switched from INITIALIZING to RUNNING.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate]Converting recovered input channels (1 channels)
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (788fee4c04d621243b03fffef57b0772) switched from INITIALIZING to RUNNING.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister]stopPersisting -1, lastSeenBarrier = -1 (COMPLETED) @ InputChannelInfo{gateIdx=0, inputChannelIdx=0}
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel]LocalInputChannel [23483146cfaa8f680c797cf8ebad07fc#0@2d0c20acbcd7e2278f1f759d97c10321]: Requesting LOCAL subpartition 0 of partition 23483146cfaa8f680c797cf8ebad07fc#0@2d0c20acbcd7e2278f1f759d97c10321. ChannelStatePersister(lastSeenBarrier=-1 (COMPLETED)}
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Requesting subpartition 0 of PipelinedResultPartition 23483146cfaa8f680c797cf8ebad07fc#0@2d0c20acbcd7e2278f1f759d97c10321 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.PipelinedSubpartition]Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321): Creating read view for subpartition 0 of partition 23483146cfaa8f680c797cf8ebad07fc#0@2d0c20acbcd7e2278f1f759d97c10321.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartition]Created PipelinedSubpartitionView(index: 0) of ResultPartition 23483146cfaa8f680c797cf8ebad07fc#0@2d0c20acbcd7e2278f1f759d97c10321
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate]Converting recovered input channels (1 channels)
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister]stopPersisting -1, lastSeenBarrier = -1 (COMPLETED) @ InputChannelInfo{gateIdx=1, inputChannelIdx=0}
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel]LocalInputChannel [31eb0ba5d3bdf88cf9cdcad0a71eea19#0@5a7eed33f1778c1406b738339da5a6b5]: Requesting LOCAL subpartition 0 of partition 31eb0ba5d3bdf88cf9cdcad0a71eea19#0@5a7eed33f1778c1406b738339da5a6b5. ChannelStatePersister(lastSeenBarrier=-1 (COMPLETED)}
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Requesting subpartition 0 of PipelinedResultPartition 31eb0ba5d3bdf88cf9cdcad0a71eea19#0@5a7eed33f1778c1406b738339da5a6b5 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.PipelinedSubpartition]Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5): Creating read view for subpartition 0 of partition 31eb0ba5d3bdf88cf9cdcad0a71eea19#0@5a7eed33f1778c1406b738339da5a6b5.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartition]Created PipelinedSubpartitionView(index: 0) of ResultPartition 31eb0ba5d3bdf88cf9cdcad0a71eea19#0@5a7eed33f1778c1406b738339da5a6b5
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0779c89dc502b0bf9e8d189be92b4d8a: SlotReport{SlotStatus{slotID=a6854902-73ff-42a3-9ee7-8543e53de45d_0, allocationID=a2d6392d18cb2c712b91cd29063ece26, jobID=f7e00431a191f50392988496d13be523, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor a6854902-73ff-42a3-9ee7-8543e53de45d: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Got ping response for sessionid: 0x30085a06e27001e after 37ms
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0779c89dc502b0bf9e8d189be92b4d8a: SlotReport{SlotStatus{slotID=a6854902-73ff-42a3-9ee7-8543e53de45d_0, allocationID=a2d6392d18cb2c712b91cd29063ece26, jobID=f7e00431a191f50392988496d13be523, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor a6854902-73ff-42a3-9ee7-8543e53de45d: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Idle slots have been returned; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 69b766fea06a637db958cda1b9264d79.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Received slot report from instance 0779c89dc502b0bf9e8d189be92b4d8a: SlotReport{SlotStatus{slotID=a6854902-73ff-42a3-9ee7-8543e53de45d_0, allocationID=a2d6392d18cb2c712b91cd29063ece26, jobID=f7e00431a191f50392988496d13be523, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing cluster partition report from task executor a6854902-73ff-42a3-9ee7-8543e53de45d: PartitionReport{entries=[]}.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Trigger heartbeat request.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Received heartbeat request from 6720d458d2a1ec9170d39fd3c3f641a5.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Received heartbeat from a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Got ping response for sessionid: 0x30085a06e27001e after 32ms
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Sending synchronous auto-commit of offsets {}
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Executing onLeavePrepare with generation Generation{generationId=-1, memberId='', protocol='null'} and memberId 
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Resetting generation due to consumer pro-actively leaving the group
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-1, groupId=metric_consumer_g] Kafka consumer has been closed
[DEBUG][22-06-18][org.apache.kafka.clients.consumer.KafkaConsumer][Consumer clientId=consumer-metric_consumer_g-2, groupId=metric_consumer_g] Kafka consumer has been closed
[WARN][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[WARN][22-06-18][org.apache.flink.runtime.taskmanager.Task]Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5) switched from INITIALIZING to FAILED with failure cause: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata

[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321).
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5).
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Release task Source: Custom Source (1/1)#0 network resources (state: FAILED).
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.TaskEventDispatcher]unregistering 31eb0ba5d3bdf88cf9cdcad0a71eea19#0@5a7eed33f1778c1406b738339da5a6b5
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.TaskEventDispatcher]unregistering 23483146cfaa8f680c797cf8ebad07fc#0@2d0c20acbcd7e2278f1f759d97c10321
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartition]Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5): Releasing PipelinedResultPartition 31eb0ba5d3bdf88cf9cdcad0a71eea19#0@5a7eed33f1778c1406b738339da5a6b5 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.PipelinedSubpartition]Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5): Released PipelinedSubpartition#0 [number of buffers: 1 (4 bytes), number of buffers in backlog: 0, finished? false, read view? false].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Released partition 31eb0ba5d3bdf88cf9cdcad0a71eea19#0 produced by 5a7eed33f1778c1406b738339da5a6b5.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartition]Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321): Releasing PipelinedResultPartition 23483146cfaa8f680c797cf8ebad07fc#0@2d0c20acbcd7e2278f1f759d97c10321 [PIPELINED_BOUNDED, 1 subpartitions, 2 pending consumptions].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.PipelinedSubpartition]Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321): Released PipelinedSubpartition#0 [number of buffers: 1 (4 bytes), number of buffers in backlog: 0, finished? false, read view? false].
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Released partition 23483146cfaa8f680c797cf8ebad07fc#0 produced by 2d0c20acbcd7e2278f1f759d97c10321.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source (1/1)#0 (5a7eed33f1778c1406b738339da5a6b5) [FAILED]
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Source: Custom Source (1/1)#0 (2d0c20acbcd7e2278f1f759d97c10321) [FAILED]
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source (1/1)#0 2d0c20acbcd7e2278f1f759d97c10321.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source (1/1)#0 5a7eed33f1778c1406b738339da5a6b5.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (2d0c20acbcd7e2278f1f759d97c10321) switched from INITIALIZING to FAILED on a6854902-73ff-42a3-9ee7-8543e53de45d @ localhost (dataPort=-1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{7288acfead59069363c79f000abfa0d2}) for execution vertex (id bc764cd8ddf7a0cff126f51c16239658_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]Calculating tasks to restart to recover the failed task bc764cd8ddf7a0cff126f51c16239658_0.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy]3 tasks should be restarted to recover the failed task bc764cd8ddf7a0cff126f51c16239658_0. 
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]No SlotExecutionVertexAssignment for logical null from physical SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3}}
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{27aa936427b23e85018405c849c4875d}) for execution vertex (id feca28aff5a3958840bee985ee7de4d3_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3}) from feca28aff5a3958840bee985ee7de4d3_0
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Cancel logical slot (SlotRequestId{85e436f2363dcf249dca732a2c078f88}) for execution vertex (id 4bf7c1955ffe56e2106d666433eaf137_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3}) from 4bf7c1955ffe56e2106d666433eaf137_0
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (f7e00431a191f50392988496d13be523) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (5a7eed33f1778c1406b738339da5a6b5) switched from INITIALIZING to CANCELING.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 5a7eed33f1778c1406b738339da5a6b5.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (788fee4c04d621243b03fffef57b0772) switched from RUNNING to CANCELING.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Attempting to cancel task Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772).
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772) switched from RUNNING to CANCELING.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Triggering cancellation of task code Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772).
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Source: Custom Source (1/1) (5a7eed33f1778c1406b738339da5a6b5) switched from CANCELING to CANCELED.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source (1/1) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{27aa936427b23e85018405c849c4875d}) for execution vertex (id feca28aff5a3958840bee985ee7de4d3_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Discarding the results produced by task execution 5a7eed33f1778c1406b738339da5a6b5.
[DEBUG][22-06-18][org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient]Close zookeeper connection 0x3b7c1c4f to master1:2181,master2:2181,utility1:2181
[DEBUG][22-06-18][org.apache.hadoop.hbase.ipc.AbstractRpcClient]Stopping rpc client
[DEBUG][22-06-18][org.apache.zookeeper.ZooKeeper]Closing session: 0x30085a06e27001e
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Closing client for session: 0x30085a06e27001e
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772) switched from CANCELING to CANCELED.
[INFO][22-06-18][org.apache.flink.runtime.taskmanager.Task]Freeing task resources for Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772).
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Release task Map -> Sink: Unnamed (1/1)#0 network resources (state: CANCELED).
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772): Releasing SingleInputGate{owningTaskName='Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772)', gateIndex=0}.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate]Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772): Releasing SingleInputGate{owningTaskName='Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772)', gateIndex=1}.
[DEBUG][22-06-18][org.apache.flink.runtime.taskmanager.Task]Ensuring all FileSystem streams are closed for task Map -> Sink: Unnamed (1/1)#0 (788fee4c04d621243b03fffef57b0772) [CANCELED]
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Discarding the results produced by task execution 5a7eed33f1778c1406b738339da5a6b5.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Un-registering task and sending final execution state CANCELED to JobManager for task Map -> Sink: Unnamed (1/1)#0 788fee4c04d621243b03fffef57b0772.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 5a7eed33f1778c1406b738339da5a6b5.
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Map -> Sink: Unnamed (1/1) (788fee4c04d621243b03fffef57b0772) switched from CANCELING to CANCELED.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Map -> Sink: Unnamed (1/1) - execution #0 to FAILED while being CANCELED.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Remove logical slot (SlotRequestId{85e436f2363dcf249dca732a2c078f88}) for execution vertex (id 4bf7c1955ffe56e2106d666433eaf137_0) from the physical slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot externally (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Release slot with slot request id SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3}
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Free reserved slot a2d6392d18cb2c712b91cd29063ece26.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SharedSlot]Release shared slot (SlotRequestId{aad7aef6482d9a69646a6e1384fa8cf3})
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge]Could not match slot AllocatedSlot a2d6392d18cb2c712b91cd29063ece26 @ a6854902-73ff-42a3-9ee7-8543e53de45d @ localhost (dataPort=-1) - 0 to any pending request.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Declare new resource requirements for job f7e00431a191f50392988496d13be523.
	required resources: []
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=1}}
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Clearing resource requirements of job f7e00431a191f50392988496d13be523
[INFO][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Job metricStream (f7e00431a191f50392988496d13be523) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker]Detected excess resources for job f7e00431a191f50392988496d13be523: [ExcessResource{numExcessResources=1, requirementProfile=ResourceProfile{UNKNOWN}, resourceProfile=ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}}]
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Reading reply sessionid:0x30085a06e27001e, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,81605000551,0  request:: null response:: null
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]Disconnecting client for session: 0x30085a06e27001e
[DEBUG][22-06-18][org.apache.zookeeper.ClientCnxn]An exception was thrown while closing send thread for session 0x30085a06e27001e : Unable to read additional data from server sessionid 0x30085a06e27001e, likely server has closed socket
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]ExecutionGraph f7e00431a191f50392988496d13be523 reached terminal state FAILED.
[INFO][22-06-18][org.apache.flink.runtime.checkpoint.CheckpointCoordinator]Stopping checkpoint coordinator for job f7e00431a191f50392988496d13be523.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Archive global failure.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-18][org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore]Shutting down
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 5a7eed33f1778c1406b738339da5a6b5.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Job f7e00431a191f50392988496d13be523 under leader id f7a50145-0949-47e2-8b6d-208acb1cc72c reached a globally terminal state FAILED.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Completing the result for job f7e00431a191f50392988496d13be523.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Cannot find task to stop for execution 5a7eed33f1778c1406b738339da5a6b5.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Job f7e00431a191f50392988496d13be523 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:216)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:206)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
[INFO][22-06-18][org.apache.flink.runtime.minicluster.MiniCluster]Shutting down Flink Mini Cluster
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Terminating the leadership runner for job f7e00431a191f50392988496d13be523.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]Terminating the JobMasterService process for job f7e00431a191f50392988496d13be523 under leader id f7a50145-0949-47e2-8b6d-208acb1cc72c.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close ResourceManager connection 69b766fea06a637db958cda1b9264d79.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shutting down rest endpoint.
[DEBUG][22-06-18][org.apache.flink.runtime.executiongraph.ExecutionGraph]Ignoring transition of vertex Source: Custom Source (1/1) - execution #0 to FAILED while being CANCELED.
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Stopping the JobMaster for job metricStream(f7e00431a191f50392988496d13be523).
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Closing TaskExecutor connection a6854902-73ff-42a3-9ee7-8543e53de45d because: The TaskExecutor is shutting down.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Close JobManager connection for job f7e00431a191f50392988496d13be523.
org.apache.flink.util.FlinkException: The TaskExecutor is shutting down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:438)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID bc764cd8ddf7a0cff126f51c16239658_0
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Unregistering task executor 0779c89dc502b0bf9e8d189be92b4d8a from the slot manager.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID feca28aff5a3958840bee985ee7de4d3_0
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job f7e00431a191f50392988496d13be523.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl]Processing shutdown of task executor a6854902-73ff-42a3-9ee7-8543e53de45d.
[DEBUG][22-06-18][org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator]There is no SharedSlot for ExecutionSlotSharingGroup of ExecutionVertexID 4bf7c1955ffe56e2106d666433eaf137_0
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Disconnect TaskExecutor a6854902-73ff-42a3-9ee7-8543e53de45d because: Stopping JobMaster for job metricStream(f7e00431a191f50392988496d13be523).
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl]Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=128.000mb (134217728 bytes), networkMemory=64.000mb (67108864 bytes)}, allocationId: a2d6392d18cb2c712b91cd29063ece26, jobId: f7e00431a191f50392988496d13be523).
org.apache.flink.util.FlinkException: Closing task slot table
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.closeAsync(TaskSlotTableImpl.java:173)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:455)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO][22-06-18][org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool]Releasing slot [a2d6392d18cb2c712b91cd29063ece26].
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(f7e00431a191f50392988496d13be523).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMaster]Close ResourceManager connection 69b766fea06a637db958cda1b9264d79.
org.apache.flink.util.FlinkException: Stopping JobMaster for job metricStream(f7e00431a191f50392988496d13be523).
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:400)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdService]Remove job f7e00431a191f50392988496d13be523 from job leader id monitoring.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Disconnect job manager 8b6d208acb1cc72cf7a50145094947e2@akka://flink/user/rpc/jobmanager_3 for job f7e00431a191f50392988496d13be523 from the resource manager.
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Free slot with allocation id a2d6392d18cb2c712b91cd29063ece26 because: Stopping JobMaster for job metricStream(f7e00431a191f50392988496d13be523).
[DEBUG][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Could not free slot for allocation id a2d6392d18cb2c712b91cd29063ece26.
org.apache.flink.runtime.taskexecutor.slot.SlotNotFoundException: Could not find slot for a2d6392d18cb2c712b91cd29063ece26.
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.freeSlot(TaskSlotTableImpl.java:429)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1906)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:1125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint jobmanager_3 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Initiating tracking of resources for job f7e00431a191f50392988496d13be523.
[DEBUG][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Releasing local state under allocation id a2d6392d18cb2c712b91cd29063ece26.
[DEBUG][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker]Stopping tracking of resources for job f7e00431a191f50392988496d13be523.
[INFO][22-06-18][org.apache.zookeeper.ZooKeeper]Session: 0x30085a06e27001e closed
[INFO][22-06-18][org.apache.zookeeper.ClientCnxn]EventThread shut down for session: 0x30085a06e27001e
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Removing cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-web-ui
[INFO][22-06-18][org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager]Shutting down TaskExecutorLocalStateStoresManager.
[DEBUG][22-06-18][org.apache.flink.runtime.io.disk.iomanager.IOManager]Shutting down I/O manager.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint]Shut down complete.
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-io-1f5e2dda-c7cc-4d5a-8b95-fb9bcc52df99
[INFO][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down the network environment and its components.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down network connection manager
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.NettyShuffleEnvironment]Shutting down intermediate result partition manager
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Releasing 0 partitions because of shutdown.
[DEBUG][22-06-18][org.apache.flink.runtime.io.network.partition.ResultPartitionManager]Successful shutdown.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/jobmanager_3 has terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner]Leadership runner for job f7e00431a191f50392988496d13be523 has been terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess]JobMasterService process for job f7e00431a191f50392988496d13be523 under leader id f7a50145-0949-47e2-8b6d-208acb1cc72c has been terminated.
[INFO][22-06-18][org.apache.flink.runtime.io.disk.FileChannelManagerImpl]FileChannelManager removed spill file directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-netty-shuffle-9eaf041a-39a4-4fbc-9ffc-c6e6c30a5755
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.KvStateService]Shutting down the kvState service and its components.
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService]Stop job leader service.
[INFO][22-06-18][org.apache.flink.runtime.filecache.FileCache]removed file cache directory /var/folders/jw/xgr00jzx6vd4jd0fjqkwbs8r0000gn/T/flink-dist-cache-1c5f4eb0-a14a-4d91-b294-515ee8bde217
[INFO][22-06-18][org.apache.flink.runtime.taskexecutor.TaskExecutor]Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint taskmanager_0 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/taskmanager_0 has terminated.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.StandaloneResourceManager]Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
[INFO][22-06-18][org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent]Closing components.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess]Stopping SessionDispatcherLeaderProcess.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Closing the slot manager.
[INFO][22-06-18][org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager]Suspending the slot manager.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint resourcemanager_1 terminated successfully.
[INFO][22-06-18][org.apache.flink.runtime.dispatcher.StandaloneDispatcher]Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor]The RpcEndpoint dispatcher_2 terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/resourcemanager_1 has terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink/user/rpc/dispatcher_2 has terminated.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcActor]The RpcEndpoint MetricQueryService terminated successfully.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]AkkaRpcActor akka://flink-metrics/user/rpc/MetricQueryService has terminated.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopping Akka RPC service.
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
[DEBUG][22-06-18][org.apache.flink.runtime.rpc.akka.SupervisorActor]Stopping supervisor actor.
[DEBUG][22-06-18][akka.event.EventStream]shutting down: StandardOutLogger
[INFO][22-06-18][org.apache.flink.runtime.blob.PermanentBlobCache]Shutting down BLOB cache
[INFO][22-06-18][org.apache.flink.runtime.blob.TransientBlobCache]Shutting down BLOB cache
[INFO][22-06-18][org.apache.flink.runtime.blob.BlobServer]Stopped BLOB server at 0.0.0.0:52210
[INFO][22-06-18][org.apache.flink.runtime.rpc.akka.AkkaRpcService]Stopped Akka RPC service.
